[{"content":"坑场景 1. 事务 A 里面注册事务成功回调 B，B 为一个加了 @Transactional 注解的方法。但结果会导致 B 里面的事务失效。 坑场景 1.事务A里面注册事务成功回调B，B为一个加了@Transactional注解的方法。但结果会导致B里面的事务失效。 参考jdk注解：\nimg.png\r注意描述：不再执行提交，除非它明确声明它需要在单独的事务中运行。\n2.事务A里面注册回调B，回调B里面注册回调C，回调C不会执行。 原因跟1里面一样，因为回调B就算加了@Transactional注解，由于没有自定义传播行为，则相当于没有添加事务（这里得注意，通过TransactionSynchronizationManager#isSynchronizationActive判断，事务仍是活动状态，但实际回调B里面相当于没有事务）所以回调C不会执行。\n解决 public class TransactionUtil { private static volatile TransactionHelper TRANSACTION_HELPER; public static void doSomethingIfCommittedOrNow(Runnable runnable){ if( TransactionSynchronizationManager.isSynchronizationActive()){ TransactionSynchronizationManager.registerSynchronization( new TransactionSynchronizationAdapter() { @Override public void afterCommit() { transactionHelper().doRequiredNewTransaction(()-\u0026gt;{ runnable.run(); }); } }); }else{ runnable.run(); } } public static void doSomethingIfCompletionOrNow(Consumer\u0026lt;Integer\u0026gt; consumer){ if( TransactionSynchronizationManager.isSynchronizationActive()){ TransactionSynchronizationManager.registerSynchronization( new TransactionSynchronizationAdapter() { @Override public void afterCompletion(int status) { transactionHelper().doRequiredNewTransaction(()-\u0026gt;{ consumer.accept(status); }); } }); }else{ consumer.accept(null); } } public static TransactionHelper transactionHelper(){ if(TRANSACTION_HELPER == null){ TRANSACTION_HELPER = SpringUtil.getBean(TransactionHelper.class); return TRANSACTION_HELPER; }else{ return TRANSACTION_HELPER; } } } @Component public class TransactionHelper { @Transactional public \u0026lt;T\u0026gt; T doRequiredTransaction(Supplier\u0026lt;T\u0026gt; supplier) { return supplier.get(); } @Transactional public void doRequiredTransaction(Runnable runnable) { runnable.run(); } @Transactional(propagation = Propagation.REQUIRES_NEW) public \u0026lt;T\u0026gt; T doRequiredNewTransaction(Supplier\u0026lt;T\u0026gt; supplier) { return supplier.get(); } @Transactional(propagation = Propagation.REQUIRES_NEW) public void doRequiredNewTransaction(Runnable runnable) { runnable.run(); } @Transactional(propagation = Propagation.NESTED) public \u0026lt;T\u0026gt; T doNestedTransaction(Supplier\u0026lt;T\u0026gt; supplier) { return supplier.get(); } @Transactional(propagation = Propagation.NESTED) public void doNestedTransaction(Runnable runnable) { runnable.run(); } } 只需要每次回调的时候在新事务里面就行了（原先的工具类没有添加新事务）。\n参考 jdk 注解：\nimg_1.png\r2. 事务 A 里面注册回调 B，回调 B 里面注册回调 C，回调 C 不会执行。 原因跟 1 里面一样，因为回调 B 就算加了 @Transactional 注解，由于没有自定义传播行为，则相当于没有添加事务（这里得注意，通过 TransactionSynchronizationManager#isSynchronizationActive 判断，事务仍是活动状态，但实际回调 B 里面相当于没有事务）所以回调 C 不会执行。\n解决 public class TransactionUtil {\nprivate static volatile TransactionHelper TRANSACTION_HELPER; public static void doSomethingIfCommittedOrNow(Runnable runnable){ if( TransactionSynchronizationManager.isSynchronizationActive()){ TransactionSynchronizationManager.registerSynchronization( new TransactionSynchronizationAdapter() { @Override public void afterCommit() { transactionHelper().doRequiredNewTransaction(()-\u0026gt;{ runnable.run(); }); } }); }else{ runnable.run(); } } public static void doSomethingIfCompletionOrNow(Consumer\u0026lt;Integer\u0026gt; consumer){ if( TransactionSynchronizationManager.isSynchronizationActive()){ TransactionSynchronizationManager.registerSynchronization( new TransactionSynchronizationAdapter() { @Override public void afterCompletion(int status) { transactionHelper().doRequiredNewTransaction(()-\u0026gt;{ consumer.accept(status); }); } }); }else{ consumer.accept(null); } } public static TransactionHelper transactionHelper(){ if(TRANSACTION_HELPER == null){ TRANSACTION_HELPER = SpringUtil.getBean(TransactionHelper.class); return TRANSACTION_HELPER; }else{ return TRANSACTION_HELPER; } } }\n@Component\npublic class TransactionHelper {\n@Transactional public \u0026lt;T\u0026gt; T doRequiredTransaction(Supplier\u0026lt;T\u0026gt; supplier) { return supplier.get(); } @Transactional public void doRequiredTransaction(Runnable runnable) { runnable.run(); } @Transactional(propagation = Propagation.REQUIRES_NEW) public \u0026lt;T\u0026gt; T doRequiredNewTransaction(Supplier\u0026lt;T\u0026gt; supplier) { return supplier.get(); } @Transactional(propagation = Propagation.REQUIRES_NEW) public void doRequiredNewTransaction(Runnable runnable) { runnable.run(); } @Transactional(propagation = Propagation.NESTED) public \u0026lt;T\u0026gt; T doNestedTransaction(Supplier\u0026lt;T\u0026gt; supplier) { return supplier.get(); } @Transactional(propagation = Propagation.NESTED) public void doNestedTransaction(Runnable runnable) { runnable.run(); } }\n只需要每次回调的时候在新事务里面就行了（原先的工具类没有添加新事务）。\n注意描述：不再执行提交，除非它明确声明它需要在单独的事务中运行。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2023-10-26T00:00:00Z","permalink":"https://runningccode.github.io/2023/transactionsynchronizationmanager#registersynchronization%E6%96%B9%E6%B3%95-aftercommit-aftercompletion-%E5%9D%91%E8%AE%B0%E5%BD%95/","title":"TransactionSynchronizationManager#registerSynchronization方法 afterCommit afterCompletion 坑记录"},{"content":"监听器会被执行多次 如果在spring.factories注册一个监听器，形如:org.springframework.context.ApplicationListener=com.zerofinance.xpay.commons.component.knife4j.Knife4jCfg4SpringStartListener\npublic class Knife4jCfg4SpringStartListener implements ApplicationListener\u0026lt;ApplicationPreparedEvent\u0026gt; { @Override public void onApplicationEvent(ApplicationPreparedEvent event) { ConfigurableEnvironment env = event.getApplicationContext().getEnvironment(); MutablePropertySources m = env.getPropertySources(); Properties p = new Properties(); p.put(\u0026quot;knife4j.enable\u0026quot;, \u0026quot;true\u0026quot;); if (EnvUtils.isTestLower(event.getApplicationContext().getEnvironment())) { } else { p.put(\u0026quot;knife4j.production\u0026quot;,\u0026quot;true\u0026quot;); } m.addFirst(new PropertiesPropertySource(\u0026quot;knife4jCfg\u0026quot;, p)); } } 则该监听器监听逻辑会被执行多次。\n原因分析 1. ApplicationContext有多个 在BootstrapApplicationListener中会创建一个bootstrap ApplicationContext。这个ApplciationContext也会发布相应的事件。\n2. RestartListener会在ContextRefreshedEvent事件触发时补发ApplicationPreparedEvent事件 RestartListener 在容器刷新时会监听 ContextRefreshedEvent 事件，然后广播ApplicationPreparedEvent 事件。至于为什么要重新广播，原因是：\n因为在应用重启的场景中，已经存在一个完整的应用上下文（即 ConfigurableApplicationContext），其他事件的处理已经在应用启动时完成，不需要重新处理。而 ApplicationPreparedEvent 是在应用启动或重启时都需要处理的事件，因此需要重新广播。\n举个例子，假设你的应用需要从多个配置源中读取配置，比如环境变量、命令行参数、配置文件等。在应用启动时，Spring Boot 会依次广播 ApplicationEnvironmentPreparedEvent、ApplicationContextInitializedEvent、ApplicationPreparedEvent 和 ApplicationStartedEvent 事件。在这些事件的处理过程中，Spring Boot 会解析并加载所有的配置源，并将它们合并成一个 Environment 对象，用于后续的应用配置。\n在容器刷新时，由于已经存在一个完整的应用上下文，这些事件的处理已经完成，不需要重新执行。而 ApplicationPreparedEvent 事件则需要重新广播，因为它涉及到应用的预热、预加载等操作，需要重新执行以确保应用正确启动。\n3.AbstractApplicationContext#publishEvent方法会补发事件到父容器 AbstractApplicationContext类源码摘要：\nprotected void publishEvent(Object event, @Nullable ResolvableType eventType) { Assert.notNull(event, \u0026quot;Event must not be null\u0026quot;); // Decorate event as an ApplicationEvent if necessary ApplicationEvent applicationEvent; if (event instanceof ApplicationEvent) { applicationEvent = (ApplicationEvent) event; } else { applicationEvent = new PayloadApplicationEvent\u0026lt;\u0026gt;(this, event); if (eventType == null) { eventType = ((PayloadApplicationEvent\u0026lt;?\u0026gt;) applicationEvent).getResolvableType(); } } // Multicast right now if possible - or lazily once the multicaster is initialized if (this.earlyApplicationEvents != null) { this.earlyApplicationEvents.add(applicationEvent); } else { getApplicationEventMulticaster().multicastEvent(applicationEvent, eventType); } // Publish event via parent context as well... if (this.parent != null) { if (this.parent instanceof AbstractApplicationContext) { ((AbstractApplicationContext) this.parent).publishEvent(event, eventType); } else { this.parent.publishEvent(event); } } } 在 Spring 应用上下文中，父子上下文的事件广播存在着一定的继承关系。当一个事件被子上下文发布时，如果父上下文也存在，那么这个事件将被传播到父上下文，然后再由父上下文向它的监听器广播。这样做的目的是为了让父上下文及其监听器也能够感知到子上下文发生的事件。\n在 publishEvent() 方法中，如果当前上下文有一个父上下文，那么它会再次将事件传递给父上下文的 publishEvent() 方法。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2023-10-25T00:00:00Z","permalink":"https://runningccode.github.io/2023/%E8%AE%B0%E5%BD%95springapplicationevent%E4%BA%8B%E4%BB%B6%E7%9B%91%E5%90%AC%E7%9A%84%E5%9D%91/","title":"记录SpringApplicationEvent事件监听的坑"},{"content":"参考资料：\n分布式一致性算法——Paxos原理与推导过程\n百度开源 / braft\nRaft 一致性协议完整解析\nPaxos，Raft，ZAB的差异对比\n前言 用于在多数据备份的分布式系统中，因机器宕机或网络异常（包括消息的延迟、丢失、重复、乱序，还有网络分区）等情况产生各个节点中的数据不一致。为了各个备份数据在最终都能与集群保持同步，拥有统一共识，所需遵守的一套协议。\n一、Paxos协议 Paxos协议相对基础，也是其他协议的基础，他的条件限制没有ZAB和RAFT那么多，仅仅想要保证的是通过Paxos协议，就算发生了数据分歧，也只有一个值被选定，即对于节点来说如果选定了某个数据，不存在其他节点选定另一个数据，只有尚未选定和选定同一个值的情况，可以保证分布式系统中数据的最终一致性。\n角色 提出者(Proposer) 提出提案 想要选定某一提案的角色 对于proposer来说，当收到一半的acceptor接受提案，即认为提案被选定。 接受者(Acceptor) 接受提案 判断是否接收提案的角色，当接受某提案之后，认为提案被选定。 学习者(Learners) 学习提案 提案若被选定，学习者即可以学习到提案。收到acceptor选定某提案请求学习之后，认为提案被选定。 注：上面三种角色是逻辑上的概念，并没有规定说不能是同一进程，只是拥有不同的角色。\n提案的提出过程 第一步：提出者向过半接受者发prepare请求 提出者为该数据的提案指定一个编号N ，编号N需要保证是该数据维度全局唯一并且自增的，发送给接受者。\n第二步：接受者忽略/拒绝prepare请求或响应prepare请求 接受者判断是否接受过比N大的提案，如果有，则接受者会忽略或拒绝此次prepare,拒绝更好，可以让提案发起者更早知道自己的提案不会被该接受者接受。 如果没有接受过比N大的提案，但接受过比N小的提案，则将接受过的最大提案号和值(M,V)返回给提出者，如果从没接受过提案,则需要承诺不再接受任何编号比N小的提案。\n第三步：提出者收到过半接受者的响应，然后向过半接受者发出Accept请求 接受者收到各种响应，如果有针对该数据的提案已经被某些接受者接受，那么从中选出一个提案号最大的，作为此次accept请求的值发送给接受者，如果没有则提出者可以自行选定值然后发accept请求。\n第四步：学习者学习被选定的value acceptor接受了某个值之后，将接受的值转发给学习者，学习者将学习到该提案。\n活锁问题 img.png\r借用一张图，上面图示就是活锁问题，当两个proposer并行的想要提交某一提案的时候，可能进入无限循环。\n优化 Multi-Paxos 是Basic-Paxos的优化版本。区别是：\n1.改变第一阶段 发起提议只有一个leader，leader选举用Basic Paxos协议产生，产生leader后接下来的提案提交只需要进行第二阶段accept请求,不用每次提议都经过第一阶段。 当多个自认为leader的节点（网络分区的情况）同时发起提议退化为Basic Paxos协议 2.解决活锁问题 因为杜绝了多个proposer同时提交提案。\n二、ZAB协议 ZAB是Zookeeper的分布式一致性协议，zookeeper通过ZAB，进行消息广播和崩溃恢复，保证各节点数据的最终一致性。\n在zookeeper中，写请求由leader处理，读请求可以由follower处理。\n注：因为是最终一致性，所以zookeeper读请求存在不能及时读到最新数据的情况，但也保证读写一致性，客户端能保证自己写的数据，自己能看到（通过转发读请求进行处理）。\n思考：为什么说Zookeeper为CP? 是相对而言来说的，比如euraka是AP，并不是说euraka并没有实现一致性，只是euraka实现的一致性比zookeeper更弱而已，euraka集群也是实现最终一致性的。而zookeeper更强，保证了客户端提交的请求，都顺序的被整个集群处理。其中重要的区别是zookeeper基于ZAB协议若处于故障恢复阶段（比如网络分区，leader宕机，网络中断），此时会重新选举leader并进行数据同步，集群服务将不可用，而Euraka没有Leader概念，整个集群数据一致性是比较数据的时间戳，点对点复制，所以可用性更高。\n进程状态 looking 领导者还没诞生的情况 Following 跟随者的状态 Leading 领导者的状态 名词解释 epoch 标明leader的版本，每次leader变更都会进行提升 Zxid 事务id，高三十二位为epoch编号，低三十二位是客户端事务的简单递增数，每次一个新leader当选，会从新leader中取出最新的Zxid然后读出epoch值，然后加1，低三十二位则重新计数。 每个节点持久化的数据 history：当前节点接收到事务提议的 log acceptedEpoch：follower 已经接受的 leader 更改epoch的 NEWEPOCH 提议。 currentEpoch：当前所处的epoch lastZxid：history 中最近接收到的提议的 zxid （最大的） 协议流程 1.领导选举（Leader election） 协议并没有规定详细的选举算法，但在实现中使用的 Fast Leader Election(FLE) 选lastZxid最大的。\n2.发现（discovery） 由上一步得到一个准leader，准leader与其他节点建立连接，发送自己的epoch和lastZxid 其他节点回复currentEpoch(cepoch),以及提议集合。（思考，为什么不是lastZxid而是提议集合，这里存疑？） 准leader得到所有回复中的epoch，包括自己，然后比对所有的epoch，用最大的epoch+1通知其他节点更新currentEpoch。 3.同步（sync） 上面准leader在发现的时候收到了其他节点的提议集合，可以判断最新的提议集合，准leader将用最新的提议集合和其他节点进行同步，当过半节点都同步完成的时候，准leader才正式成为leader，follower 只会接收 zxid 比自己的 lastZxid 大的提议。\n4.广播(Broadcast) leader将client发过来的请求生成一个事务proposal，然后发送给Follower，多数Follower应答之后，Leader再发送Commit给全部的Follower让其进行提交。\n协议实现（与协议流程相区别） 1.Fast Leader Election 快速选举阶段 标准：\n选epoch最大的 epoch相等，选 zxid 最大的 epoch和zxid都相等，选择server id最大的（就是我们配置zoo.cfg中的myid） 先投票给自己，当接收其他节点的选票时，会根据上面的标准更改自己的选票并重新发送选票给其他节点，当有一个节点的得票超过半数，该节点会设置自己的状态为 leading，其他节点会设置自己的状态为 following。\n2.Recovery Phase 数据恢复阶段 相当于合并了协议流程中的发现和同步，因为经过FLE之后选举出来的准leader就是拥有最新提议历史（lastZxid最大）的节点。 fllower发送自己的lastZxid给leader，leader根据自己的lastZxid发送三种命令给fllower，使得fllower与leader的lastZxid保持一致 三种策略：\nSNAP 快照同步 fllower的lastZxid太老了，直接进行全量快照同步 DIFF 提案同步 不是很老，把fllower的lastZxid到leader的lastZxid之间的提案重新发给fllower进行同步 TRUNC 同步 超过了leader的lastZxid，发送指令让fllower lastZxid 到 leader lastZxid 之间的提案进行删除 fllower同步完成后发送ACK-LD给leader，leader才会将该fllower加入可用fllower列表。\n3.Broadcast Phase 广播阶段 leader将client发过来的请求生成一个事务proposal，然后发送给Follower，多数Follower应答之后，Leader再发送Commit给全部的Follower让其进行提交。\n三、Raft协议 角色 领导者 跟随者 候选人 每个节点持久化的数据 currentTerm 类似zab的currentEpoch term 类似zab的epoch 名词解释 复制状态机 不展开 raft协议的前提就是将所有的节点看做复制状态机模型 日志 由logEntry组成，包含term_number，command，任期号和指令。 选举 当没有收到leader心跳一段时间（随机时间），节点切换身份为候选人节点，开始发起选举，向其他节点并行请求其他节点给自己投票,请求数据包括term，和日志索引，同时把自己的选票投给自己，其他节点收到请求后，比对收到的请求数据中的term和日志索引是否与本地日志相比，哪一个更新，如果请求节点的更新，则返回响应将票投给该leader。\n上一步如何比较更新了？ 优先比较任期编号，其次比较日志索引\n若一段时间（随机时间）后没有收到大部分选票，则选举失败，反之成功。\n若在这段选举时间内收到其他leader发送的数据包，将比对数据包中的term，如果比自己大，则停止选举，切换为跟随者身份，反之继续选举。\n选举的结果：一定包含了所有已提交的日志, 从而避免那些没有含有所有已提交日志的结点成为候选人。\n约束 1.日志约束 领导者的日志只能增添而不能删除 日志数据的流向只能从领导人到其它结点, 不允许对日志项做删除或变更位置(改变索引)操作 任期+索引 能够确定一个唯一的logEntry 当领导人与跟随者的日志不相同时, 跟随者会用领导人的日志覆盖自身的 2. 安全性约束(Safety) 不允许领导人直接提交当前任期之前的日志， 而必须是先尝试提交当前任期的日志，即把当前任期的日志先尝试复制到其他大多数节点（这个过程不只是当前任期当前索引的日志会被复制，前面任期前面索引的日志都会复制到跟随者，并可能产生覆盖，反正一切以领导者日志为准）， 等到了提交这一步，判断当前任期日志之前的日志是否已经提交（节点会维护一个 commitIndex, 标志当前最新的已提交的日志项的索引),如果没有提交，则进行提交。 为什么这么做，假设可以提交，在直接提交当前任期之前的日志（这里称为preLog）之后，leader突然宕机，导致不含有preLog的节点，但最新日志中的任期号比较大的节点当选领导者（因为其他节点中的日志都没他新），他最新日志的索引可能和preLog相同，当要求日志同步的时候，则可能覆盖preLog，这是严重的错误，将导致已提交的日志被覆盖，所以限制了在提交preLog的时候，要求满足一个条件,即就算自己宕机，下一个领导者因为含有当前任期的日志所以会获得多数派节点的承认当选领导者，间接也会拥有preLog，保证preLog能够同步给其他节点。\n简而言之，一条非当期任期的logEntry就算已经拷贝到了大多数结点上也不会提交，除非在当前任期Term内也提交了一条日志，上一条日志才会被顺带提交。\n日志数据同步 领导者维护一个nextIndex列表，表示每个节点需要接受的下一个日志项的索引。在刚刚当选领导者的时候，所有节点的nextIndex为领导者最大的index+1。领导者给跟随者发送该节点nextIndex标识的日志项的时候也会携带上一个日志项的preTerm,preIndex，跟随者比对是否含有preTertm,preIndex的日志项，若没有将拒绝此次日志项同步，领导者则将nextIndex列表中，该节点的nextIndex-1,然后继续发送日志项，直到跟随者不拒绝为止。若此次日志项本节点已经有了，则该日志项将被覆盖，以领导者发过来的日志项为准。当集群中的大多数结点都获得了某一日志项副本之后, 领导人才会提交该日志，才能进行下一日志项的提交。\n日志压缩 通过快照的方式压缩日志 快照中的日志项必须是已提交过的, 集群中的各个结点自主生成已提交的日志项的快照, 领导人每隔一段时间也会将自己的快照广播发送给集群中的所有其它结点, 其它结点收到快照后会根据快照中说明的索引检查自己是否已含有此快照中的全部信息, 如果没有, 则删除自己全部的日志项, 然后应用这个快照, 若快照的最大索引小于自身的, 则结点简单地将快照对应的索引之前的所有日志项删除, 并应用这个快照, 而自身原有的在此索引之后的日志项仍然保留, 这样以来, 即便是新加入集群的结点, 也可以使用快照快速达到与领导人的同步。\n四、三者对比 选举触发 raft 跟随者没有没有收到leader心跳超时 zab 跟随者 领导者 都可以发起选举 领导者在没有收到过半跟随者心跳超时时触发 跟随者在心跳到不了领导者超时的时候触发 Paxos 与具体实现有关 basic-Paxos中并没有领导者 如何处理上一轮未提交的日志（幽灵复现） raft 先复制本轮日志到大部分节点，再提交上一轮日志 zab 采取激进的策略，对于所有过半还是未过半的日志都判定为提交，都将其应用到状态机中 Paxos 没有深入讨论 脑裂问题 raft 不会,就算网络分区产生脑裂，但也只有一个leader能提供服务，因为需要过半支持，而在重新选举的时候，服务不可用。 zab 不会,就算网络分区产生脑裂，但也只有一个leader能提供服务，因为需要过半支持，而在重新选举的时候，服务不可用。 Paxos 允许 多主 所以可能出现\n请求的连续性 raft 不可能先commit后面的日志，在commit前面的日志。 zab 也是连续的 通过锁和队列依次提交写请求的命令 Paxos 不连续 连leader都没有，每个节点都是平等的，如果要就某数据分歧达成一致都要走一次paxos 日志同步 raft leader维护各个节点的nextIndex和前一条日志preIndex,preTerm，各个节点比对是否有前一条相同任期，相同索引的日志，若没有则拒绝追加日志，leader则更新nextIndex-1,然后接着发，直到不拒绝为止。因为日志连续，所以能保证。 zab 在选举为准领导者的时候，直接进行recovery,以leader日志为准，不管提没提交，直接进行同步 Paxos 没有细讲，同步一次就是一次paxos协议 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2022-04-27T00:00:00Z","permalink":"https://runningccode.github.io/2022/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE-paxoszabraft-%E6%80%BB%E7%BB%93/","title":"一致性协议-Paxos,ZAB,Raft 总结"},{"content":" 来源于《深入理解JAVA内存模型》——程晓明加上个人理解，因为有些知识点查了很多还是不确定是什么意思，有个人理解在里面，仅供参考\n一、 JMM是什么 JAVA的并发编程模型，屏蔽硬件层，是一种高层次的抽象，把内存区域划分为主内存和本地内存（逻辑划分，不用对应某一块真实的区域），屏蔽硬件层，对多线程编程可能导致的有序性，内存可见性问题，提出的解决模型。\nJMM把内存抽象为主内存和本地内存，图示：\nimg.png\r注意:\n这里线程间共享变量，主要指堆以及元空间，栈中的信息不会在线程之间共享，不会有内存可见性问题，也不受JMM影响。 这里的主内存与本地内存的区分，主要是受缓存，写缓冲区，寄存器，以及其他硬件和编译器优化影响产生的区分，导致线程的本地内存和主内存存在差异。 二、 前置知识 数据依赖性 两个操作访问一个变量，其中一个操作为写，则两个操作有数据依赖性，比如写后读，写后写，读后写。\n控制依赖性 A和B操作存在控制依赖性，指的是B操作是否执行依赖于A的结果，编译器和处理器可能用猜测执行的方式对A操作和B操作进行重排，比如提前执行B操作获取B操作的结果存在一个临时变量里，当A有结果的时候再把这个临时变量赋值给应该被赋值的变量。\n单线程中，这样是没问题的，但多线程中，可能会改变程序的执行结果。\n重排序 java中的重排序，主要有以下几个原因：\nJIT编译重排序 JIT编译的时候会基于效率优化指令执行顺序。JIT指Java即时编译，针对热点代码 进行深层的机器码编译,够加快代码执行速度，因为一次访问一次解释执行的速度比不上直接执行编译后的机器码速度 指令重排序 cpu执行指令也可能不是按顺序执行的，会基于效率去优化执行顺序。 内存重排序 由于硬件写缓冲区 和 无效队列的存在，写没有完成写入到主存就执行下面的代码了，其他cpu对前面的指令执行无法感知，也是一种顺序错乱。（有些人说有缓存一致性协议，为什么还会产生内存重排序，其原因是MESI没有完全解决问题，因为后来硬件上又优化了写缓冲区和无效队列，这就只能使用内存屏障的汇编指令在编码层面进行控制了）。 后面两种指令重排序和内存重排序可以合并看为CPU重排序。\n注：任何重排序都会考虑数据依赖性，不会改变存在数据依赖性的两个操作的执行顺序，仅仅指单个线程或单个处理器中执行的操作的数据依赖性，不同处理器或不同线程之间产生的逻辑上的数据依赖性不被编译器和处理器考虑。\n重排序的结果：\n可能导致多线程程序出现内存可见性问题，上面的重排序虽然单线程/单处理器下不会出现问题，但没有考虑在多线程情况下，若两个线程有数据依赖性，则应该使用同步机制来避免。\n三、 JMM核心概念 happens-before原则 JSR-133（JMM规范提案）提出的概念，用以阐述操作之间的内存可见性，提供一个简单易懂的规则给程序员，用以判断两个操作之间的内存可见性情况，使得程序员不用关心内存可见性之下的原因，这些原因可能涉及复杂的重排序规则和这些规则的具体实现。内容包括：\n程序顺序规则：一个线程中的每个操作,happen-before与该线程中的任意后续操作（个人理解，就是代码书写下的程序顺序）。 volatile 变量规则，对于 volatile 修饰的变量的写的操作， 一定 happens-before 后续对于 volatile 变量的读操作; 监视器锁规则：对一个监视器的解锁，happens-before于随后对这个监视器的加锁（监视器锁，指的synchronized） 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 happen-before含义: 操作A happens-before 操作B 的意义是指 操作A的结果对操作B可见。意义只是操作A的结果对操作B可见。\n并不要求A一定先于B执行 当B不需要A的执行结果可见的时候（不存在数据依赖性），A和B还是可以被重排序，这样的重排序并不是非法的。\nas-if-serial语义 是一个语义，编译器，runtime,处理器都会遵守。指不管怎么重排序，程序的执行结果不能被改变。个人理解，产生数据依赖性的操作，顺序不能变，但没有数据依赖性的操作，顺序可能变。\n顺序一致性内存模型 是一个理论模型，JMM并不是顺序一致性模型，指一个线程的所有操作必须按程序的顺序来执行，所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。\nJMM与其的区别：\n不保证单线程的操作按照程序顺序执行 不保证所有线程看到一致的执行顺序 不保证64位变量写的原子性（32位系统） 但是JMM做了一个保证: 如果程序是正确同步的，程序的执行将具有顺序一致性-即程序的执行结果与该程序在顺序一致性模型中的执行结果相同。但JMM允许在不改变正确同步程序结果的情况下还是可以让编译器和cpu优化比如synchoronized保证临界区内编码的原子性不允许其内部的指令逸出临界区外，但可以在临界区内被重排序。\n四、JMM中保证正确同步的实现方式 注： 以下是个人理解的比较懵的地方，网上也没有找到统一答案，这里权当作个记录，仅供参考。\n主要指volatile，synchoronized等java提供的同步机制。他们使用到了内存屏障（注，这里的内存屏障和硬件层的内存屏障语义不一样，这里的内存屏障指的是JMM抽象的内存屏障）。这里主要介绍下volatile关键字，synchoronized留着后面在单独总结下吧。\nvolatile特性是保证内存可见性，保证单个变量读写的原子性。\n其中内存层面的语义是：读volatile将本地内存变量置为失效，从主内存读。写volatile把把本地内存共享变量同步至主内存。\n主要如何实现这个内存语义勒。我的理解是：\n写volatile的时候： v写的时候当前线程正常程序流的读写都已经完成，这样才能确保所有应该写入主内存的刷入主内存，其他线程如果读v，也能按顺序一致性模型来分析当前线程。 确保写完成的时候同步到主内存。 读volatile的时候： v读的时候正常程序流下后面所有的读都没有执行，这样才能确保v读之后读的数据都是主内存的。 同时应该避免v读之后的写重排到v读之前，确保当其他线程执行v写的时候不应该写入主内存的就还没有被写入。 前面已经说了重排序分为编译器重排和CPU重排。下面说说JMM是怎么解决这两个重排的。\n编译器重排 主要是基于重排序规则如下图：\nimg_1.png\r注：第一个操作 第二个操作并不是指相邻的两个操作，而是指单线程前后的两个操作。\n从图中可以看到：\n当第二个操作是volatile写的时候，不管第一个操作是什么，都不能重排到volatile写之后。 当第一个操作是volatile读的时候，不管第二个操作是什么，都不能重排到volaite读之前。 当第一个操作是volatile写，第二个操作是volatile读的时候，不能重排序。 个人理解：\n使得当构成线程通信的时候（A线程volatile写，B线程volatile读），这个时机的触发，可以按照顺序一致性内存模型分析A线程，即B线程可以理解A线程volatile写之前的指令已经执行完毕，A线程可以理解B线程volatile读之后的命令还没有执行。 这里保证了编译器重排序不会影响可见性（默认变量的任意读写在各个线程的本地内存是没有差异的），因为引起各个线程本地内存差异的原因是cpu重排序，所以要解决这个问题还得在cpu重排序的层面引入内存屏障解决cpu重排序。 CPU重排序 主要是引入了内存屏障，内存屏障是java在编译生成字节码的时候插入的，并没有具体到硬件层面，他是属于JMM中抽象概念，不同处理器，实现可能略有不同，有四种：\nLoadLoad屏障 顺序控制语义： 前面的Load和后面的Load不能乱序。\nStoreStore屏障 顺序控制语义： 前面的Store和后面的Store不能乱序。 刷新主存控制语义：前面的Store在后面的Store之前也会把线程本地内存刷新到主内存。\nLoadStore屏障 顺序控制语义： 前面的Load和后面的Store不能乱序。 刷新主存控制语义： 确保后一个Store刷新主存之前执行Load。\nStoreLoad屏障 顺序控制语义：前面的Store和后面的Load不能乱序。 刷新主存控制语义： Store完成后会把线程本地内存刷新到主内存。\n以上是个人理解含有的语义，可能会有偏差，下面贴出书中的概念：\nimg_2.png\r另外，个人觉得前后的Load,Store,应该是以多个线程而不是单个线程内来说的把。比如StoreLoad之后，Load不光指本线程的Load，也指其他线程的Load把，如果不是这样，感觉解释不通。\nvolaite读写插入的内存屏障 其实这部分，为什么这么插入，自己也不是理解得很透彻，有些疑问，所以就大概按照书中的介绍写把，不作过多介绍。\nvolatile写：\nimg_3.png\r在每个volatile写操作前插入StoreStore屏障，在写操作后插入StoreLoad屏障；\n疑问：为什么在前面不加LoadStore屏障？（编译器重排序规则明明写了普通读和volatile写不能重排）\nvolatile读：\nimg_4.png\r在每个volatile读操作后插入LoadLoad屏障，以及LoadStore屏障；\n注： 上面的内存屏障过于保守，在操作系统差异，还有某些不必要的情况下比如屏障重复，编译器会根据情况作优化，省略屏障。\n最后：感觉自己还是太菜了，看了这个概念纠结了一星期左右，还是不太明白，但还是做个记录把。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2022-03-04T00:00:00Z","permalink":"https://runningccode.github.io/2022/jmmjava-memory-modeljava%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E8%A7%84%E8%8C%83/","title":"JMM（java-memory-model,java内存模型）规范"},{"content":"本文主要源于阅读以下文章整理所得：\n似水流年——一篇文章读懂阻塞，非阻塞，同步，异步\n似水流年——IO多路复用的三种机制Select，Poll，Epoll\n罗培羽——epoll 的本质是什么？\n理解IO模型 《Unix网络编程》将IO模型分为：\nBlocking IO - 阻塞IO NoneBlocking IO - 非阻塞IO IO multiplexing - IO多路复用 ignal driven IO - 信号驱动IO asynchronous IO - 异步IO 在理解IO模型之前需要了解数据从网络传输再到应用程序接受到数据的过程，这个过程可以分为两个阶段：\n第一阶段：系统内核准备数据阶段 数据从网卡写入内核 dma 第二阶段：copy阶段 数据从内核copy到用户进程 阻塞IO img.png\r指应用调用recvfrom是必须阻塞等待两个阶段都完成才会return\n非阻塞IO img_1.png\r应用调用recvfrom 若第一阶段未完成会返回 no datagram ready,等第一阶段完成之后应用程序调用recvfrom才会返回数据。\nIO多路复用 img_2.png\r应用有一个线程处理监听第一阶段数据到没有（可以同时监听多个socket不像Bio一样只能监听一个socket），多路复用就是指的所有的socket都用一个线程来监听第一阶段数据就绪没有。另外，虽然select仍然是阻塞的，但只有在所有channel都没就绪的时候才会阻塞。\nlinux中的监听机制：\nselect poll epoll 后面再剖析这三者的区别。\n异步IO img_3.png\r应用程序 通过aio_read 调用内核，这里的含义是通知内核进行读取操作并将数据拷贝至进程中，完事后通知进程整个操作全部完成（绑定一个回调函数处理数据）。读取操作会立刻返回，程序可以进行其它的操作，所有的读取、拷贝工作都由内核去做，做完以后通知进程，进程调用绑定的回调函数来处理数据。\n对比： img_4.png\r理解同步/异步，阻塞/非阻塞 这是linux比较权威划分的IO模型，但是JAVA面试时经常问到的是BIO,NIO,AIO,Netty。然后说BIO是同步阻塞，NIO是同步非阻塞，AIO是异步非阻塞。怎么理解这些关系了？\n同步/异步 同步指的是用户进程使用数据，需要同步等待。 异步指的是用户进程使用数据的时候传入回调，内核会通知并唤醒回调方法。\n阻塞/非阻塞 阻塞/非阻塞是针对上述第一阶段来说的。 阻塞指的是应用需要阻塞等待第一阶段数据就绪，然后就绪之后才copy到用户空间。。 非阻塞指的是应用在数据没就绪的情况，立马可以得到返回，或者通过IO多路复用，得到就绪的Channel\n所以NIO,BIO都是同步的，因为在读数据的时候，应用程序都是阻塞的。而不像AIO一样提供一个缓冲区给OS，OS直接就把数据copy到缓冲区了，然后回调应用程序。比如下面AIO的JAVA代码：\npackage com.cman777.springc.sample.test; import lombok.SneakyThrows; import java.io.IOException; import java.net.InetSocketAddress; import java.nio.ByteBuffer; import java.nio.channels.AsynchronousServerSocketChannel; import java.nio.channels.AsynchronousSocketChannel; import java.nio.channels.CompletionHandler; /** * @author chenzhicong * @time 2022/2/17 14:36 */ public class Aio { public static class AIOEchoServer { public final static int PORT = 8001; public final static String IP = \u0026quot;127.0.0.1\u0026quot;; private AsynchronousServerSocketChannel server = null; public AIOEchoServer() { try { //同样是利用工厂方法产生一个通道，异步通道 AsynchronousServerSocketChannel server = AsynchronousServerSocketChannel.open().bind(new InetSocketAddress(IP, PORT)); } catch (IOException e) { e.printStackTrace(); } } //使用这个通道(server)来进行客户端的接收和处理 public void start() { System.out.println(\u0026quot;Server listen on \u0026quot; + PORT); //注册事件和事件完成后的处理器，这个CompletionHandler就是事件完成后的处理器 server.accept(null, new CompletionHandler\u0026lt;AsynchronousSocketChannel, Object\u0026gt;() { @Override @SneakyThrows public void completed(AsynchronousSocketChannel result, Object attachment) { //连接完成之后的回调 System.out.println(\u0026quot;completed1\u0026quot;); final ByteBuffer buffer = ByteBuffer.allocate(1024); System.out.println(Thread.currentThread().getName()); result.read(buffer, null, new CompletionHandler\u0026lt;Integer, Object\u0026gt;() { @SneakyThrows @Override public void completed(Integer resultRead, Object attachment) { //读取数据完成之后的回调 //将数据写回客户端 System.out.println(\u0026quot;completed2\u0026quot;); System.out.println(\u0026quot;In server: \u0026quot; + new String(buffer.array())); buffer.flip(); result.write(buffer).get(); result.close(); System.out.println(\u0026quot;completed2finish\u0026quot;); } @Override public void failed(Throwable exc, Object attachment) { } }); //连接成功再接受一个连接 server.accept(null,this); System.out.println(\u0026quot;completed1finish\u0026quot;); } @Override public void failed(Throwable exc, Object attachment) { System.out.println(\u0026quot;failed:\u0026quot; + exc); } }); } public static void main(String[] args) { new AIOEchoServer().start(); while (true) { try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } } } public static class AIOClient { public static void main(String[] args) throws IOException { final AsynchronousSocketChannel client = AsynchronousSocketChannel.open(); InetSocketAddress serverAddress = new InetSocketAddress(\u0026quot;127.0.0.1\u0026quot;, 8001); CompletionHandler\u0026lt;Void, ? super Object\u0026gt; handler = new CompletionHandler\u0026lt;Void, Object\u0026gt;() { @Override public void completed(Void result, Object attachment) { //建立连接成功的回调 System.out.println(\u0026quot;completed1\u0026quot;); client.write(ByteBuffer.wrap(\u0026quot;Hello\u0026quot;.getBytes()), null, new CompletionHandler\u0026lt;Integer, Object\u0026gt;() { @Override public void completed(Integer result, Object attachment) { //写数据完成之后的回调 System.out.println(\u0026quot;completed2\u0026quot;); final ByteBuffer buffer = ByteBuffer.allocate(1024); client.read(buffer, null, new CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt;() { @SneakyThrows @Override public void completed(Integer result, ByteBuffer attachment) { //读数据成功之后的回调 System.out.println(\u0026quot;completed3\u0026quot;); buffer.flip(); System.out.println(new String(buffer.array())); client.close(); System.out.println(\u0026quot;completed3finish\u0026quot;); } @Override public void failed(Throwable exc, ByteBuffer attachment) { } }); System.out.println(\u0026quot;completed2finish\u0026quot;); } @Override public void failed(Throwable exc, Object attachment) { } }); System.out.println(\u0026quot;completed1finish\u0026quot;); } @Override public void failed(Throwable exc, Object attachment) { } }; client.connect(serverAddress, null, handler); try { Thread.sleep(10000000); } catch (InterruptedException e) { e.printStackTrace(); } } } } 再附一个NIO的JAVA代码,可作对比：\npackage com.cman777.springc.sample.test; import java.io.IOException; import java.io.OutputStreamWriter; import java.io.PrintWriter; import java.net.InetSocketAddress; import java.net.Socket; import java.nio.ByteBuffer; import java.nio.channels.SelectionKey; import java.nio.channels.Selector; import java.nio.channels.ServerSocketChannel; import java.nio.channels.SocketChannel; import java.nio.charset.Charset; import java.util.Iterator; import java.util.Set; /** * @author chenzhicong * @time 2022/2/17 10:53 */ public class Nio { public static void main(String[] args) throws IOException { Selector serverSelector = Selector.open(); Selector clientSelector = Selector.open(); new Thread(() -\u0026gt; { try { // 对应IO编程中服务端启动 ServerSocketChannel listenerChannel = ServerSocketChannel.open(); listenerChannel.socket().bind(new InetSocketAddress(8000)); listenerChannel.configureBlocking(false); listenerChannel.register(serverSelector, SelectionKey.OP_ACCEPT); while (true) { // 监测是否有新的连接，这里的1指的是阻塞的时间为 1ms if (serverSelector.select(1) \u0026gt; 0) { Set\u0026lt;SelectionKey\u0026gt; set = serverSelector.selectedKeys(); Iterator\u0026lt;SelectionKey\u0026gt; keyIterator = set.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isAcceptable()) { try { // (1) 每来一个新连接，不需要创建一个线程，而是直接注册到clientSelector SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept(); clientChannel.configureBlocking(false); clientChannel.register(clientSelector, SelectionKey.OP_READ); } finally { keyIterator.remove(); } } } } } } catch (IOException ignored) { } }).start(); new Thread(() -\u0026gt; { try { while (true) { // (批量轮询是否有哪些连接有数据可读，这里的1指的是阻塞的时间为 1ms if (clientSelector.select(1) \u0026gt; 0) { Set\u0026lt;SelectionKey\u0026gt; set = clientSelector.selectedKeys(); Iterator\u0026lt;SelectionKey\u0026gt; keyIterator = set.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isReadable()) { try { SocketChannel clientChannel = (SocketChannel) key.channel(); //面向byteBuffer ByteBuffer byteBuffer = ByteBuffer.allocate(1024); int readByteNum = clientChannel.read(byteBuffer); if(readByteNum == -1){ clientChannel.close(); continue; } byteBuffer.flip(); System.out.println(Charset.defaultCharset().newDecoder().decode(byteBuffer) .toString()); } finally { if(key.isValid()){ keyIterator.remove(); key.interestOps(SelectionKey.OP_READ); } } } } } } } catch (IOException ignored) { } }).start(); new Thread(() -\u0026gt; { try { while (true){ Thread.sleep(3000); Socket socket = new Socket(\u0026quot;127.0.0.1\u0026quot;,8000); PrintWriter printWriter = new PrintWriter(socket.getOutputStream()); printWriter.print(\u0026quot;你好\u0026quot;); printWriter.close(); socket.close(); } } catch (IOException ignored) { } catch (InterruptedException e) { e.printStackTrace(); } }).start(); } } select/poll/epoll理解 基础知识 需了解：\nsocket结构 内核接收网络数据过程 socket结构 分为发送缓冲区，接受缓冲区，等待队列（等待被唤醒的进程（阻塞状态））。\n内核接收网络数据全过程：\nimg_5.png\r计算机收到了对端传送的数据(步骤 ①) 数据经由网卡传送到内存(步骤 ②) 然后网卡通过中断信号通知 CPU 有数据到达，CPU 执行中断程序(步骤 ③)。\n此处的中断程序主要有两项功能，先将网络数据写入到对应 Socket 的接收缓冲区里面(步骤 ④)，再唤醒进程 A(步骤 ⑤)，重新将进程 A 放入工作队列中。\nselect 传入文件描述符集合 线程加入所有socket的等待队列 等待任意一个socket数据从王卡写入内核完毕 完毕之后 将唤醒线程 select将返回就绪socket数量\n应用程序接着在遍历检查是哪个socket有数据 再接受数据\npoll 跟select差不多，但select对文件描述符集合有限制。poll无限制，效率没有提升。\nepoll 提供3个函数：\nepoll_create epoll_ctl epoll_wait epoll_create 作用是让内核创建eventpoll 对象。eventpoll 对象包括的数据有需要监听的socket_fd监听符（红黑树结构），就绪列表（rdList,就绪状态的socket信息，双向链表） ，等待队列（把调用epollwait函数的进程放到这里面去）。\nepoll_ctl 作用是删减监听的socket。\nepoll_wait 作用是检查就绪列表，返回就绪socket数量， 同时把就绪socket信息copy到传入的数组指针里面的数组上 所以与select不同的是不需要遍历检查socket，而只是直接去查询数组就知道那些socket就绪。\nepoll流程： epoll_create 函数添加socket监视，内核会把eventpoll添加到socket的等待队列当中。数据从socket写入内核空间完毕后,传送中断信号给cpu，cpu执行中断程序，中断程序将socket的等待队列中的eventpoll中的就绪列表添加socket引用，然后唤醒eventpoll中等待队列中的进程。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2022-02-18T00:00:00Z","permalink":"https://runningccode.github.io/2022/bio/nio-%E5%90%8C%E6%AD%A5/%E5%BC%82%E6%AD%A5-%E9%98%BB%E5%A1%9E/%E9%9D%9E%E9%98%BB%E5%A1%9E-select/poll/epoll-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/","title":" BIO/NIO 同步/异步 阻塞/非阻塞 select/poll/epoll 知识点整理"},{"content":" 根据数据结构总结jdk常见的非并发容器，小结。\n数据结构 基于数组 ArrayList ArrayDeque(循环数组) 只需要双端队列效果时使用，比linkedList还要快 不提供索引方法 判断元素存不存在需遍历 效率低 EnumMap 特殊的 专门提供给键为enum使用的map 数组的索引和枚举的ordinal对应，get方法直接是先把枚举的ordinal获取到然后从数组里面获取值。 基于链表 应用场景： 要求有序，两端访问情况多\nLinkedList 可作队列（双端队列） 可作List 基于hash（数组+链表（红黑树）） 应用场景：不需要重复元素，随机访问\nhashMap 在并发操作中扩容操作容易形成环，引起死循环，应当使用并发容器ConcurrentHashMap hashSet 基于排序二叉树 应用场景：需要有序\nTreeMap TreeSet 基于链表+hash 应用场景：需要实现访问排序\nLinkedHashMap LinkedHashSet 基于位向量 EnumSet 原理: 使用long 64位 保存枚举集合，二进制中的一个位表示一个元素的两种状态，0表示不包含该枚举值，1表示包含该枚举值。 两个实现RegularEnumSet(一个long保存数据)，JumboEnumSet（long数组保存） BitSet 可以方便地对指定位置的位进行操作，与其他位向量进行位运算。 原理：内部使用long数组存储位向量，构造函数传入需要构造的位向量的位数。 基于堆（完全二叉树） 概念 要求最后一层不一定是满的，但要求最后一层几点从左到右是连续的，不能间隔 种类 分为最大堆（根节点的值最大）与最小堆（根节点的值最小） 存储结构 数组，因为堆是完全二叉树，所以每个节点的位置可以对应数组的下标 PriorityQueue 优先级队列 单端队列 应用：实时求中间值，实时求最大或最小值，优先级任务队列。 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2022-02-16T00:00:00Z","permalink":"https://runningccode.github.io/2022/jdk%E5%B8%B8%E7%94%A8%E9%9D%9E%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8%E6%95%B4%E7%90%86/","title":"jdk常用非并发容器整理"},{"content":" 闲来无事，复习一遍以前学习《Java编程的逻辑》的笔记，巩固一下基础。里面提到了通过堆这个数据结构取中值的应用场景，但没给出具体解法，这里查阅了相关资料后自己写了一个求中值的容器。 img.png\r堆的概念 堆就是完全二叉树，要求最后一层不一定是满的，但要求最后一层几点从左到右是连续的，不能间隔。形如： img_1.png\r堆的种类 分为最大堆（根节点的值最大）,最小堆（根节点的值最小）\n堆的存储结构 可以用数组存储，因为堆是完全二叉树，所以每个节点的位置可以对应数组的下标。\n堆的实现 jdk里面基于堆的逻辑结构的实现类是PriorityQueue（优先级队列，单端队列）。\n代码部分 PriorityQueue应用场景可以为：\n求中间值 一个最大堆 一个最小堆 实时求最大或最小值 构建优先级任务队列 个人对求中间值不是很理解，所以根据网上资料自己尝试写了实时求中间值的容器，因为关注点在于理解怎么求中间值，所以没有实现jdk容器接口，只实现实时求中间值。\n下面贴出代码：\n容器代码部分：\npackage com.cman777.springc.common.helper; import com.cman777.springc.common.bean.exception.BusinessException; import java.util.Collections; import java.util.Comparator; import java.util.PriorityQueue; import java.util.function.BiFunction; /** * @author chenzhicong * @time 2022/2/16 14:29 * 可以动态获取中值的容器 */ public class MiddleContainer\u0026lt;T\u0026gt; { private PriorityQueue\u0026lt;T\u0026gt; maxP = null; private PriorityQueue\u0026lt;T\u0026gt; minP = null; private Comparator\u0026lt;T\u0026gt; comparator = null; private int count = 0; private T middle = null; /** * 容器为偶数个的时候中值怎么求的函数 */ private BiFunction\u0026lt;T, T, T\u0026gt; mergeFunction = null; public MiddleContainer(Class\u0026lt;T\u0026gt; tClass, BiFunction\u0026lt;T, T, T\u0026gt; mergeFunction) { ValidateUtil.isTrue(Comparable.class.isAssignableFrom(tClass), \u0026quot;class参数必须实现Comparable接口\u0026quot;); maxP = new PriorityQueue(Collections.reverseOrder()); minP = new PriorityQueue(); this.mergeFunction = mergeFunction; } public MiddleContainer(Comparator\u0026lt;T\u0026gt; tComparator, BiFunction\u0026lt;T, T, T\u0026gt; mergeFunction) { maxP = new PriorityQueue\u0026lt;\u0026gt;(tComparator.reversed()); minP = new PriorityQueue\u0026lt;\u0026gt;(tComparator); this.mergeFunction = mergeFunction; } /** * 效果：确保最小堆是大于或等于中间值的数 * 最大堆是小于或等于中间值的数 * 并且 最大堆最小堆之间的size相差不小于1 * 则中间值就是最大/最小堆的peek值或两者peek的平均数 */ public T add(T t) { if (count == 0) { maxP.offer(t); middle = t; count++; return t; } //如果当前添加值小于中间值 添加到最大堆 int compareResult = compare(t, middle); if (compareResult \u0026gt; 0) { //大于中间值 添加到最小堆 minP.offer(t); } else if (compareResult \u0026lt; 0) { //小于中间值 添加到最大堆 maxP.offer(t); } else { //与中间值相等 添加到任意一个堆 maxP.offer(t); } //校验当前最大堆和最小堆size相差不超过1 if (maxP.size() - minP.size() \u0026gt;= 2) { //从最大堆poll一个值到最小堆 T tempVal = maxP.poll(); minP.offer(tempVal); } else if (minP.size() - maxP.size() \u0026gt;= 2) { //从最小堆poll一个值到最大堆 T tempVal = minP.poll(); maxP.offer(tempVal); } count++; //返回中间值 middle = getMiddle(); return middle; } public int size() { return count; } public T getMiddle() { if (count \u0026lt;= 1) { return maxP.peek(); } if (maxP.size() == minP.size()) { return mergeFunction.apply(maxP.peek(), minP.peek()); } if (maxP.size() - minP.size() == 1) { return maxP.peek(); } if (minP.size() - maxP.size() == 1) { return minP.peek(); } throw new BusinessException(\u0026quot;数据错误\u0026quot;); } public int compare(T t1, T t2) { if (comparator == null) { return ((Comparable\u0026lt;T\u0026gt;) t1).compareTo(t2); } else { return comparator.compare(t1, t2); } } } 测试类代码：\npackage com.cman777.springc.sample.test; import com.cman777.springc.common.helper.MiddleContainer; /** * @author chenzhicong * @time 2021/10/19 9:52 */ public class TestMain { private static int[] array = {1, 2, 3, 3, 3, 4, 4, 5, 6}; public static void main(String[] args) { MiddleContainer\u0026lt;Double\u0026gt; middleContainer = new MiddleContainer\u0026lt;\u0026gt;(Double.class, (a, b) -\u0026gt; (a + b) / 2); for (int i = 0; i \u0026lt; array.length; i++) { middleContainer.add((double) array[i]); } System.out.println(middleContainer.getMiddle()); } } 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2022-02-16T00:00:00Z","permalink":"https://runningccode.github.io/2022/priorityqueue%E5%8F%96%E4%B8%AD%E4%BD%8D%E6%95%B0-%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E8%83%BD%E5%A4%9F%E5%8A%A8%E6%80%81%E5%8F%96%E4%B8%AD%E4%BD%8D%E6%95%B0%E7%9A%84%E5%AE%B9%E5%99%A8/","title":"PriorityQueue取中位数-实现一个能够动态取中位数的容器"},{"content":"BUG 前段时间写了个幂等框架 环绕通知是这么写的\n@Around(\u0026quot;idempotence()\u0026quot;) public Object around(ProceedingJoinPoint proceedingJoinPoint) throws Throwable { //略 boolean isExists = !idempotenceClient.saveIfAbsent(idempotenceId, () -\u0026gt; { ResultWrapper resultWrapper = new ResultWrapper(); Object result = null; try { result = proceedingJoinPoint.proceed(args); } catch (Throwable e) { resultWrapper.setHasException(true); resultWrapper.setException(e); } try { resultWrapper.setResult((Serializable) result); } catch (ClassCastException e) { resultType[0] = true; log.error(\u0026quot;class={}必须实现序列化接口\u0026quot;, result.getClass().getName()); } resultWrapperFlag[0]=resultWrapper; return resultWrapper; }); //略 } } 其中proceedingJoinPoint.proceed(args)是切入点方法逻辑，里面调用了AopContext.currentProxy()获取当前类的代理类。但是获取的代理类却有点奇怪，不能强转为我想象中的被切入对象。缘由是我的切入点逻辑被包含在idempotenceClient#saveIfAbsent方法中去回调了，但saveIfAbsent也有切面，springboot设置当前代理类是在JdkDynamicAopProxy（JDK代理对象）或CglibAopProxy.DynamicAdvisedInterceptor （CGLIB代理对象）的invoke或intercept方法里面。这里以JdkDynamicAopProxy#invoke方法为例：\npublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable{ //略 if (this.advised.exposeProxy) { // Make invocation available if necessary. oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; } //略 } AopContext：\nprivate static final ThreadLocal\u0026lt;Object\u0026gt; currentProxy = new NamedThreadLocal\u0026lt;\u0026gt;(\u0026quot;Current AOP proxy\u0026quot;); @Nullable static Object setCurrentProxy(@Nullable Object proxy) { Object old = currentProxy.get(); if (proxy != null) { currentProxy.set(proxy); } else { currentProxy.remove(); } return old; } public static Object currentProxy() throws IllegalStateException { Object proxy = currentProxy.get(); if (proxy == null) { throw new IllegalStateException( \u0026quot;Cannot find current proxy: Set 'exposeProxy' property on Advised to 'true' to make it available, and \u0026quot; + \u0026quot;ensure that AopContext.currentProxy() is invoked in the same thread as the AOP invocation context.\u0026quot;); } return proxy; } 可以看到，AopContext.currentProxy()取到代理类实际上就是在一个线程变量里面取，而这个线程变量在执行代理类方法的时候被设置进去,所以，我这里取被代理对象只能取到当前被代理对象，而由于当前方法最近的切面是idempotenceClient这个类生成的切面，所以只能取到idempotenceClient的代理类。说起来有点绕。\n总结 当在切面逻辑方法中用另一个切面方法来回调proceedingJoinPoint.proceed(args)方法的时候，需要注意AopContext.currentProxy()取到的是另一个切面方法对应的代理类对象。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2021-08-03T00:00:00Z","permalink":"https://runningccode.github.io/2021/bug%E6%96%B9%E6%B3%95-a-%E6%9C%89%E7%8E%AF%E7%BB%95%E5%88%87%E9%9D%A2-a-%E5%8C%85%E5%90%AB%E6%96%B9%E6%B3%95-b-%E6%9C%89%E7%8E%AF%E7%BB%95%E5%88%87%E9%9D%A2-b%E7%84%B6%E5%90%8E%E5%9C%A8-a-%E5%88%87%E9%9D%A2%E8%B0%83%E7%94%A8-b-%E6%96%B9%E6%B3%95b-%E6%96%B9%E6%B3%95%E5%9B%9E%E8%B0%83-a-%E6%96%B9%E6%B3%95%E7%9A%84%E5%88%87%E5%85%A5%E7%82%B9%E6%96%B9%E6%B3%95%E5%AF%BC%E8%87%B4%E7%9A%84-aopcontext.currentproxy-%E6%8B%BF%E4%B8%8D%E5%88%B0%E6%96%B9%E6%B3%95-a-%E7%9A%84%E4%BB%A3%E7%90%86%E7%B1%BB/","title":"BUG：方法 A 有环绕切面 a 包含方法 B 有环绕切面 b，然后在 a 切面调用 b 方法，B 方法回调 A 方法的切入点方法，导致的 AopContext.currentProxy () 拿不到方法 A 的代理类"},{"content":"缘由 生产上发生了事故，用户服务其中一台服务很多情况下都不可用，于是查询日志，发现了在从redis获取用户信息缓存的时候出现了错误，栈如下：\norg.springframework.data.redis.RedisSystemException: Unknown redis exception; nested exception is java.util.concurrent.RejectedExecutionException: event executor terminated at org.springframework.data.redis.FallbackExceptionTranslationStrategy.getFallback(FallbackExceptionTranslationStrategy.java:53) at org.springframework.data.redis.FallbackExceptionTranslationStrategy.translate(FallbackExceptionTranslationStrategy.java:43) at org.springframework.data.redis.connection.lettuce.LettuceConnection.convertLettuceAccessException(LettuceConnection.java:270) at org.springframework.data.redis.connection.lettuce.LettuceStringCommands.convertLettuceAccessException(LettuceStringCommands.java:799) at org.springframework.data.redis.connection.lettuce.LettuceStringCommands.set(LettuceStringCommands.java:180) at org.springframework.data.redis.connection.DefaultedRedisConnection.set(DefaultedRedisConnection.java:288) at com.hyzh.common.redis.util.RedisDistributedLock.lambda$setRedis$0(RedisDistributedLock.java:152) at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:228) at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:188) at com.hyzh.common.redis.util.RedisDistributedLock.setRedis(RedisDistributedLock.java:152) at com.hyzh.common.redis.util.RedisDistributedLock.lock(RedisDistributedLock.java:71) at com.hyzh.common.redis.aspect.DistributedLockAspect.around(DistributedLockAspect.java:70) at sun.reflect.GeneratedMethodAccessor748.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:644) at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethod(AbstractAspectJAdvice.java:633) at org.springframework.aop.aspectj.AspectJAroundAdvice.invoke(AspectJAroundAdvice.java:70) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:175) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:749) at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:95) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:749) at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:691) at com.hyzh.unistars.user.service.impl.UserActionServiceImpl$$EnhancerBySpringCGLIB$$59bb712b.findActionConfigByCode(\u0026lt;generated\u0026gt;) at com.hyzh.unistars.user.service.impl.UserActionServiceImpl.findActionConfigByCode(UserActionServiceImpl.java:160) at com.hyzh.unistars.user.service.impl.UserActionServiceImpl$$FastClassBySpringCGLIB$$5acd3627.invoke(\u0026lt;generated\u0026gt;) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:771) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:749) at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:95) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:749) at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:691) at com.hyzh.unistars.user.service.impl.UserActionServiceImpl$$EnhancerBySpringCGLIB$$59bb712b.findActionConfigByCode(\u0026lt;generated\u0026gt;) at com.hyzh.unistars.user.service.impl.UserTaskServiceImpl.addUserAccountLogsByUserIdsAndCodes(UserTaskServiceImpl.java:505) at org.apache.dubbo.common.bytecode.Wrapper53.invokeMethod(Wrapper53.java) at org.apache.dubbo.rpc.proxy.javassist.JavassistProxyFactory$1.doInvoke(JavassistProxyFactory.java:47) at org.apache.dubbo.rpc.proxy.AbstractProxyInvoker.invoke(AbstractProxyInvoker.java:84) at org.apache.dubbo.config.invoker.DelegateProviderMetaDataInvoker.invoke(DelegateProviderMetaDataInvoker.java:56) at org.apache.dubbo.rpc.protocol.InvokerWrapper.invoke(InvokerWrapper.java:56) at org.apache.dubbo.monitor.support.MonitorFilter.invoke(MonitorFilter.java:92) at org.apache.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke(ProtocolFilterWrapper.java:82) at org.apache.dubbo.rpc.protocol.dubbo.filter.TraceFilter.invoke(TraceFilter.java:81) at org.apache.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke(ProtocolFilterWrapper.java:82) at org.apache.dubbo.rpc.filter.TimeoutFilter.invoke(TimeoutFilter.java:48) at org.apache.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke(ProtocolFilterWrapper.java:82) at com.hyzh.common.dubbo.ExceptionFilter.invoke(ExceptionFilter.java:33) at org.apache.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke(ProtocolFilterWrapper.java:82) at com.alibaba.csp.sentinel.adapter.dubbo.SentinelDubboProviderFilter.invoke(SentinelDubboProviderFilter.java:73) at org.apache.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke(ProtocolFilterWrapper.java:82) at com.hyzh.common.dubbo.DistributedLogTracerInterceptor.invoke(DistributedLogTracerInterceptor.java:41) at org.apache.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke(ProtocolFilterWrapper.java:82) at org.apache.dubbo.rpc.filter.ContextFilter.invoke(ContextFilter.java:96) at org.apache.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke(ProtocolFilterWrapper.java:82) at org.apache.dubbo.rpc.filter.GenericFilter.invoke(GenericFilter.java:148) at org.apache.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke(ProtocolFilterWrapper.java:82) at org.apache.dubbo.rpc.filter.ClassLoaderFilter.invoke(ClassLoaderFilter.java:38) at org.apache.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke(ProtocolFilterWrapper.java:82) at org.apache.dubbo.rpc.filter.EchoFilter.invoke(EchoFilter.java:41) at org.apache.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke(ProtocolFilterWrapper.java:82) at org.apache.dubbo.rpc.protocol.ProtocolFilterWrapper$CallbackRegistrationInvoker.invoke(ProtocolFilterWrapper.java:157) at org.apache.dubbo.rpc.protocol.dubbo.DubboProtocol$1.reply(DubboProtocol.java:152) at org.apache.dubbo.remoting.exchange.support.header.HeaderExchangeHandler.handleRequest(HeaderExchangeHandler.java:102) at org.apache.dubbo.remoting.exchange.support.header.HeaderExchangeHandler.received(HeaderExchangeHandler.java:193) at org.apache.dubbo.remoting.transport.DecodeHandler.received(DecodeHandler.java:51) at org.apache.dubbo.remoting.transport.dispatcher.ChannelEventRunnable.run(ChannelEventRunnable.java:57) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.util.concurrent.RejectedExecutionException: event executor terminated at io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:926) at io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:353) at io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:346) at io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:828) at io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:818) at io.netty.util.concurrent.AbstractScheduledEventExecutor.schedule(AbstractScheduledEventExecutor.java:261) at io.netty.util.concurrent.AbstractScheduledEventExecutor.schedule(AbstractScheduledEventExecutor.java:177) at io.netty.util.concurrent.AbstractEventExecutorGroup.schedule(AbstractEventExecutorGroup.java:50) at io.netty.util.concurrent.AbstractEventExecutorGroup.schedule(AbstractEventExecutorGroup.java:32) at io.lettuce.core.protocol.CommandExpiryWriter.potentiallyExpire(CommandExpiryWriter.java:164) at io.lettuce.core.protocol.CommandExpiryWriter.write(CommandExpiryWriter.java:111) at io.lettuce.core.RedisChannelHandler.dispatch(RedisChannelHandler.java:187) at io.lettuce.core.StatefulRedisConnectionImpl.dispatch(StatefulRedisConnectionImpl.java:171) at io.lettuce.core.AbstractRedisAsyncCommands.dispatch(AbstractRedisAsyncCommands.java:467) at io.lettuce.core.AbstractRedisAsyncCommands.set(AbstractRedisAsyncCommands.java:1213) at sun.reflect.GeneratedMethodAccessor749.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at io.lettuce.core.FutureSyncInvocationHandler.handleInvocation(FutureSyncInvocationHandler.java:57) at io.lettuce.core.internal.AbstractInvocationHandler.invoke(AbstractInvocationHandler.java:80) at com.sun.proxy.$Proxy219.set(Unknown Source) at org.springframework.data.redis.connection.lettuce.LettuceStringCommands.set(LettuceStringCommands.java:178) ... 66 common frames omitted 同时，也发现了在用户服务也报了内存溢出日志如下：\nDecember 9th 2020, 12:35:16.041\t2020-12-09 12:25:52,956 [WARN ] [lettuce-eventExecutorLoop-3-2] [] [i.n.u.c.SingleThreadEventExecutor] Unexpected exception from an event executor: java.lang.OutOfMemoryError: GC overhead limit exceeded 于是开始重点分析SingleThreadEventExecutor这个类，这个类是netty中eventloop的父类，所以可以理解为他的作用就是轮询读写事件的。\n分析总结 从栈信息可以看出，错误出现的时机通过netty的channel往redis写数据的时候抛的这个异常，写数据的时候代码会执行到SingleThreadEventExecutor类的excute方法：\nprivate void execute(Runnable task, boolean immediate) { boolean inEventLoop = inEventLoop(); addTask(task); if (!inEventLoop) { startThread(); if (isShutdown()) { boolean reject = false; try { if (removeTask(task)) { reject = true; } } catch (UnsupportedOperationException e) { // The task queue does not support removal so the best thing we can do is to just move on and // hope we will be able to pick-up the task before its completely terminated. // In worst case we will log on termination. } if (reject) { reject(); } } } if (!addTaskWakesUp \u0026amp;\u0026amp; immediate) { wakeup(inEventLoop); } } 分析这个方法，大意是将当前写事件提交到SingleThreadEventExecutor任务队列中。报错是在addTask这个方法：\n/** * Add a task to the task queue, or throws a {@link RejectedExecutionException} if this instance was shutdown * before. */ protected void addTask(Runnable task) { ObjectUtil.checkNotNull(task, \u0026quot;task\u0026quot;); if (!offerTask(task)) { reject(task); } } 这个方法判断如果没有插入任务队列成功就调用reject(task)拒绝任务，reject(task)里面抛出的异常就是我们看到的最外面的异常。于是看一看offerTask方法：\nfinal boolean offerTask(Runnable task) { if (isShutdown()) { reject(); } return taskQueue.offer(task); } @Override public boolean isShutdown() { return state \u0026gt;= ST_SHUTDOWN; } private static final int ST_NOT_STARTED = 1; private static final int ST_STARTED = 2; private static final int ST_SHUTTING_DOWN = 3; private static final int ST_SHUTDOWN = 4; private static final int ST_TERMINATED = 5; 这里可以看出offerTask的时候校验了当前的SingleThreadEventExecutor的状态是否是结束和终止。那么问题来了，是什么导致的SingleThreadEventExecutor的状态为终止的勒，通过前面缘由中日志可以看到SingleThreadEventExecutor也打印了堆溢出的错误日志，搜索这个日志是哪里打印的，发现：\nprivate void doStartThread() { assert thread == null; executor.execute(new Runnable() { @Override public void run() { thread = Thread.currentThread(); if (interrupted) { thread.interrupt(); } boolean success = false; updateLastExecutionTime(); try { SingleThreadEventExecutor.this.run(); success = true; } catch (Throwable t) { logger.warn(\u0026quot;Unexpected exception from an event executor: \u0026quot;, t); } finally { for (;;) { int oldState = state; if (oldState \u0026gt;= ST_SHUTTING_DOWN || STATE_UPDATER.compareAndSet( SingleThreadEventExecutor.this, oldState, ST_SHUTTING_DOWN)) { break; } } // Check if confirmShutdown() was called at the end of the loop. if (success \u0026amp;\u0026amp; gracefulShutdownStartTime == 0) { if (logger.isErrorEnabled()) { logger.error(\u0026quot;Buggy \u0026quot; + EventExecutor.class.getSimpleName() + \u0026quot; implementation; \u0026quot; + SingleThreadEventExecutor.class.getSimpleName() + \u0026quot;.confirmShutdown() must \u0026quot; + \u0026quot;be called before run() implementation terminates.\u0026quot;); } } try { // Run all remaining tasks and shutdown hooks. At this point the event loop // is in ST_SHUTTING_DOWN state still accepting tasks which is needed for // graceful shutdown with quietPeriod. for (;;) { if (confirmShutdown()) { break; } } // Now we want to make sure no more tasks can be added from this point. This is // achieved by switching the state. Any new tasks beyond this point will be rejected. for (;;) { int oldState = state; if (oldState \u0026gt;= ST_SHUTDOWN || STATE_UPDATER.compareAndSet( SingleThreadEventExecutor.this, oldState, ST_SHUTDOWN)) { break; } } // We have the final set of tasks in the queue now, no more can be added, run all remaining. // No need to loop here, this is the final pass. confirmShutdown(); } finally { try { cleanup(); } finally { // Lets remove all FastThreadLocals for the Thread as we are about to terminate and notify // the future. The user may block on the future and once it unblocks the JVM may terminate // and start unloading classes. // See https://github.com/netty/netty/issues/6596. FastThreadLocal.removeAll(); STATE_UPDATER.set(SingleThreadEventExecutor.this, ST_TERMINATED); threadLock.countDown(); int numUserTasks = drainTasks(); if (numUserTasks \u0026gt; 0 \u0026amp;\u0026amp; logger.isWarnEnabled()) { logger.warn(\u0026quot;An event executor terminated with \u0026quot; + \u0026quot;non-empty task queue (\u0026quot; + numUserTasks + ')'); } terminationFuture.setSuccess(null); } } } } }); } 这个方法，这个方法里面同时在finally设置了状态为ST_TERMINATED，这个方法大意是启动eventloop真正的线程来轮询读写事件，这个方法将在首次执行excute方法的时候被调用，注意这里的executor变量是一个线程池，于是查阅资料才认识到，eventloopgroup下的每个eventloop实际上都会引用到一个线程池，这个线程池线程数目与eventloop总大小相同，是fork/join框架创建的，真正执行轮询逻辑实际上是这个线程池去提交的。eventloop的意义实际上只是为了保存任务队列。真正的去轮询任务队列的逻辑是由其代理的线程池去执行的。\n这样也就清楚了，整个事故实际很简单，就是先由于内存溢出的错误导致eventloop的状态为终止，这个终止状态据我看到的代码貌似是不可逆的，然后当在调用chanel.write的时候提交任务到eventloop校验状态为终止所以报了拒绝错误。\n解决这个bug的重要途径是找到内存溢出的原因，这里就不赘述了，因为在网上没有搜到这个错的原因，所以好奇跟了一下代码，同时对netty的eventloop加深了一丢丢理解，所以这里做个记录。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2020-12-09T00:00:00Z","permalink":"https://runningccode.github.io/2020/%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83-java.util.concurrent.rejectedexecutionexception-event-executor-terminated-%E9%94%99%E8%AF%AF%E5%88%86%E6%9E%90/","title":"生产环境 java.util.concurrent.RejectedExecutionException: event executor terminated 错误分析"},{"content":"前言 最近学了极客时间王争的《设计模式之美》，感觉以前对设计模式理解的太浅显了。文章上有一篇设计幂等框架的练习，作者给了简单的思路和实现，但是没有给出切面控制，理由是幂等跟业务是高耦合的，我在看之后结合经验，觉得有必要实现一下功能更强大的功能所以手写了个小组件。\n代码地址：我的个人组件仓库 其中的idempotence模块。\n实现功能 基于切面实现注解控制需要实现幂等的接口，幂等号根据方法参数与注解上的spel表达式得到，由业务控制。 判断幂等号相等后，判断为幂等，注解可以指定两种策略，抛出一个固定的异常或者根据上次执行结果返回执行结果或返回上次执行时产生的异常。 注解可以业务发生异常的时候，哪些异常是不需要删除幂等号记录的，默认不删除幂等号记录即下一次请求可以正常通过。 如果想在业务中实现幂等，可以应用IdempotenceClient提供的常用的方法。 组件扩展性 Storge为幂等号的存储对象，根据需要自由选择不同的存储实现，比如基于redisson，mogodb什么的。 提供CacheableStorage，可以注入两个storge,一个为缓存用的，一个为持久化用的。 代码 代码结构：\nimg.png\rIdempotenceClient 幂等组件，相关方法的统一入口 package com.cman777.springc.idempotence; import com.cman777.springc.idempotence.storage.Storage; import org.springframework.beans.factory.annotation.Autowired; import java.io.Serializable; import java.util.function.Supplier; /** * @author chenzhicong * @time 2020/9/15 10:29 */ public class IdempotenceClient { private Storage storage; @Autowired public IdempotenceClient(Storage cache) { this.storage = cache; } public boolean saveIfAbsent(String idempotenceId, Supplier\u0026lt;ResultWrapper\u0026gt; supplier) { return storage.setIfAbsent(idempotenceId,supplier); } public \u0026lt;T extends Serializable\u0026gt; ResultWrapper\u0026lt;T\u0026gt; getResult(String idempotenceId){ return storage.getResult(idempotenceId); } public boolean delete(String idempotenceId) { return storage.delete(idempotenceId); } public boolean exists(String idempotenceId){ return storage.exists(idempotenceId); } } Storage 提供存储幂等标识职责 package com.cman777.springc.idempotence.storage; import com.cman777.springc.idempotence.ResultWrapper; import java.io.Serializable; import java.util.function.Supplier; /** * @author chenzhicong * @time 2020/9/15 10:30 */ public interface Storage { /** * supplier不一定会执行 保证原子性 */ boolean setIfAbsent(String key, Supplier\u0026lt;ResultWrapper\u0026gt; supplier); /** * 得保证幂等性 */ boolean setIfAbsent(String key, ResultWrapper value); boolean delete(String idempotenceId); \u0026lt;T extends Serializable\u0026gt; ResultWrapper\u0026lt;T\u0026gt; getResult(String idempotenceId); boolean exists(String key); } package com.cman777.springc.idempotence; import lombok.Getter; import lombok.Setter; import java.io.Serializable; /** * @author chenzhicong * @time 2020/9/15 18:14 */ @Getter @Setter public class ResultWrapper\u0026lt;T extends Serializable\u0026gt; implements Serializable { private Throwable exception; private T result; private boolean hasException; } package com.cman777.springc.idempotence.storage; import com.cman777.springc.idempotence.ResultWrapper; import com.cman777.springc.redis.annotation.RedisLock; import lombok.extern.log4j.Log4j2; import java.io.Serializable; import java.util.function.Supplier; /** * @author chenzhicong * @time 2020/9/15 10:41 */ @Log4j2 public class CacheableStorage implements Storage { private Storage cacheStorage; private Storage persistenceStorage; public CacheableStorage(Storage cacheStorage, Storage persistenceStorage) { this.cacheStorage = cacheStorage; this.persistenceStorage = persistenceStorage; } @Override @RedisLock(fixedPrefix = \u0026quot;Idempoment_CacheableStorage_setIfAbsent\u0026quot;,salt = \u0026quot;#key\u0026quot;) public boolean setIfAbsent(String key, Supplier\u0026lt;ResultWrapper\u0026gt; supplier){ if(this.exists(key)){ return false; }else{ return this.setIfAbsent(key,supplier.get()); } } @Override public boolean setIfAbsent(String key, ResultWrapper resultWrapper) { boolean cacheHave = !cacheStorage.setIfAbsent(key,resultWrapper); if (cacheHave) { log.info(\u0026quot;缓存中已存在幂等键={}\u0026quot;, key); return false; } else { boolean success = persistenceStorage.setIfAbsent(key,resultWrapper); if (!success) { log.info(\u0026quot;持久层中已存在幂等键={}\u0026quot;, key); } return success; } } @Override public boolean delete(String idempotenceId) { try { cacheStorage.delete(idempotenceId); persistenceStorage.delete(idempotenceId); return true; } catch (Exception e) { log.info(\u0026quot;删除幂等键异常\u0026quot;); log.error(e.getMessage(), e); return false; } } @Override public \u0026lt;T extends Serializable\u0026gt; ResultWrapper\u0026lt;T\u0026gt; getResult(String idempotenceId) { ResultWrapper result = cacheStorage.getResult(idempotenceId); if(result == null){ result = persistenceStorage.getResult(idempotenceId); cacheStorage.setIfAbsent(idempotenceId,result); } return result; } @Override public boolean exists(String key) { boolean isExists = cacheStorage.exists(key); if(!isExists){ isExists = persistenceStorage.exists(key); } return isExists; } } package com.cman777.springc.idempotence.storage; import com.cman777.springc.idempotence.ResultWrapper; import lombok.extern.log4j.Log4j2; import org.apache.commons.codec.binary.Base64; import org.apache.commons.lang3.StringUtils; import org.redisson.api.RBucket; import org.redisson.api.RedissonClient; import java.io.*; import java.util.concurrent.TimeUnit; import java.util.function.Supplier; /** * @author chenzhicong * @time 2020/9/15 10:55 */ @Log4j2 public class RedissionStorageAdapter implements Storage { private Long cacheTime; private RedissonClient redissonClient; public RedissionStorageAdapter(RedissonClient redissonClient, Long cacheTime) { this.redissonClient = redissonClient; this.cacheTime = cacheTime; } @Override public boolean setIfAbsent(String key, Supplier\u0026lt;ResultWrapper\u0026gt; supplier) { if (this.exists(key)) { return false; } else { return this.setIfAbsent(key, supplier.get()); } } @Override public boolean setIfAbsent(String key, ResultWrapper value) { String valueStr = null; try { ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); ObjectOutputStream out = new ObjectOutputStream(byteArrayOutputStream); out.writeObject(value); byte[] exceptionBytes = byteArrayOutputStream.toByteArray(); valueStr = Base64.encodeBase64String(exceptionBytes); log.info(valueStr.length()); } catch (Exception ex) { log.error(\u0026quot;序列化错误\u0026quot;, ex); return false; } if (cacheTime == null) { return redissonClient.getBucket(key).trySet(valueStr); } else { return redissonClient.getBucket(key).trySet(valueStr, cacheTime, TimeUnit.SECONDS); } } @Override public boolean delete(String idempotenceId) { return redissonClient.getBucket(idempotenceId).delete(); } @Override public \u0026lt;T extends Serializable\u0026gt; ResultWrapper\u0026lt;T\u0026gt; getResult(String idempotenceId) { ResultWrapper\u0026lt;T\u0026gt; resultWrapper = null; try { String value = String.valueOf(redissonClient.getBucket(idempotenceId).get()); if(StringUtils.isBlank(value)){ return null; } log.info(value.length()); ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(Base64.decodeBase64(value))); resultWrapper = (ResultWrapper\u0026lt;T\u0026gt;) ois.readObject(); } catch (Exception e) { log.error(\u0026quot;反序列化错误\u0026quot;, e); } return resultWrapper; } @Override public boolean exists(String key) { return redissonClient.getBucket(key).isExists(); } } RedisLock注解是另外实现了的分布式锁切面可以在项目中其他模块看到。 使用的序列化方式是jdk的序列化，然后base64成字符串，这里待优化，效率低并且没法扩展，实际可以将序列化反序列化抽象为接口。 IdempotenceId 注解与IdempotenceAspect，基于注解实现接口幂等 package com.cman777.springc.idempotence.annotation; import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; /** * @author chenzhicong * @time 2020/9/15 11:32 */ @Retention(RetentionPolicy.RUNTIME) @Target({ElementType.METHOD}) public @interface IdempotenceId { /** * 与salt共同决定IdempotenceId */ String prefix() default \u0026quot;\u0026quot;; /** * spel表达式 */ String salt(); /** * 默认发生异常则删除幂等键，但有些情况不删除则填入这里，比如有些异常携带了业务含义 * 就算重试了100次还是同样的结果，则添加异常类在这里 */ Class\u0026lt;? extends Exception\u0026gt;[] notDeleteForException() default {}; /** * 触发幂等的策略 * THROW_EXCEPTION： 抛出一个固定的异常 * RETURN_RESULT： 抛出原来的异常（添加到notDeleteForException的异常），或原来的返回结果 */ Strategy strategy() default Strategy.RETURN_RESULT; enum Strategy{ THROW_EXCEPTION(\u0026quot;THROW_EXCEPTION\u0026quot;,\u0026quot;抛异常\u0026quot;), RETURN_RESULT(\u0026quot;RETURN_RESULT\u0026quot;,\u0026quot;返回结果或抛出原异常\u0026quot;); private String code; private String msg; Strategy(String code,String msg){ this.code=code; this.msg = msg; } } } IdempotenceConfig 用于在上层注册好IdempotenceClient后自动注入切面到容器 package com.cman777.springc.idempotence.config; import com.cman777.springc.idempotence.IdempotenceClient; import com.cman777.springc.idempotence.IdempotenceAspect; import com.cman777.springc.redis.config.RedisConfig; import org.springframework.boot.autoconfigure.AutoConfigureAfter; import org.springframework.boot.autoconfigure.condition.ConditionalOnBean; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; /** * @author chenzhicong * @time 2020/9/15 11:31 */ @Configuration @AutoConfigureAfter(RedisConfig.class) public class IdempotenceConfig { @Bean @ConditionalOnBean(IdempotenceClient.class) @SuppressWarnings(\u0026quot;all\u0026quot;) public IdempotenceAspect idempotenceAspect(IdempotenceClient idempotenceClient){ return new IdempotenceAspect(idempotenceClient); } } 以上就是组件包的所有代码，没有提供持久层存储的Storge实现，需要上层业务端自己实现，然后注册IdempotenceClient才会生效。\n业务端引用 MybatisPlusStorageAdapter：mybatisPlus的存储实现 package com.cman777.springc.sample.config.idempotence; import com.cman777.springc.idempotence.ResultWrapper; import com.cman777.springc.idempotence.storage.Storage; import com.cman777.springc.redis.annotation.RedisLock; import com.cman777.springc.sample.bean.po.Idempotence; import com.cman777.springc.sample.service.IdempotenceService; import lombok.extern.log4j.Log4j2; import org.apache.commons.codec.binary.Base64; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Component; import org.springframework.transaction.annotation.Propagation; import org.springframework.transaction.annotation.Transactional; import java.io.*; import java.util.function.Supplier; /** * @author chenzhicong * @time 2020/9/15 14:16 */ @Component @Log4j2 public class MybatisPlusStorageAdapter implements Storage { @Autowired private IdempotenceService idempotenceService; @Override @Transactional(propagation = Propagation.REQUIRES_NEW,rollbackFor = Exception.class) @RedisLock(entity = Idempotence.class,salt = \u0026quot;#key\u0026quot;) public boolean setIfAbsent(String key, Supplier\u0026lt;ResultWrapper\u0026gt; supplier) { if(this.exists(key)){ return false; }else{ return this.setIfAbsent(key,supplier.get()); } } @Override @RedisLock(entity = Idempotence.class,salt = \u0026quot;#key\u0026quot;) @Transactional(propagation = Propagation.REQUIRES_NEW,rollbackFor = Exception.class) public boolean setIfAbsent(String key, ResultWrapper value) { Idempotence idempotence = idempotenceService.selectByIdempotentceId(key); if (idempotence != null) { return false; } else { String valueStr = null; try { ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); ObjectOutputStream out = new ObjectOutputStream(byteArrayOutputStream); out.writeObject(value); byte[] exceptionBytes = byteArrayOutputStream.toByteArray(); valueStr = Base64.encodeBase64String(exceptionBytes); } catch (Exception ex) { log.error(\u0026quot;序列化错误\u0026quot;, ex); return false; } Idempotence idempotenceNew = new Idempotence(); idempotenceNew.setIdempotenceId(key); idempotenceNew.setValue(valueStr); idempotenceService.save(idempotenceNew); return true; } } @Override @Transactional(propagation = Propagation.NOT_SUPPORTED) public boolean delete(String idempotenceId) { idempotenceService.deleteByIdempotentceId(idempotenceId); return true; } @Override public \u0026lt;T extends Serializable\u0026gt; ResultWrapper\u0026lt;T\u0026gt; getResult(String idempotenceId) { Idempotence idempotence = idempotenceService.selectByIdempotentceId(idempotenceId); ResultWrapper\u0026lt;T\u0026gt; resultWrapper = null; try { String value = idempotence.getValue(); ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(Base64.decodeBase64(value))); resultWrapper = (ResultWrapper\u0026lt;T\u0026gt;) ois.readObject(); } catch (Exception e) { log.error(\u0026quot;反序列化错误\u0026quot;, e); } return resultWrapper; } @Override @Transactional(propagation = Propagation.REQUIRES_NEW,rollbackFor = Exception.class) public boolean exists(String key) { Idempotence idempotence = idempotenceService.selectByIdempotentceId(key); if (idempotence != null) { return true; } else { return false; } } } IdempotenceConfig 注册IdempotenceClient与CacheableStorage package com.cman777.springc.sample.config.idempotence; import com.cman777.springc.idempotence.IdempotenceClient; import com.cman777.springc.idempotence.storage.CacheableStorage; import com.cman777.springc.idempotence.storage.RedissionStorageAdapter; import org.redisson.api.RedissonClient; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; /** * @author chenzhicong * @time 2020/9/15 14:14 */ @Configuration public class IdempotenceConfig { private Long cacheSeconds = 60L; @Bean public CacheableStorage cacheableStorage(RedissonClient redissonClient, MybatisPlusStorageAdapter mybatisPlusStorageAdapter){ return new CacheableStorage(new RedissionStorageAdapter(redissonClient,cacheSeconds),mybatisPlusStorageAdapter); } @Bean public IdempotenceClient idempotenceClient(CacheableStorage cacheableStorage){ return new IdempotenceClient(cacheableStorage); } } 为什么要单独注册cacheableStorage是因为需要应用cacheableStorage的分布式锁切面，为了让springboot自己生成代理类。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2020-09-17T00:00:00Z","permalink":"https://runningccode.github.io/2020/%E6%89%8B%E5%86%99%E4%B8%80%E4%B8%AA%E5%B9%82%E7%AD%89%E7%BB%84%E4%BB%B6%E5%88%86%E4%BA%AB/","title":"手写一个幂等组件分享"},{"content":"缘由 在工作中经常遇到需要写一个公共jar包封装其他开源框架，提供一些最佳实践配置的情况，但是有些开源框架又要求配在配置文件里面，如果业务项目引用了公共jar包，还需要去配很多东西就很烦。解决这个问题最方便的思路是能不能就在jar包里面用java代码的形式增加或修改属性源，但必须在spring容器刷新之前，因为引用的开源框架会在容器刷新时初始化。\n因为前面粗浅研究过过springboot，了解到springboot在启动过程中从spring.factories文件加载所有的SpringApplicationRunListener，并在容器启动过程中回调所有SpringApplicationRunListener注册的监听方法。在默认情况下，这里的RunListener只有一个就是EventPublishingRunListener，这个EventPublishingRunListener间接的又发布了很多ApplicationEvent给ApplicationListener（这里的ApplicationListener也是扫描spring.factories文件发现的，所以除了在spring.factories注册SpringApplicationRunListener，也可以注册ApplicationListener），ApplicationListener，是springboot提供的一个扩展点，可以直接通过注册bean实现来监听各种实现，但是得注意了，普通注册bean（注解的方式）来监听容器刷新前的事件是不行的，原因很简单，容器都还没刷新怎么发现到ApplicationListener的呀。所以必须使用spring.factories的方式提前加载。\n做法 做法很简单。有两种方式，第一种注册RunListener，第二种注册ApplicationListener。\n第一种 第一步 先新建一个SpringApplicationRunListener，为了避免冲突，把这个order设为1，在EventPublishingRunListener之后。\npublic class MyListener implements SpringApplicationRunListener, Ordered { public MyListener(SpringApplication application, String[] args) { } @Override public void starting() { } @Override public void environmentPrepared(ConfigurableEnvironment environment) { MutablePropertySources m = environment.getPropertySources(); Properties p = new Properties(); p.put(\u0026quot;test\u0026quot;, \u0026quot;123\u0026quot;); //addFirst优先级是最高的，如果已经存在值，将会覆盖 m.addFirst(new PropertiesPropertySource(\u0026quot;mypop\u0026quot;,p)); } @Override public void contextPrepared(ConfigurableApplicationContext context) { } @Override public void contextLoaded(ConfigurableApplicationContext context) { } @Override public void started(ConfigurableApplicationContext context) { } @Override public void running(ConfigurableApplicationContext context) { } @Override public void failed(ConfigurableApplicationContext context, Throwable exception) { } @Override public int getOrder() { return 1; } } 第二步 在META-INF文件夹新建spring.factories文件，添加一行，注册上面的SpringApplicationRunListener:org.springframework.boot.SpringApplicationRunListener=com.cman777.springc.sample.MyListener。\n第二种 第一步 public class ConfigListener4RunSprboot implements ApplicationListener\u0026lt;ApplicationPreparedEvent\u0026gt; { @Override public void onApplicationEvent(ApplicationPreparedEvent event) { ConfigurableEnvironment env = event.getApplicationContext().getEnvironment(); MutablePropertySources m = env.getPropertySources(); Properties p = new Properties(); p.put(\u0026quot;test\u0026quot;, \u0026quot;123\u0026quot;); //addFirst优先级是最高的，如果已经存在值，将会覆盖 m.addFirst(new PropertiesPropertySource(\u0026quot;mypop\u0026quot;,p)); } } 第二步 在META-INF文件夹新建spring.factories文件，添加一行，注册上面的ApplicationListener:org.springframework.context.ApplicationListener=com.cman777.springc.sample.ConfigListener4RunSprboot。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2020-07-13T00:00:00Z","permalink":"https://runningccode.github.io/2020/%E5%9C%A8spring%E5%AE%B9%E5%99%A8%E5%88%B7%E6%96%B0%E4%B9%8B%E5%89%8D%E5%A6%82%E4%BD%95%E6%94%B9%E5%8F%98application%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%B1%9E%E6%80%A7/","title":"在Spring容器刷新之前如何改变application配置文件属性"},{"content":"写在前面 因为有个交流群里突然有个小伙伴问为什么jdk建议threadLocal用private static修饰，于是小研究了下。这里就记录一下吧。\nThreadLocal的原理 源码细节就不贴了，简单描述下原理吧：\n当调用threadLocal对象的set方法的时候，实际上是在当前线程对象的threadLocals成员变量（类型是ThreadLocal的的内部类：ThreadLocalMap,可以暂时理解为就是一个Map，但比较简单，数据结构就是一个entry数组，只不过内部寻址是通过hash寻址）里面注册了一个entry，key为threadLocal对象，value为threadLocal包装的值。 当调用threadLocal对象的get方法的时候，实际上就是threadLocal对象这个key从线程对象的threadLocals成员变量这个map里面取对应的值。 这里面还需要注意的有个细节，get,set方法也针对内存泄露问题做了优化，在get或set方法时针对部分entry判断是否key=null，如果key=null则进行清除，等会会将为什么key=null，但entry却还在。\n易导致内存泄露的原因 看下ThreadLocal类的这部分：\nimg.png\rThread类成员变量ThreadLocals的Entry构造方法中调用了WeakReference的构造方法，注册key即ThreadLocal实例为弱引用。弱引用是什么勒？\n这里总结下对象的引用类型,不同的引用类型jvm的回收策略不同：\n强引用 对象被生命周期类的变量所引用。 软引用 SoftReference包装的对象是软引用,软引用在回收之后还是内存还是不够的情况下会被回收。 弱引用 WeakReference包装的对象是弱引用,下一次gc时会被回收。 虚引用 PhantomReference包装的对象是虚引用，无法通过虚引用访问对象的任何属性或函数，下一次gc时会被回收。 在回过头来看Entry这里，这就挺危险了，如果不用static修饰对象的成员变量，那么当对象被回收的时候，threadLocal丢失强引用（如果是static就是存放在永久代，不会被回收），就只被threadLocals的entry弱引用，但是因为是弱引用，所以也会被回收，但是这里的回收就有问题了，ThreadLocals的entry是threadLocals的强引用，entry并不会整个回收，只是这里的key被回收了，value还被entry强引用着，这意味着只要线程对象一直存在，那么threadLocal包装的value就有可能一直不会被回收，这种情况还是挺多的，因为现在的web容器都是线程池，线程执行完后可能被复用，并不会回收线程对象。这也是threadLocal的get.set方法有扫描部分entry判断是否key=null然后进行清除的原因。\n实践 那么在实践中如何使用threadLocal勒。有几个注意的点：\n一般必须用static修饰threadLocal对象,避免entry丢失key的引用。 注意现在的web容器都是线程池，在线程的逻辑业务结束之后一定要手动remove。 若threadLocal包装的对象需要提供给外部使用，最好不直接暴露threadLocal，可以建立一个XXXWrapper类，比如仅提供get方法，remove逻辑自己写在拦截器或过滤器中。 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2020-06-06T00:00:00Z","permalink":"https://runningccode.github.io/2020/%E6%B5%85%E8%B0%88threadlocal%E7%9A%84%E5%86%85%E5%AD%98%E6%B3%84%E9%9C%B2%E9%97%AE%E9%A2%98/","title":"浅谈ThreadLocal的内存泄露问题"},{"content":"一、写在前面 新入职的公司是做C端产品的，比较注重网路安全这一块，所以要求前后端交互报文加解密、签名。这里总结下在落地过程中怎么做的以及一些启发。\n二、前后端加解密 （一）防重放攻击 防重放攻击指的是中间人拦截交互报文然后对请求进行重放进行的一种攻击。解决方案一般两种：\n签名+时间戳 签名+时间戳+nonce 缺点是签名流程照样可以模拟，因为对于前端来说，前端代码是没有绝对安全的。如果是app破解难度还稍微高一些，但h5或pc则最多只能混淆进行处理。\n（二）防中间人攻击以及传输加密 1.https https自带加密,但缺点有：\n建立连接过程中的中间人攻击无法防范 用浏览器抓包或者用Fillder还是能够分析出传输明文 2.代码层面仿https加密 对称加密+非对称加密，为什么不单独用非对称，因为单独用非对称加密效率会慢。\n具体思路为：\n客户端生成随机AES密钥，并用RSA公钥（说是公钥但只是相对的，这里的公钥同样不能泄露）对AES密钥进行加密，AES密钥用于对传输的数据进行加密，然后服务器使用RSA私钥对AES密钥进行解密，用得到的AES密钥对数据进行解密得到真正的数据。\n在代码层面可以利用springboot的RequestBodyAdvice和ResponseBodyAdvice扩展来实现消息体的加解密，业务代码侵入小。\n三、感受 自我感觉前后端加密签名意义并不是很大，因为签名必须得通过真正意义上的私钥签名，但前端没有私钥可言，只是说通过这套机制可以稍微增加些破解难度，要想绝对安全，将这套加解密签名应用于服务端交互更有意义。\n四、网络攻击名词扫盲 XSS攻击 指植入恶意脚本。\n两种方式，一种是反射性，一种是存储型，反射型主要通过脚本参数植入，然后服务器恰巧会原样返回这个参数在浏览器端，存储型则是通过某种方式比如发帖回复，参数携带xss脚本，此时服务器会将内容存储到库中，然后在某个页面返回。\n解决方式有对接口参数进行转义处理，或者前端用一些防xss注入组件。\nCSRF攻击 指恶意网站植入恶意链接，引导用户携带自己的cookie请求我们的服务器，解决方式有判断referer请求头禁止跨域、cookie加上httpOnly属性、禁止窃取cookie、随机token或提交请求加上验证码。\nSQL注入 #{}\nDDos 具体有三种方式：\nDNS Query Flood攻击（DNS查询攻击）利用大量不存在的域名洪水攻击dns服务器，因为是不存在的域名，所以dns会一层层查询到根域名服务器.\nSYN Flood攻击 三次握手中，在第三次握手的时候，客户端不发送ack，服务器会重试SYN+ACK，浪费了服务器资源。\nCC攻击 利用代理服务器发起http攻击\n总结：即控制大量肉鸡或http代理，发起大量syn,dnsQuery，http请求\n解决方式主要是，自己服务器设置IP高频访问限流，黑名单、白名单。最好是利用第三方网盾,比如DDoS高防IP，可以把攻击流量都导入到云厂商的高防IP的服务器上去，他们有专业的技术方案和算法来防御。\n文件上传漏洞 指黑客绕过上传文件的一般校验如扩展名校验、mime校验，然后上传恶意文件。\n攻击方式有：\n上传文件是病毒或者木马时，主要用于诱骗用户或者管理员下载执行或者直接自动运行； 上传文件是WebShell时，攻击者可通过访这些网页后门，执行命令并控制服务器； 上传文件是其他恶意脚本时，攻击者可直接执行脚本进行攻击； 上传文件是恶意图片时，图片中可能包含了脚本，加载或者点击这些图片时脚本会悄无声息的执行； 解决方案有：\nmagic number判断文件类型（文件开头9字节，魔数） 校验白名单 限制文件大小，重命名 压缩文件，比如图片可以利用开源包如imagemagick对文件进行缩放，破坏其原本二进制结构 把文件上传目录设置无脚本执行权限； 使用沙箱（sandbox）/强制访问控制（mandatory access control）方案，关闭对应服务进程所有不必要的路径访问、文件读写和进程执行权限。； 使用最低级别的帐号启动web容器进程； 对上传的文件进行重命名并隐藏上传后的文件名； 直接用OSS。 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2020-05-28T00:00:00Z","permalink":"https://runningccode.github.io/2020/%E7%AE%80%E5%8D%95%E5%B0%8F%E7%BB%93%E6%8E%A5%E5%8F%A3%E4%BA%A4%E4%BA%92%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98/","title":"简单小结接口交互安全问题"},{"content":"问题 问题的背景： 由于springcache并不是很好用，并不针对细粒度的过期时间控制。所以我司基于spel表达式写了一个缓存切面实现类似的功能。 但近期发现在使用过程中有产生缓存的时候key为null的情况。\n问题的表现主要是发现有redis中有时候用户的redis缓存里面居然存在key后缀为null的缓存数据，但是值却有用户信息:\nimg.png\r原因 经偶然间，发现突然复现了这个问题，于是开始深入debug，发现原来spring自带的ParameterNameDiscoverer（用于获取方法参数名的工具）解析方法参数名的时候是用ASM（一个字节码操作框架，Cglib就是用的这个）解析的class文件获取的参数名。在ParameterNameDiscoverer内部的方法中，发现解析的class输入流居然为null，为什么勒，于是猜测磁盘文件的class不存在了。于是重启项目，发现能够顺利获取参数名，执行mvn clean命令后就获取不到了。又由于在解析参数名的时候每次都new了LocalVariableTableParameterNameDiscoverer对象获取的，所以没有应用到其自带的缓存机制，导致该问题较频繁的出现。\n在思考线上测试环境的问题，原来线上jenkens执行的脚本是先复制文件在kill进程。\n解决方案 调整redis缓存切面，若spel表达式无法顺利获取到值则不进行缓存。 调整spel工具，不用每次都new一个LocalVariableTableParameterNameDiscoverer，而使用单例，应用其自身的缓存机制，不然每次都产生磁盘io效率太低。 调整jenkens执行脚本，先kill进程再重启项目。 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2020-05-07T00:00:00Z","permalink":"https://runningccode.github.io/2020/%E5%8A%A8%E6%80%81%E8%8E%B7%E5%8F%96%E6%96%B9%E6%B3%95%E5%8F%82%E6%95%B0%E7%9A%84%E5%9D%91/","title":"动态获取方法参数的坑"},{"content":"一、 先了解下各种后置处理器扩展点 (一)BeanFactoryPostProcessor——bean工厂后置处理 BeanFactory 标准初始化完毕后(经过包扫描后所有的 BeanDefinition 已经被注册)，可以对这个 BeanFactory 进行后置处理。\n(二)BeanDefinitionRegistryPostProcessor——bean定义注册表后置处理 BeanFactoryPostProcessor的子接口，多了一个postProcessBeanDefinitionRegistry方法，这个方法允许在Bean实例化之前对BeanDefinitionRegistry（bean定义注册表）进行后置处理。\n(三)BeanPostProcessor——bean后置处理器 提供对实例化后的bean进行后置处理的扩展点。一般用于对将要实例化到容器的bean进行再次加工。\n(四)MergedBeanDefinitionPostProcessor——合并Bean定义后置处理器 BeanPostProcessor的子接口，多一个postProcessMergedBeanDefinition(RootBeanDefinition beanDefinition, Class\u0026lt;?\u0026gt; beanType, String beanName) 方法，用于后置处理合并Bean定义。\n二、容器刷新流程 （一）主要代码 //最终调到AbstractApplicationContext的refresh方法 public void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) { // Prepare this context for refreshing. // 初始化前的预处理，初始化Environment里面的PropertySources(猜测是webXML里面的东西)，debug下没有什么用 prepareRefresh(); // Tell the subclass to refresh the internal bean factory. // 获取BeanFactory,直接返回的是前面初始化的beanFactory,只不过设置了一下SerializationId ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. // 3. BeanFactory的预处理配置 //(1) 在容器注册了ApplicationContextAwareProcessor这个Bean后置处理器用于处理实现了XXXAware接口的bean，调用其setXXX方法。 //(2)忽略一些自动注入，以及添加一些自动注入的支持，为什么要忽略这些自动注入勒，因为当beanDefinition的AutowireMode为1（按setXXX方法的名称进行注入）和2（按setXXX方法返回值类型进行自动注入）时，若自动注入生效，该Bean的setXXX方法将被自动注入，那么为了避免和XXXAware接口冲突，所以进行了忽略。 //(3) 添加一些自动注入支持，包含BeanFactory，ResourceLoader，ApplicationEventPublisher，ApplicationContext。 //(4) 在容器注册了new ApplicationListenerDetector(this)这个Bean后置处理器用于收集所有实现了ApplicationListener接口的bean并收集到容器中的一个集合中。 prepareBeanFactory(beanFactory); try { // Allows post-processing of the bean factory in context subclasses. // 4. 准备BeanFactory完成后进行的后置处理 //以servlet环境为例： //(1) 添加了一个bean的后置处理器处理ServletContextAware和ServletConfigAware，用于注入ServletContext和ServletConfig。 //(2) 往容器注册Scope，Scope描述的是Spring容器如何新建Bean实例的，这里注册了Request以及Session两个Scope并且注册ServletRequest、ServletResponse、HttpSession、WebRequest为自动装配。 //(3)当判断容器的basePackages属性不为null的时候进行包扫描（但debug下这里没执行）。 //(4)当判断容器的annotatedClasses属性不为null也进行注册(debug下没执行)。 postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. // 5. 执行BeanFactory创建后的后置处理器， // 这一步里面会处理ConfigurationClassPostProcessor这个bd后置处理器完成所有的bd注册 invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. // 6. 注册Bean的后置处理器 registerBeanPostProcessors(beanFactory); // Initialize message source for this context. // 7. 初始化MessageSource initMessageSource(); // Initialize event multicaster for this context. // 8. 初始化事件派发器 initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. // 9. 子类的多态onRefresh，比如在 ServletWebServerApplicationContext.onRefresh方法中启动了web容器 onRefresh(); // Check for listener beans and register them. // 10. 注册监听器 registerListeners(); //到此为止，BeanFactory已创建完成 // Instantiate all remaining (non-lazy-init) singletons. // 11. 初始化所有剩下的单例Bean finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. // 12. 完成容器的创建工作 finishRefresh(); } catch (BeansException ex) { if (logger.isWarnEnabled()) { logger.warn(\u0026quot;Exception encountered during context initialization - \u0026quot; + \u0026quot;cancelling refresh attempt: \u0026quot; + ex); } // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; } finally { // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... // 13. 清除缓存 resetCommonCaches(); } } } (二)核心点 1.prepareBeanFactory(beanFactory)-BeanFactory的预处理配置 protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) { // Tell the internal bean factory to use the context's class loader etc. // 设置BeanFactory的类加载器、表达式解析器等 beanFactory.setBeanClassLoader(getClassLoader()); beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver(beanFactory.getBeanClassLoader())); beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, getEnvironment())); // Configure the bean factory with context callbacks. // 配置一个BeanPostProcessor，这个Bean后处理器将实现了以下几个Aware的bean分别回调对应的方法 beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this)); // 配置ignoreDependencyInterface，是的这些类型自动装配无效，但实测@Autowired注入时还是能装配，故这里的意思是为了避免其他bd设置了自动注入，即AutowireMode，而不是指使用@Autowired注解进行的依赖注入。 beanFactory.ignoreDependencyInterface(EnvironmentAware.class); beanFactory.ignoreDependencyInterface(EmbeddedValueResolverAware.class); beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class); beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class); beanFactory.ignoreDependencyInterface(MessageSourceAware.class); beanFactory.ignoreDependencyInterface(ApplicationContextAware.class); // BeanFactory interface not registered as resolvable type in a plain factory. // MessageSource registered (and found for autowiring) as a bean. // 自动注入的支持 beanFactory.registerResolvableDependency(BeanFactory.class, beanFactory); beanFactory.registerResolvableDependency(ResourceLoader.class, this); beanFactory.registerResolvableDependency(ApplicationEventPublisher.class, this); beanFactory.registerResolvableDependency(ApplicationContext.class, this); // Register early post-processor for detecting inner beans as ApplicationListeners. // 配置一个可加载所有监听器的组件 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this)); // Detect a LoadTimeWeaver and prepare for weaving, if found. if (beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) { beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); // Set a temporary ClassLoader for type matching. beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); } // Register default environment beans. // 注册了默认的运行时环境、系统配置属性、系统环境的信息 if (!beanFactory.containsLocalBean(ENVIRONMENT_BEAN_NAME)) { beanFactory.registerSingleton(ENVIRONMENT_BEAN_NAME, getEnvironment()); } if (!beanFactory.containsLocalBean(SYSTEM_PROPERTIES_BEAN_NAME)) { beanFactory.registerSingleton(SYSTEM_PROPERTIES_BEAN_NAME, getEnvironment().getSystemProperties()); } if (!beanFactory.containsLocalBean(SYSTEM_ENVIRONMENT_BEAN_NAME)) { beanFactory.registerSingleton(SYSTEM_ENVIRONMENT_BEAN_NAME, getEnvironment().getSystemEnvironment()); } } 这里主要干了四件事：\n在容器注册了ApplicationContextAwareProcessor这个Bean后置处理器用于处理实现了XXXAware接口的bean，调用其setXXX方法。 忽略一些自动注入，以及添加一些自动注入的支持，为什么要忽略这些自动注入勒，因为当beanDefinition的AutowireMode为1（按setXXX方法的名称进行注入）和2（按setXXX方法返回值类型进行自动注入）时，若自动注入生效，该Bean的setXXX方法将被自动注入，那么为了避免和XXXAware接口冲突，所以进行了忽略。 添加一些自动注入支持，包含BeanFactory，ResourceLoader，ApplicationEventPublisher，ApplicationContext。 在容器注册了new ApplicationListenerDetector(this)这个Bean后置处理器用于收集所有实现了ApplicationListener接口的bean并收集到容器中的一个集合中。 2.postProcessBeanFactory(beanFactory)-准备BeanFactory完成后进行的后置处理 以servlet为例:这里的ApplicationContext实现实际上为AnnotationConfigServletWebServerApplicationContext。在AnnotationConfigServletWebServerApplicationContext类中该方法主要是完成了：\n往容器注册了一个bean的后置处理器处理ServletContextAware和ServletConfigAware，用于注入ServletContext和ServletConfig。 往容器注册Scope，Scope描述的是Spring容器如何新建Bean实例的，这里注册了Request以及Session两个Scope并且注册ServletRequest、ServletResponse、HttpSession、WebRequest为自动装配。 当判断容器的basePackages属性不为null的时候进行包扫描（但debug下这里没执行）。 当判断容器的annotatedClasses属性不为null也进行注册(debug下没执行)。 3.invokeBeanFactoryPostProcessors(beanFactory)-执行BeanFactory创建后的后置处理器 代码：\nprotected void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) { // 执行BeanFactory后置处理器 PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, getBeanFactoryPostProcessors()); // Detect a LoadTimeWeaver and prepare for weaving, if found in the meantime // (e.g. through an @Bean method registered by ConfigurationClassPostProcessor) if (beanFactory.getTempClassLoader() == null \u0026amp;\u0026amp; beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) { beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); } } 进入 PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, getBeanFactoryPostProcessors())：\npublic static void invokeBeanFactoryPostProcessors( ConfigurableListableBeanFactory beanFactory, List\u0026lt;BeanFactoryPostProcessor\u0026gt; beanFactoryPostProcessors) { // Invoke BeanDefinitionRegistryPostProcessors first, if any. //用于存放已经执行了的processedBeans Set\u0026lt;String\u0026gt; processedBeans = new HashSet\u0026lt;\u0026gt;(); // 这里要判断BeanFactory的类型，默认SpringBoot创建的BeanFactory是DefaultListableBeanFactory // 这个类实现了BeanDefinitionRegistry接口，则此if结构必进 if (beanFactory instanceof BeanDefinitionRegistry) { BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory; List\u0026lt;BeanFactoryPostProcessor\u0026gt; regularPostProcessors = new LinkedList\u0026lt;\u0026gt;(); List\u0026lt;BeanDefinitionRegistryPostProcessor\u0026gt; registryProcessors = new LinkedList\u0026lt;\u0026gt;(); //遍历已经注册到beanFactory的BeanFactoryPostProcessor后置处理器，然后分类为regularPostProcessors和registryProcessors for (BeanFactoryPostProcessor postProcessor : beanFactoryPostProcessors) { if (postProcessor instanceof BeanDefinitionRegistryPostProcessor) { BeanDefinitionRegistryPostProcessor registryProcessor = (BeanDefinitionRegistryPostProcessor) postProcessor; registryProcessor.postProcessBeanDefinitionRegistry(registry); registryProcessors.add(registryProcessor); } else { regularPostProcessors.add(postProcessor); } } // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let the bean factory post-processors apply to them! // Separate between BeanDefinitionRegistryPostProcessors that implement // PriorityOrdered, Ordered, and the rest. //这个currentRegistryProcessors变量用于分阶段执行方法，因为有PriorityOrdered和Ordered接口的存在 List\u0026lt;BeanDefinitionRegistryPostProcessor\u0026gt; currentRegistryProcessors = new ArrayList\u0026lt;\u0026gt;(); // First, invoke the BeanDefinitionRegistryPostProcessors that implement PriorityOrdered. // 首先，调用实现PriorityOrdered接口的BeanDefinitionRegistryPostProcessors并添加到processedBeans String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) { if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); } } //排序 sortPostProcessors(currentRegistryProcessors, beanFactory); //添加到registryProcessors registryProcessors.addAll(currentRegistryProcessors); //执行 invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); // Next, invoke the BeanDefinitionRegistryPostProcessors that implement Ordered. // 接下来，调用实现Ordered接口的BeanDefinitionRegistryPostProcessors。 postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) { if (!processedBeans.contains(ppName) \u0026amp;\u0026amp; beanFactory.isTypeMatch(ppName, Ordered.class)) { currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); } } //排序 sortPostProcessors(currentRegistryProcessors, beanFactory); //添加到registryProcessors registryProcessors.addAll(currentRegistryProcessors); //执行 invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); // Finally, invoke all other BeanDefinitionRegistryPostProcessors until no further ones appear. // 最后，调用所有其他BeanDefinitionRegistryPostProcessor boolean reiterate = true; while (reiterate) { reiterate = false; postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) { if (!processedBeans.contains(ppName)) { currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); reiterate = true; } } //排序添加执行 sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); } // Now, invoke the postProcessBeanFactory callback of all processors handled so far. // 回调所有BeanFactoryPostProcessor的postProcessBeanFactory方法 invokeBeanFactoryPostProcessors(registryProcessors, beanFactory); invokeBeanFactoryPostProcessors(regularPostProcessors, beanFactory); // 先回调BeanDefinitionRegistryPostProcessor的postProcessBeanFactory方法 // 再调用BeanFactoryPostProcessor的postProcessBeanFactory方法 } // 如果BeanFactory没有实现BeanDefinitionRegistry接口，则进入下面的代码流程 else { // Invoke factory processors registered with the context instance. // 调用在上下文实例中注册的工厂处理器。 invokeBeanFactoryPostProcessors(beanFactoryPostProcessors, beanFactory); } // 下面的部分是回调BeanFactoryPostProcessor，思路与上面的几乎一样 // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let the bean factory post-processors apply to them! String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, true, false); // Separate between BeanFactoryPostProcessors that implement PriorityOrdered, // Ordered, and the rest. List\u0026lt;BeanFactoryPostProcessor\u0026gt; priorityOrderedPostProcessors = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;String\u0026gt; orderedPostProcessorNames = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;String\u0026gt; nonOrderedPostProcessorNames = new ArrayList\u0026lt;\u0026gt;(); for (String ppName : postProcessorNames) { if (processedBeans.contains(ppName)) { // skip - already processed in first phase above } else if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { priorityOrderedPostProcessors.add(beanFactory.getBean(ppName, BeanFactoryPostProcessor.class)); } else if (beanFactory.isTypeMatch(ppName, Ordered.class)) { orderedPostProcessorNames.add(ppName); } else { nonOrderedPostProcessorNames.add(ppName); } } // First, invoke the BeanFactoryPostProcessors that implement PriorityOrdered. sortPostProcessors(priorityOrderedPostProcessors, beanFactory); invokeBeanFactoryPostProcessors(priorityOrderedPostProcessors, beanFactory); // Next, invoke the BeanFactoryPostProcessors that implement Ordered. List\u0026lt;BeanFactoryPostProcessor\u0026gt; orderedPostProcessors = new ArrayList\u0026lt;\u0026gt;(); for (String postProcessorName : orderedPostProcessorNames) { orderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); } sortPostProcessors(orderedPostProcessors, beanFactory); invokeBeanFactoryPostProcessors(orderedPostProcessors, beanFactory); // Finally, invoke all other BeanFactoryPostProcessors. List\u0026lt;BeanFactoryPostProcessor\u0026gt; nonOrderedPostProcessors = new ArrayList\u0026lt;\u0026gt;(); for (String postProcessorName : nonOrderedPostProcessorNames) { nonOrderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); } invokeBeanFactoryPostProcessors(nonOrderedPostProcessors, beanFactory); // Clear cached merged bean definitions since the post-processors might have // modified the original metadata, e.g. replacing placeholders in values... // 清理缓存 beanFactory.clearMetadataCache(); } 上面代码有点长主要干得事有：\n第一步获取已经注册到容器（与beanDifinitionMap相区别，这里用了容器内beanFactoryPostProcessors这个变量存的而不是从beanDefinition获取的）的beanFactoryPostProcessor的beanFactoryPostProcessors，并筛选实现了BeanDefinitionRegistryPostProcessor接口的，执行其postProcessBeanDefinitionRegistry方法. 第二步获取容器内已注册的beanDefinition中BeanDefinitionRegistryPostProcessor类型的bean，筛选实现了PriorityOrdered接口的，进行排序，然后回调其postProcessBeanDefinitionRegistry。这里执行最重要的ConfigurationClassPostProcessor，他会对当前beandifinitonMap中的带有configraution注解的进行处理，比如处理@Component 、@ComponentScan 、@Import 、@ImportResource、@PropertySource 、@ComponentScan 、@Import 、@ImportResource 、@Bean注解，注册所有的beanDefinition,等一下展开讲这个ConfigurationClassPostProcessor。 第三步获取容器内已注册的beanDefinition中BeanDefinitionRegistryPostProcessor类型的bean，会根据是否实现PriorityOrdered接口Ordered接口进行排序（大体顺序是PriorityOrdered优先Ordered优先没实现接口的，同一接口的按方法返回值确定顺序），然后调用其postProcessBeanDefinitionRegistry方法。 第四步执行上面所有beanDefinitionRegistryPostProcessor类型的bean的postBeanFactory方法。 第五步对于ApplicationContext内（与beanDifinitionMap相区别，这里用了beanFactoryPostProcessors这个变量存而不是beanDefinition存）的beanFactoryPostProcessor，不属于BeanDefinitionRegistryPostProcessor接口的（即只是BeanFactoryProcessor），调用其postBeanFactory方法。 第六步调用其他所有BeanFactoryPostProcessor的postBeanFactory方法，也会解析PriorityOrdered及Ordered接口。 接下来重点看一看在第二步执行的ConfigurationClassPostProcessor的源码，内部就不详细展开了，大概的流程已经写入注释中：\npublic void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) { int registryId = System.identityHashCode(registry); if (this.registriesPostProcessed.contains(registryId)) { throw new IllegalStateException( \u0026quot;postProcessBeanDefinitionRegistry already called on this post-processor against \u0026quot; + registry); } if (this.factoriesPostProcessed.contains(registryId)) { throw new IllegalStateException( \u0026quot;postProcessBeanFactory already called on this post-processor against \u0026quot; + registry); } this.registriesPostProcessed.add(registryId); processConfigBeanDefinitions(registry); } 这里的意思是获取容器Id,获取其是否调用过，如果没有则继续执行processConfigBeanDefinitions。看一看 processConfigBeanDefinitions(registry)方法：\npublic void processConfigBeanDefinitions(BeanDefinitionRegistry registry) { List\u0026lt;BeanDefinitionHolder\u0026gt; configCandidates = new ArrayList\u0026lt;\u0026gt;(); String[] candidateNames = registry.getBeanDefinitionNames(); // 确定配置类和组件 //带有@Configuration注解的bd的configurationClass值设为full， //带有@Component 、@ComponentScan 、@Import 、@ImportResource注解或方法中添加了带@Bean的方法 //（只是将带bean的方法收集起来，并没有注册bd）的设为configurationClass值设为lite，并加入到configCandidates for (String beanName : candidateNames) { BeanDefinition beanDef = registry.getBeanDefinition(beanName); if (ConfigurationClassUtils.isFullConfigurationClass(beanDef) || ConfigurationClassUtils.isLiteConfigurationClass(beanDef)) { if (logger.isDebugEnabled()) { logger.debug(\u0026quot;Bean definition has already been processed as a configuration class: \u0026quot; + beanDef); } } else if (ConfigurationClassUtils.checkConfigurationClassCandidate(beanDef, this.metadataReaderFactory)) { configCandidates.add(new BeanDefinitionHolder(beanDef, beanName)); } } // Return immediately if no @Configuration classes were found if (configCandidates.isEmpty()) { return; } // Sort by previously determined @Order value, if applicable // 对配置类进行排序 configCandidates.sort((bd1, bd2) -\u0026gt; { int i1 = ConfigurationClassUtils.getOrder(bd1.getBeanDefinition()); int i2 = ConfigurationClassUtils.getOrder(bd2.getBeanDefinition()); return Integer.compare(i1, i2); }); // Detect any custom bean name generation strategy supplied through the enclosing application context // 加载获取BeanNameGenerator SingletonBeanRegistry sbr = null; if (registry instanceof SingletonBeanRegistry) { sbr = (SingletonBeanRegistry) registry; if (!this.localBeanNameGeneratorSet) { BeanNameGenerator generator = (BeanNameGenerator) sbr.getSingleton(CONFIGURATION_BEAN_NAME_GENERATOR); if (generator != null) { this.componentScanBeanNameGenerator = generator; this.importBeanNameGenerator = generator; } } } if (this.environment == null) { this.environment = new StandardEnvironment(); } // Parse each @Configuration class // 初始化配置类解析器 ConfigurationClassParser parser = new ConfigurationClassParser( this.metadataReaderFactory, this.problemReporter, this.environment, this.resourceLoader, this.componentScanBeanNameGenerator, registry); //需要解析的配置类集合 Set\u0026lt;BeanDefinitionHolder\u0026gt; candidates = new LinkedHashSet\u0026lt;\u0026gt;(configCandidates); //已经解析的配置类集合 Set\u0026lt;ConfigurationClass\u0026gt; alreadyParsed = new HashSet\u0026lt;\u0026gt;(configCandidates.size()); do { // 解析配置类，最重要的方法 //对configCandidates按照@Order进行排序并遍历进行递归一直解析父类， //需解析@PropertySource 、@ComponentScan 、@Import 、@ImportResource 、@Bean注解(注这一步还没有对扫描到的组件完全进行Bd注册， //而只是注册了包扫描到的bd以及处理@Import注解时实现了ImportBeanDefinitionRegistrar或者ImportSelector接口的bd, //并且这里会先处理一个类的嵌套配置类） parser.parse(candidates); //校验 parser.validate(); Set\u0026lt;ConfigurationClass\u0026gt; configClasses = new LinkedHashSet\u0026lt;\u0026gt;(parser.getConfigurationClasses()); configClasses.removeAll(alreadyParsed); // Read the model and create bean definitions based on its content if (this.reader == null) { this.reader = new ConfigurationClassBeanDefinitionReader( registry, this.sourceExtractor, this.resourceLoader, this.environment, this.importBeanNameGenerator, parser.getImportRegistry()); } //解析配置类中的内容 //将通过@Import、@Bean注解方式注册的类以及处理@ImportResource注解引入的配置文件解析成BeanDefinition，然后注册到BeanDefinitionMap中。 this.reader.loadBeanDefinitions(configClasses); alreadyParsed.addAll(configClasses); candidates.clear(); if (registry.getBeanDefinitionCount() \u0026gt; candidateNames.length) { //当前的bdNames String[] newCandidateNames = registry.getBeanDefinitionNames(); //上一次解析之前的bdNames Set\u0026lt;String\u0026gt; oldCandidateNames = new HashSet\u0026lt;\u0026gt;(Arrays.asList(candidateNames)); //这次解析的bdNames Set\u0026lt;String\u0026gt; alreadyParsedClasses = new HashSet\u0026lt;\u0026gt;(); for (ConfigurationClass configurationClass : alreadyParsed) { alreadyParsedClasses.add(configurationClass.getMetadata().getClassName()); } //遍历当前bdNames，若不是以前有的，并且是配置类，并且没有被解析到，则添加到candidates，下一次循环再解析一次 for (String candidateName : newCandidateNames) { if (!oldCandidateNames.contains(candidateName)) { BeanDefinition bd = registry.getBeanDefinition(candidateName); if (ConfigurationClassUtils.checkConfigurationClassCandidate(bd, this.metadataReaderFactory) \u0026amp;\u0026amp; !alreadyParsedClasses.contains(bd.getBeanClassName())) { candidates.add(new BeanDefinitionHolder(bd, candidateName)); } } } candidateNames = newCandidateNames; } } while (!candidates.isEmpty()); // Register the ImportRegistry as a bean in order to support ImportAware @Configuration classes // 将ImportRegistry注册为Bean，以支持ImportAware 和@Configuration类 if (sbr != null \u0026amp;\u0026amp; !sbr.containsSingleton(IMPORT_REGISTRY_BEAN_NAME)) { sbr.registerSingleton(IMPORT_REGISTRY_BEAN_NAME, parser.getImportRegistry()); } // 清除缓存 if (this.metadataReaderFactory instanceof CachingMetadataReaderFactory) { // Clear cache in externally provided MetadataReaderFactory; this is a no-op // for a shared cache since it'll be cleared by the ApplicationContext. ((CachingMetadataReaderFactory) this.metadataReaderFactory).clearCache(); } } 4.registerBeanPostProcessors(beanFactory)-注册bean后置处理器（包含MergedBeanDefinitionPostProcessor） 注册逻辑跟注册beanFactoryPostProcessor差不多，注册顺序都会判断priorityOrdered与Ordered接口，并且先注册MergedBeanDefinitionPostProcessor再注册beanFactoryPostProcessor。\n这里有两个MergedBeanDefinitionPostProcessor，一个是AutowiredAnnotationBeanPostProcessor，一个是ApplicationListenerDetector。\nAutowiredAnnotationBeanPostProcessor 会解析bean的自动注入属性，判断是否有需要依赖的项，并通过registerExternallyManagedConfigMember注册依赖项。 ApplicationListenerDetector 作用，收集一个beanName为键，是否单例为值的map。 5. initMessageSource()-初始化MessageSource messageSource主要是spring提供的国际化组件，与之对应的yml配置是：\nspring: messages: basename: i18n/messages encoding: UTF-8 这样messageSource的getMessage方法就可以通过在i18n目录下查找对应的messageXXX.properties文件，于参数中的Locale进行对比，返回对应国际化后的message。\nmessageSource中的方法有：\n//有默认值返回 String getMessage(String code, @Nullable Object[] args, @Nullable String defaultMessage, Locale locale); //无默认值，找不到抛异常 String getMessage(String code, @Nullable Object[] args, Locale locale) throws NoSuchMessageException; //MessageSourceResolvable封装了code,args,defaultMessage String getMessage(MessageSourceResolvable resolvable, Locale locale) throws NoSuchMessageException; 参数解释：\ncode 信息的键，properties中的key、 args,用于模板替换message中的参数。 locale 国家地区 MessageSourceResolvable 封装了code,args,defaultMessage，code参数为String[]数组形式，通过遍历调用的方式去获取信息，只要其中一个code能够获取到值，便直接返回。查询不出数据时且defaultMessage为空时，直接抛出NoSuchMessageException异常。 源码这里会检查bd里面是否有messageSource，没有的话直接初始化默认的messageSource，DelegatingMessageSource类。\n6. initApplicationEventMulticaster()-初始化事件广播器 这里也是扫描bd中有没有applicationEventMulticaster，如果没有则使用默认的SimpleApplicationEventMulticaster，这里需要注意的是，在这之前，springboot启动过程中其实也有事件分发，比如SpringApplicationRunListener的start方法在容器开始的时候就被调用了，它是如何做的勒？实际上它是使用的内部的SimpleApplicationEventMulticaster来完成的事件广播。而这里和后面将要注册的listener不是使用的同一个事件广播器。\n7.onRefresh 不同的web容器会多态实现，比如ServletWebServerApplicationContext会创建嵌入式Servlet容器。\n8.registerListeners()-注册监听器 注册监听器，并广播早期事件（只是一个供springboot自己用的一个扩展点，这个扩展点允许在后置处理器和监听器都被创建好，其余的单实例Bean还没有创建时广播一些早期事件），通过debug看这里目前没有任何早期事件存入。\n9.finishBeanFactoryInitialization(beanFactory)-初始化单例bean 比较核心，初始化其他的单例bean，解决了循环依赖。\n10.finishRefresh() 清除资源缓存(如扫描的ASM元数据)、处理生命周期处理器（Lifecycle接口）、发布容器刷新完成的事件。ServletWebServerApplicationContext在最后会调用 WebServer 的start方法。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2020-03-02T00:00:00Z","permalink":"https://runningccode.github.io/2020/springboot%E6%B5%85%E6%9E%90%E4%B8%89%E5%AE%B9%E5%99%A8%E5%88%B7%E6%96%B0%E6%B5%81%E7%A8%8B/","title":"Springboot浅析（三）——容器刷新流程"},{"content":"大概是水平有限，最近跟读代码与相关书籍感觉巨费时间，想深入弄明白所有的东西很难，所以也只能带着问题来学习springboot了，以后遇到确切的问题再做深入了解把，给自己定个目标，暂时只弄清楚容器启动大体流程，了解组件扫描，自动配置，解决循环依赖这几个问题。 一般启动的Main方法为SpringApplication.run(启动类.class, args);,跟下去的话会发现调用的就是new SpringApplication(启动类).run(args)由于容器刷新内容最关键也最复杂，先来了解下除容器刷新之外的流程。\n（一） SpringApplication的初始化 1.代码 public SpringApplication(ResourceLoader resourceLoader, Class\u0026lt;?\u0026gt;... primarySources) { this.resourceLoader = resourceLoader; Assert.notNull(primarySources, \u0026quot;PrimarySources must not be null\u0026quot;); //通常情况下primarySources就是启动类，暂时理解这里就是将启动类设置为主配置资源来源 this.primarySources = new LinkedHashSet\u0026lt;\u0026gt;(Arrays.asList(primarySources)); //通过类路径中寻找相关类，判断当前环境是NONE（标准环境(classPath下没有javax.servlet.Servlet以及org.springframework.web.context.ConfigurableWebApplicationContext）、SERVLET（Servlet环境）、REACTIVE（响应式） this.webApplicationType = WebApplicationType.deduceFromClasspath(); //添加initializers，设置初始化器，这些初始化器将在在容器刷新前回调，原理是通过SpringFactoriesLoader的loadFactoryNames方法在 spring.factories文件中找到的ApplicationContextInitializer接口的配置的实现类的全限定类名，并实例化。 setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class)); //同上，设置ApplicationListener，添加 spring.factories文件中ApplicationListener配置的响应实现类。 setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); //通过构造一个运行时异常，然后去栈帧中寻找方法名为main的方法来得到入口类的名字并设置为mainApplicationClass this.mainApplicationClass = deduceMainApplicationClass(); } 2.注 （1）ApplicationContextInitializer有哪些？ debug发现有这些：\n他们的作用是：\nConfigurationWarningsApplicationContextInitializer：报告IOC容器的一些常见的错误配置 ContextIdApplicationContextInitializer：设置Spring应用上下文的ID DelegatingApplicationContextInitializer：加载 application.properties 中 context.initializer.classes 配置的类 ServerPortInfoApplicationContextInitializer：将内置servlet容器实际使用的监听端口写入到 Environment 环境属性中 SharedMetadataReaderFactoryContextInitializer：创建一个 SpringBoot 和 ConfigurationClassPostProcessor 共用的 CachingMetadataReaderFactory 对象 ConditionEvaluationReportLoggingListener：将 ConditionEvaluationReport 写入日志 （2）ApplicationListener有哪些 debug发现有这些：\n他们的作用是：\nClearCachesApplicationListener：应用上下文加载完成后对缓存做清除工作 ParentContextCloserApplicationListener：监听双亲应用上下文的关闭事件并往自己的子应用上下文中传播 FileEncodingApplicationListener：检测系统文件编码与应用环境编码是否一致，如果系统文件编码和应用环境的编码不同则终止应用启动 AnsiOutputApplicationListener：根据 spring.output.ansi.enabled 参数配置 AnsiOutput ConfigFileApplicationListener：从常见的那些约定的位置读取配置文件 DelegatingApplicationListener：监听到事件后转发给 application.properties 中配置的 context.listener.classes 的监听器 ClasspathLoggingApplicationListener：对环境就绪事件 ApplicationEnvironmentPreparedEvent 和应用失败事件 ApplicationFailedEvent 做出响应 LoggingApplicationListener：配置 LoggingSystem。使用 logging.config 环境变量指定的配置或者缺省配置 LiquibaseServiceLocatorApplicationListener：使用一个可以和 SpringBoot 可执行jar包配合工作的版本替换 LiquibaseServiceLocator BackgroundPreinitializer：使用一个后台线程尽早触发一些耗时的初始化任务 （3）REACTIVE是什么 REACTIVE是响应式编程的东西，指的是应用WebFlux框架下的应用环境，是NIO同步非阻塞IO，未来可能替代当前的MVC，由于是比较新的技术，应用场景比较有限，暂时不做深入了解。\n（二）容器刷新之前的操作 1.代码 public ConfigurableApplicationContext run(String... args) { //这个组件是用来监控启动时间的，不是很重要 StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; //SpringBootExceptionReporter这个东西是一个异常解析器，实现类只有一个是FailureAnalyzers， //用于打印异常信息，这个集合在下面③处会初始化，集合里面装了针对各式各样的解析器， //在catch到异常后，会遍历这个集合，寻找合适的解析器，然后打印异常日志 Collection\u0026lt;SpringBootExceptionReporter\u0026gt; exceptionReporters = new ArrayList\u0026lt;\u0026gt;(); //刷新系统属性java.awt.headless的值，如果没有值则设为true，这个值表示无头模式（意指缺少显示设备，键盘或鼠标的系统配置）， //在无头模式下java.awt.Toolkit将使用特定的无头模式下的实现类，因为就算没有显示设备，有些操作任能够被允许。 configureHeadlessProperty(); //① SpringApplicationRunListeners listeners = getRunListeners(args); listeners.starting(); try { //② ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); //配置系统参数spring.beaninfo.ignore,默认值为ture，字面意思是跳过搜索BeanInfo类，但具体是什么我暂时也不清楚。 configureIgnoreBeanInfo(environment); //③ Banner printedBanner = printBanner(environment); //④ context = createApplicationContext(); //⑤ exceptionReporters = getSpringFactoriesInstances(SpringBootExceptionReporter.class, new Class[]{ConfigurableApplicationContext.class}, context); //⑥ prepareContext(context, environment, listeners, applicationArguments, printedBanner); refreshContext(context); //刷新后的处理，是个空实现 afterRefresh(context, applicationArguments); //计时器结束 stopWatch.stop(); if (this.logStartupInfo) { new StartupInfoLogger(this.mainApplicationClass).logStarted(getApplicationLog(), stopWatch); } //发布started事件 listeners.started(context); //运行器回调，即实现了ApplicationRunner接口或CommandLineRunner接口的bean callRunners(context, applicationArguments); } catch (Throwable ex) { handleRunFailure(context, ex, exceptionReporters, listeners); throw new IllegalStateException(ex); } try { listeners.running(context); } catch (Throwable ex) { handleRunFailure(context, ex, exceptionReporters, null); throw new IllegalStateException(ex); } return context; } 2.代码注释 ① 内部又是调getSpringFactoriesInstances方法，取spring.factories中所有的SpringApplicationRunListener，然后对外暴露SpringApplicationRunListeners。 SpringApplicationRunListeners封装所有的SpringApplicationRunListener，用于容器启动间的事件发布到所有的SpringApplicationRunListener中。\nSpringApplicationRunListener中定义的方法有：\nvoid starting();首次启动run方法时立即调用。可用于非常早期的初始化。 void environmentPrepared(ConfigurableEnvironment environment);准备好环境（Environment构建完成），但在创建ApplicationContext之前调用。 void contextPrepared(ConfigurableApplicationContext context);在创建和构建ApplicationContext之后，但在加载之前调用。 void contextLoaded(ConfigurableApplicationContext context);ApplicationContext已加载但在刷新之前调用。 void started(ConfigurableApplicationContext context);ApplicationContext已刷新，应用程序已启动，但尚未调用CommandLineRunners和ApplicationRunners void running(ConfigurableApplicationContext context);在运行方法彻底完成之前立即调用，刷新ApplicationContext并调用所有CommandLineRunners和ApplicationRunner。 void failed(ConfigurableApplicationContext context, Throwable exception);在运行应用程序时失败时调用。 值得注意的是，started、running、failed方法是 SpringBoot2.0 才加入的。\n通过Debug，发现默认情况下加载的listeners有一个，类型为 EventPublishingRunListener。它在SpringBoot应用启动的不同时间点发布不同应用事件类型(ApplicationEvent)，如果有哪些事件监听者(ApplicationListener)对这些事件感兴趣，则可以接收并且处理。SpringApplicationRunListener与ApplicationListener的区别是SpringApplicationRunListener比ApplicationListener更靠前，SpringApplicationRunListener监听的是SpringApplication相关方法的执行，属于第一层监听器，他会发布相应的事件给ApplicationListener。\n② 根据不同的webApplicationType完成Environment的初始化，一般是使用StandardServletEnvironment实现类，Environment用于描述应用程序当前的运行环境，其抽象了两个方面的内容：配置文件(profile)和属性(properties)，其实就是对应的配置文件、环境变量、命令行参数里面的内容。这里Environment构建完成时发布了environmentPrepared事件,并且将最新的配置值绑定到了SpringbootApplication中，也就是当前的对象中。比如yml里面配的spring. main开头的一些属性值。\n③ 根据Enviroment中配置获取对应的banners没有则用默认的SpringbootBanner打印启动信息，就是启动应用时候控制台打印的logo\n④ 根据WebApplicationType,反射创建不同的ApplicationContext实现（Servlet是AnnotationConfigServletWebServerApplicationContext）。这里Servlet是AnnotationConfigServletWebServerApplicationContext，在他的父类GenericApplicationContext构造方法中,其中注入了一个DefaultListableBeanFactory,这个BeanFactory很关键，实际上AnnotationConfigServletWebServerApplicationContext的BeanFactory能力就是从DefaultListableBeanFactory扩展而来。 另外在这一步中也注册了ConfigurationClassPostProcessor、DefaultEventListenerFactory、EventListenerMethodProcessor、AutowiredAnnotationBeanPostProcessor、CommonAnnotationBeanPostProcessor这些beanDefinition，作为基础组件。ConfigurationClassPostProcessor这个组件是最重要的，其他的暂时没有深究什么作用，ConfigurationClassPostProcessor是BeanFactoryPostProcessor，负责在容器刷新时加载扫描配置类注解进行组件解析，注册BeanDefinition。\n⑤ 创建一系列SpringBootExceptionReporter，创建流程是通过SpringFactoriesLoader获取到所有实现SpringBootExceptionReporter接口的class，\n⑥ 初始化ApplicationContext，主要完成以下工作:\n将准备好的Environment设置给ApplicationContext 进一步执行ApplicationContext的后置处理，包括注册BeanName生成器， 设置资源加载器和类加载器，设置类型转换器ConversionService等等，这里的东西暂时不用深究。 遍历调用所有的ApplicationContextInitializer的 initialize()方法来对已经创建好的ApplicationContext进行进一步的处理。 调用SpringApplicationRunListener的 contextPrepared()方法，通知所有的监听者：ApplicationContext已经准备完毕。 创建启动类的beanDefiniton注册到容器中。 调用SpringApplicationRunListener的 contextLoaded()方法，通知所有的监听者：ApplicationContext已经装载完毕。 小结 容器刷新前，整个流程分三个步骤：\n初始化SpringApplication对象，比如设置webApplicationType，加载ApplicationListener，ApplicationContextInitializer。 初始化Environment对象，封装配置文件，命令行参数。 初始化ConfigurableApplicationContext（与ApplicationContext的区别是ConfigurableApplicationContext可以对容器进行写，而ApplicationContext只提供读的方法)，并且将启动类的beanDefiniton先行注册到容器中。 这是主线程可以看到的，其他没有看到的ApplicationContextInitializer与ApplicationListener干了什么暂时还没有进行深究。 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2020-02-22T00:00:00Z","permalink":"https://runningccode.github.io/2020/springboot%E6%B5%85%E6%9E%90%E4%BA%8C%E5%AE%B9%E5%99%A8%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/","title":"Springboot浅析（二）——容器启动流程"},{"content":"为什么学习Springboot勒，一方面主要实在是Springboot应用得太广泛了，加深对其的理解对自己的开发工作很有必要，因为如果遇到或业务场景需要进行一些稍微高级点的基于Springboot的扩展以及应用，可能就会不明白不理解。另一方面就是作为一个编码爱好者本身对springboot这么一个优秀的框架的好奇。文章基于SpringBoot2.1.9。\n一、SpringBoot的手动装配 （一） 手动装配的方式 主要包含以下几种方式：\n使用模式注解 @Component 等（Spring2.5+）,常用，但无法装配jar包目录的组件，为此可以使用 @Configuration 与 @Bean，手动装配组件 使用配置类 @Configuration 与 @Bean （Spring3.0+）,注册过多，会导致编码成本高，维护不灵活等问题。 使用模块装配 @EnableXXX 与 @Import （Spring3.1+）,@Import用于装配指定的类，@EnableXXX用于启用指定的@Import 前面一两种都是在项目中经常会用到的，就不过多介绍了，这里主要介绍下使用模块装配，这也是Spring各种Starter得以被装配的主要方式。\n（二）@EnableXXX与@Import的使用 这里分四种场景，导入普通类、导入配置类、导入ImportSelector，导入ImportBeanDefinitionRegistrar。接下来用4个小demo来演示怎么使用。\n1.导入普通类 我们的目标是导入一个TestService类，TestService是一个普通的类。\n步骤 （1）新建一个注解EnableTest @Documented @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Import(TestService.class) public @interface EnableTest { } （2）在能被扫描的配置类或启动类标注 @EnableTest @SpringBootApplication @EnableTest public class SpringbootexampleApplication { public static void main(String[] args) { SpringApplication.run(SpringbootexampleApplication.class, args); } } 2.导入配置类 没有位与启动类下的@Configuration注解默认情况是不会被扫描的，这个时候要装配@Configuration标注的类以及其下面注册的bean，可以通过这种方式手动装配。\n步骤 （1）新建一个配置类 @Configuration public class TestRegistrarConfiguration { @Bean public Test2Service yellow() { return new Test2Service(); } } (2)修改EnableTest，添加该配置类的引入 @Documented @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Import({TestService.class,TestRegistrarConfiguration.class}) public @interface EnableTest { } 这样启动时也间接装配了TestRegistrarConfiguration配置类下面的所有bean。\n3.导入ImportSelector ImportSelector接口规定了String[] selectImports(AnnotationMetadata importingClassMetadata)方法,这个方法的返回值String数组，表示要装配的bean的className数组。SpringBoot的自动装配，实际上就是用的这种方式。\n步骤 (1) 新建一个ImportSelector public class TestImportSelector implements ImportSelector { @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) { // 返回类.getName()。 return new String[]{Test3Service.class.getName()}; } } 注：参数importingClassMetadata，代表的是被 @Import 标记的类的信息。（不算注解），这里就是@EnableTest修饰的配置类即SpringbootexampleApplication。\n（2）修改EnableTest,添加该导入选择器的引入 @Documented @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Import({TestService.class, TestRegistrarConfiguration.class, TestImportSelector.class}) public @interface EnableTest { } 4. 导入ImportBeanDefinitionRegistrar ImportBeanDefinitionRegistrar接口规定了 void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry)方法， 这里有两个参数AnnotationMetadata跟前面ImportSelector接口方法参数一样，registry是用于注册BeanDefinition的注册者，BeanDefinition是整个Bean信息的封装，容器刷新时可以根据容器类的BeanDefinition创建实例。\n步骤 (1)新建一个ImportBeanDefinitionRegistrar public class TestImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar { @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { registry.registerBeanDefinition(\u0026quot;Test4Service\u0026quot;, new RootBeanDefinition(Test4Service.class)); } } （2）修改EnableTest,添加该导入bean定义的注册器的引入 @Documented @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Import({TestService.class, TestRegistrarConfiguration.class, TestImportSelector.class, TestImportBeanDefinitionRegistrar.class}) public @interface EnableTest { } 二、SpringBoot的自动配置 自动配置其实就是Spring框架应用了手动装配的原理，装配了自己的默认组件，也提供了一些扩展点，可以让应用扩展我们自己的组件。当然，SpringBoot对于应用basePackage下面扩展的组件通过组件扫描@ComponentScan也能完成配置类的装配。\n（一）简单看下@ComponentScan 我们知道@SpringBootApplication是一个组合注解，包含@SpringBootConfiguration、@EnableAutoConfiguration、@ComponentScan。如下：\n@SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication 这里@ComponentScan中默认定义了两个过滤器，TypeExcludeFilter与AutoConfigurationExcludeFilter，其中TypeExcludeFilter的核心代码如下：\npublic boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException { if (this.beanFactory instanceof ListableBeanFactory \u0026amp;\u0026amp; getClass() == TypeExcludeFilter.class) { //获取容器内实现了TypeExcludeFilter的所有bean Collection\u0026lt;TypeExcludeFilter\u0026gt; delegates = ((ListableBeanFactory) this.beanFactory) .getBeansOfType(TypeExcludeFilter.class).values(); for (TypeExcludeFilter delegate : delegates) { //遍历获取的TypeExcludeFilter的bean，如果满足，则通过 if (delegate.match(metadataReader, metadataReaderFactory)) { return true; } } } return false; } 主要提供一个扩展点，获取容器内的所有TypeExcludeFilter类型的bean，执行一下match方法，返回true的话，则进行过滤。其中两个参数metadataReader用于读取扫描得到的类型信息。metadataReaderFactory用于获取其他类型的metadataReader。比如如果不想扫描某个类，则注册一个TypeExcludeFilter的子类bean，其match方法的逻辑是从metadataReader获取ClassName，然后判断若和xxxClass相等则返回true进行过滤就行了。\nAutoConfigurationExcludeFilter的作用则是排除是否是自动装配的Configuration，如果是则进行扫描过滤，其核心代码如下：\n@Override public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException { return isConfiguration(metadataReader) \u0026amp;\u0026amp; isAutoConfiguration(metadataReader); } private boolean isConfiguration(MetadataReader metadataReader) { return metadataReader.getAnnotationMetadata().isAnnotated(Configuration.class.getName()); } private boolean isAutoConfiguration(MetadataReader metadataReader) { return getAutoConfigurations().contains(metadataReader.getClassMetadata().getClassName()); } protected List\u0026lt;String\\\u0026gt; getAutoConfigurations() { if (this.autoConfigurations == null) { this.autoConfigurations = SpringFactoriesLoader.loadFactoryNames(EnableAutoConfiguration.class, this.beanClassLoader); } return this.autoConfigurations; } isConfiguration(metadataReader) 判断是否带有@Configuration注解， isAutoConfiguration(metadataReader)判断是否是自动装配类（后面仔细介绍这里的自动装配方式），所以通过这我们知道@Component组件扫描和自动装配机制并不冲突。\n二、自动配置原理 核心注解是@EnableAutoConfiguration。其内容如下：\n@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @AutoConfigurationPackage @Import(AutoConfigurationImportSelector.class) public @interface EnableAutoConfiguration （一）AutoConfigurationPackage的作用 其中，@AutoConfigurationPackage的内容如下：\n@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @Import(AutoConfigurationPackages.Registrar.class) public @interface AutoConfigurationPackage 这里引入了一个AutoConfigurationPackages的内部类：Registrar,来看看内容：\n/** * {@link ImportBeanDefinitionRegistrar} to store the base package from the importing * configuration. * （用于保存导入的配置类所在的根包。） */ static class Registrar implements ImportBeanDefinitionRegistrar, DeterminableImports { @Override public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { register(registry, new PackageImport(metadata).getPackageName()); } @Override public Set\u0026lt;Object\u0026gt; determineImports(AnnotationMetadata metadata) { return Collections.singleton(new PackageImport(metadata)); } } register(registry, new PackageImport(metadata).getPackageName())掉用了外部类AutoConfigurationPackages的register方法，因为metadata这里实际上就是启动类的元数据，所以new PackageImport(metadata).getPackageName()返回的就是启动类的包路径。再看看AutoConfigurationPackages的register方法：\npublic static void register(BeanDefinitionRegistry registry, String... packageNames) { private static final String BEAN = AutoConfigurationPackages.class.getName(); public static void register(BeanDefinitionRegistry registry, String... packageNames) { // 判断 BeanFactory 中是否包含名为AutoConfigurationPackages.class.getName()的beanDefinition if (registry.containsBeanDefinition(BEAN)) { BeanDefinition beanDefinition = registry.getBeanDefinition(BEAN); ConstructorArgumentValues constructorArguments = beanDefinition.getConstructorArgumentValues(); // addBasePackages：merge当前的packageNames参数（根包）到原来的构造器参数里面 constructorArguments.addIndexedArgumentValue(0, addBasePackages(constructorArguments, packageNames)); } else { //构造一个BasePackages的BD，设置构造器参数为当前的packageNames参数 GenericBeanDefinition beanDefinition = new GenericBeanDefinition(); beanDefinition.setBeanClass(BasePackages.class); beanDefinition.getConstructorArgumentValues().addIndexedArgumentValue(0, packageNames); beanDefinition.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); registry.registerBeanDefinition(BEAN, beanDefinition); } } } 综上，AutoConfigurationPackage的作用就是注册一个BasePackage的BeanDefinition，其构造函数包含EnableAutoConfiguration标识的类的包路径。\n（二）@Import(AutoConfigurationImportSelector.class)的作用 看一下AutoConfigurationImportSelector：\n@Override public String[] selectImports(AnnotationMetadata annotationMetadata) { if (!isEnabled(annotationMetadata)) { return NO_IMPORTS; } AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader .loadMetadata(this.beanClassLoader); // 加载自动配置类 AutoConfigurationEntry autoConfigurationEntry = getAutoConfigurationEntry(autoConfigurationMetadata, annotationMetadata); return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations()); } 返回值是StringUtils.toStringArray(autoConfigurationEntry.getConfigurations())，重点看看autoConfigurationEntry里面的Configurations怎么生成的，进入getAutoConfigurationEntry方法：\nprotected Class\u0026lt;?\u0026gt; getSpringFactoriesLoaderFactoryClass() { return EnableAutoConfiguration.class; } protected List\u0026lt;String\u0026gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) { List\u0026lt;String\u0026gt; configurations = SpringFactoriesLoader.loadFactoryNames(getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); Assert.notEmpty(configurations, \u0026quot;No auto configuration classes found in META-INF/spring.factories. If you \u0026quot; + \u0026quot;are using a custom packaging, make sure that file is correct.\u0026quot;); return configurations; } 这里的关键是 SpringFactoriesLoader.loadFactoryNames(getSpringFactoriesLoaderFactoryClass(),getBeanClassLoader());前面一个参数是EnableAutoConfiguration.class，后面是一个类加载器。 SpringFactoriesLoader.loadFactoryNames(getSpringFactoriesLoaderFactoryClass(),getBeanClassLoader());这个方法在Springboot源码中是非常非常常见的，比如容器启动获取初始化器ApplicationContextInitializer实例等等，接下来看看这里做了什么：\nblic static final String FACTORIES_RESOURCE_LOCATION = \u0026quot;META-INF/spring.factories\u0026quot;; public static List\u0026lt;String\u0026gt; loadFactoryNames(Class\u0026lt;?\u0026gt; factoryClass, @Nullable ClassLoader classLoader) { String factoryClassName = factoryClass.getName(); //通过factoryClassName的className从一个map里面取相应的className集合 return loadSpringFactories(classLoader).getOrDefault(factoryClassName, Collections.emptyList()); } private static Map\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; loadSpringFactories(@Nullable ClassLoader classLoader) { //如果有，这从缓存里面拿，否则去生成 MultiValueMap\u0026lt;String, String\u0026gt; result = cache.get(classLoader); if (result != null) { return result; } //使用 classLoader 去加载了指定常量路径下的资源： FACTORIES_RESOURCE_LOCATION ，而这个常量指定的路径实际是：META-INF/spring.factories 。 try { Enumeration\u0026lt;URL\u0026gt; urls = (classLoader != null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION)); result = new LinkedMultiValueMap\u0026lt;\u0026gt;(); while (urls.hasMoreElements()) { URL url = urls.nextElement(); UrlResource resource = new UrlResource(url); //以 Properties 的形式加载 Properties properties = PropertiesLoaderUtils.loadProperties(resource); for (Map.Entry\u0026lt;?, ?\u0026gt; entry : properties.entrySet()) { String factoryClassName = ((String) entry.getKey()).trim(); for (String factoryName : StringUtils.commaDelimitedListToStringArray((String) entry.getValue())) { //键为Properties文件的键，值为properties文件的值用逗号分隔 result.add(factoryClassName, factoryName.trim()); } } } cache.put(classLoader, result); return result; } catch (IOException ex) { throw new IllegalArgumentException(\u0026quot;Unable to load factories from location [\u0026quot; + FACTORIES_RESOURCE_LOCATION + \u0026quot;]\u0026quot;, ex); } } spring.factories可以在任意工程的meta-inf目录，例如我们可以在 spring-boot-autoconfiguration 包下找到spring.factories文件如图：\nimg.png\r综上@Import(AutoConfigurationImportSelector.class)的作用就是加载spring.factories文件下标识的以EnableAutoConfiguration的className为键的classNames集合，然后把这些对应的class的BeanDefinition注册到容器。如果有对JDK的SPI机制有了解的朋友会感觉很熟悉，这里的这个机制有点类似JDK原生的SPI机制， 都是从配置文件获取相应的className并实例化，jdk的SPI指的是扩展者针对某接口自己定义实现类，做法也是在meta-inf/service目录下对对应接口的配置文件内容进行编辑，标识实现类的className。但是个人认为还是与SpringFactoryLoader这里有些区别。JDK的SPI是将加载的类抽象为策略（策略模式），通过抽象类加载配置文件配置的实现。Spring更像是将加载的类抽象为产品（工厂模式），通过给定工厂的类全限定名加载配置文件配置的类全限定名，从而可以以此加载配置的类。\n（三）小结 自动配置的核心注解@EnableAutoConfiguration主要做了两件事：\n装配AutoConfigurationPackages.Registrar，注册者注册了一个类为BasePackages的BeanDefinition，并且在这个BD的构造器参数上添加了启动类路径。 装配AutoConfigurationImportSelector，选择器扫描了META-INF目录下spring.factories文件中键为EnableAutoConfiguration.class.getName的所有类。将其BeanDefinition注册进入了容器。 三、总结 今天主要介绍了手动装配与自动配置，了解了Springboot自动配置的原理。通过此我们在平时也可以开发一些自己针对其他框架的封装，或者自己开发一套组件，然后其他项目引入jar包自动进行配置，不用再自己进行一次配置，是很方便的。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2020-02-21T00:00:00Z","permalink":"https://runningccode.github.io/2020/springboot%E6%B5%85%E6%9E%90%E4%B8%80%E8%87%AA%E5%8A%A8%E9%85%8D%E7%BD%AE/","title":"Springboot浅析（一）——自动配置"},{"content":"业务中经常会涉及到一些初始化操作如加载数据到内存，初始化成员变量等等一系列操作。现将各个方式总结一下。\n一、 bean的生命周期 首先我们要明白一个springboot下Web环境里面bean的生命周期:\nbean的实例化\n注入对象属性\n处理XXXXAware接口,包含EnvironmentAware、EmbeddedValueResolverAware、ResourceLoaderAware、ApplicationEventPublisherAware、MessageSourceAware、ApplicationContextAware，执行其特定的方法。\n扫描所有BeanPostProcessor接口。传入当前对象执行定义的所有postProcessBeforeInitialization(Object bean, String beanName)方法。\n执行@PostConstruct注解方法\n处理InitializingBean接口的 afterPropertiesSet()方法。\n处理IOC配置文件当前bean配置的init-method方法（很少用）。\n扫描所有的BeanPostProcessor接口。传入当前对象执行定义的所有postProcessAfterInitialization(Object bean, String beanName)方法。 9 容器刷新完成扫描实现Lifecycle接口的bean根据需要调用其start方法。\n容器刷新完成事件广播扫描实现ApplicationListener接口的Bean，并调用其onApplicationEvent方法。\n启动最后一步扫描实现了ApplicationRunner或CommandRunner的bean，调用其对应方法。\n容器close时扫描实现Lifecycle接口的bean根据需要调用其stop方法。\n当bean销毁时，一般是例如Spring上下文初始化失败时，或在Spring容器关闭时（正常关闭kill -15，而不是kill -9），会处理DisposableBean接口的destroy方法以及@PreDestroy注解的方法，其中@PreDestroy注解的方法较DisposableBean接口的destroy方法先执行。\n总结了下流程图：\nimg.png\r二、给应用添加初始化操作的方式 常用的给应用添加初始化操作的方式，按执行顺序排列的话：\n实现XXXAware接口。 实现InitializingBean接口。 实现ApplicationListener接口，注册容器刷新后的监听器。 实现接口ApplicationRunner接口。 实现SmartLifecycle接口。 注：其中1和2的区别是1有特定的用途，用于注入特定的对象，可以通过@DependOn注解标识依赖的beanName,控制bean之间的初始化顺序。3和4的区别是4是整个应用启动完成后执行，3是在启动过程中容器刷新完成的时候。5和3效果差不多，但是5比3早一点 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2020-02-19T00:00:00Z","permalink":"https://runningccode.github.io/2020/%E5%BA%94%E7%94%A8%E5%90%AF%E5%8A%A8%E5%88%9D%E5%A7%8B%E5%8C%96%E6%93%8D%E4%BD%9C%E7%9A%84%E5%90%84%E4%B8%AA%E6%96%B9%E5%BC%8F%E5%8F%8A%E5%8C%BA%E5%88%AB%E6%80%BB%E7%BB%93/","title":"应用启动初始化操作的各个方式及区别总结"},{"content":"一、 网络模型 网络模型分两种，一种是OSI模型，一种是TCP/IP模型，后者应用更加广泛。这里也主要介绍TCP/IP模型。\n（一）TCP/IP模型 首先分为4层，从上到下依次是应用层、传输层、网络层、数据链路层。 OSI模型中将网络分为：应用层、表示层、会话层、传输层、网络层、数据链路层、物理层。 TCP/IP的应用层是OSI模型中应用层、表示层、会话层的集合，而物理层由于不是我们经常考虑的问题，所以TCP/IP模型没有把物理层算上。\n1、数据链路层 数据链路层的核心是以太网协议。以太网协议规定一组电信号是一个数据包，叫一个振，每个帧（frame）分为标头（head）和数据（data），标头包含一些说明性东西，比如发送者，接收者，和数据类型之类的。例如一个电脑发个数据包出去，会广播给局域网(子网)内所有电脑设备的网卡，然后每台设备都从数据包获取接收者的mac地址与自己网卡的mac地址比对，如果一样就说明这是发给自己的数据包。\n2、网络层 定义了一套IP协议，有IPV4和IPV6，以IPV4为例，由32个二进制数字组成，用4个10进制数字表示。\nIP地址分为三类：\nA类：第一个字节为网络号，后三个字节为主机号。该类IP地址的最前面为“0”，所以地址的网络号取值于1~126之间。一般用于大型网络。 B类：前两个字节为网络号，后两个字节为主机号。该类IP地址的最前面为“10”，所以地址的网络号取值于128~191之间。一般用于中等规模网络。 C类：前三个字节为网络号，最后一个字节为主机号。该类IP地址的最前面为“110”，所以地址的网络号取值于192~223之间。一般用于小型网络。 这三类IP地址构成了三级网络，将网络划分为了三层，不同子网通过上层网关进行转发。\nip之间的通信\n同一个子网的通信： ip之间是如何通信的勒，这里有一个子网以及子网掩码的概念。比如192.168.56.1和192.168.56.2判断是否是一个子网，子网掩码是255.255.255.0，子网掩码用来与ip地址进行与运算，得到的一个值是否完全一样来判断是否是属于一个子网。192.168.56.1与255.255.255.0进行位与与192.168.56.2与255.255.255.0进行位与是一样的，都为192.168.56.0,即他们是同一个子网。同一个子网之间的Ip可以直接进行网络通信，因为子网内的所有设备会上传自己的ip地址和mac地址映射，设备会缓存同一子网其他设备的Ip地址以及mac地址映射，当拆开IP协议层数据包时可以迅速以目标IP查询ARP缓存，以这个mac地址构建数据链路层的数据包。\n不同子网的通信 那么不同子网又是如何联通的勒？这里就是网关发挥作用的时候了，网关有多张网卡。以自己家里的路由器为例，也是一个网关，路由器有一张网卡与部的网关相连，也有一张网卡与内部设备进行相连。当然路由器与网关相连使用的是一个公网ip，这里有一个NAT转换的概念，路由器上装有NAT软件，可以将私有网络（也就是我们自己家连上这台路由器所有的网络设备）的私有ip转换为同一个公网ip，这样做主要是为了解决ip不足的问题，因为ipv4协议就那么几位，ip肯定是不够的。路由器在内网与公网之间中起着转发者的作用，当然路由器上面也有网关，当我们要访问某个Ip也不在路由器所有网卡位与的子网中时还需要进行多个网关的转发，这种转发路径由路由表决定。路由表的的生成方式可以是动态（协议共享）或者静态（手动配置）的，当匹配路由表都失败时使用缺省路由，路由到相连的其他网关。\n3.传输层 传输层的协议有TCP协议或UDP协议，TCP协议是一套基于端口的点对点通信协议，规定包括如何连接，如何发送，读取消息。是有连接的，不允许丢数据。 UDP则不需要建立连接，传完数据也不需要确认，允许丢数据。 针对此层协议的操作，可使用SocketAPI进行编程，其函数create，listen，accept，connect，read和write等等抽象了TCP/IP协议的一些操作。\n4.应用层 合并了OSI中的应用层会话层表示层，协议有比如常见的HTTP协议，邮件协议等等，定义的是通过TCP拿到数据之后怎么处理的协议。\n二、常见问题 （一）请求网址的整个过程 请求dns服务器，解析得到访问域名的ip 开始打包数据包经过应用层（按HTTP协议封装成HTTP的数据包），传输层（按TCP协议封装数据包-设置端口），网络层（按IP协议封装数据包），数据链路层（按以太网协议封装数据包）将数据包发送给网关。因为在网络层时通过子网掩码判断不在一个子网，所以会直接发送给网关。以太网包是有限制的，上述的数据包可能会被切为多个包。 网关收到数据包后，会进行路由表进行路由 经过多次路由后到达了目标IP所对应的服务器 对应的服务器依次层层解包，获取http请求报文，处理请求然后又层层封包返回响应 （二）TCP协议的三次握手、四次挥手 首先解释下涉及到的TCP数据包关键头信息。\n标志位含义\nSYN：同步，1和0表示是否是建立联机的数据包 ACK：确认，1和0表示确认号是否有效 FIN：终止，1和0表示是否要求释放资源，复位连接，即释放运输连接 序号含义\nseq 序号。TCP将一次连接中所有发送的字节都编上序号，seq表示本报文段的序号。 ack 确认号,期待收到对方下一个报文段的序号。 注：通常情况下数据传输阶段返回的ack为上一个接收包的seq+len,但三次握手阶段和四次挥手则直接为seq+1。即三次握手四次挥手的序号计算方式忽略了报文长度。\n注意：ACK、SYN和FIN这些大写的单词表示标志位，其值要么是1，要么是0；ack、seq小写的单词表示序号。\n1、三次握手、 第一次握手 客户端发SYN，具体是SYN=1,seq = x(随机值)，然后客户端处于SYN_SENT状态（同步已发送） 第二次握手 服务器返回SYN+ACK,具体是SYN=1,ACK=1,seq=y(随机值),ack=x+1，然后服务端状态处于SYN_RECV状态（同步已收到） 第三次握手 客户端发送ACK。具体是ACK=1,seq=x+1,ack=y+1，当包发送完毕后，客户端和服务器进入ESTABLISHED（TCP连接成功）状态 为什么是三次？为了让双方都知道彼此已准备好。设想一个场景，客户端很久之前的第一次握手到达服务器了，但是在这之前，后面重试的握手请求已经建立了连接发送了数据，那么在这个时候客户端是不认识服务器的响应的，所以当服务器返回响应的时候，客户端不会认识这个响应，无法建立连接。\n2、四次挥手 第一次挥手 客户端发FIN(FIN=1,seq=u) 第二次挥手 服务器ACK（ACK=1,ack=u+1,seq=v） 第三次挥手 服务器FIN(FIN=1,ack=u+1,seq=w) 第四次挥手 客户端ACK(ACK=1,seq=u+1,ack=w+1) 为什么连接的时候是三次握手，关闭的时候却是四次握手？因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，\u0026ldquo;你发的FIN报文我收到了\u0026rdquo;。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四步握手。另外，第四次挥手后，客户端会等待一段时间在释放连接，确保不会再次收到服务器的第三次挥手（服务器如果没收到客户端的第四次挥手则会尝试重发第三次挥手）。\n（三）HTTP协议 1、1.0,1.1,2.0的区别 1.0 一个请求建立关闭一次TCP连接,大量时间资源耗费在建立连接和关闭连接中 1.1 建立一次TCP连接之后不会马上关闭，过一段时间再关闭 2.0 多路复用，并行发送请求，而1.1是串行发送请求 2、HTTPS原理 也就是HTTP+SSL协议,具体流程如图：\nimg.png\r问题：\n客户端如何验证网站返回的证书？ 通过本地权威证书的公钥解开发过来的证书，得到证书的信息摘要，对比证书内容。\n为什么中间用了数字信封(即单钥加密消息，公私钥只用于加密单钥)？ 因为单钥加解密比非对称加密效率高。非对称加密的核心效果只是用作签名（保证接收者接收的信息来自受信任的私钥密码拥有者或发送的消息只能被拥有私钥方解开）。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2020-02-16T00:00:00Z","permalink":"https://runningccode.github.io/2020/%E9%9D%9E%E7%A7%91%E7%8F%AD%E7%94%9F%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%BF%85%E4%BC%9A%E7%9F%A5%E8%AF%86%E7%82%B9%E5%BD%92%E7%BA%B3/","title":"非科班生网络通信必会知识点归纳"},{"content":"本文是学习极客时间《DDD实战课》后结合自己思考所整理的归纳总结，课程链接在：\nDDD实战课 基于DDD的微服务拆分与设计\n一、DDD名词解释 （一）域 对业务领域细分后对一定范围内的问题划定边界，这个范围内的问题组成了域，域是有层次的，一个域可以由多个‘子域’聚合。 子域的种类可以分为：\n核心域 决定公司产品核心竞争力的子域 通用域 同时被多个子域使用的功能的子域 支撑域 既不包含决定产品和公司核心竞争力的功能，也不包含通用功能的子域，它就是支撑域，例如数据代码类的数据字典等系统。 子域还可根据需要进一步拆分为子子域，比如，支付子域可继续拆分为收款和付款子子域。拆到一定程度后，有些子子域的领域边界就可能变成限界上下文的边界了。\n（二）限界上下文 限界上下文定义领域模型的边界，每个领域模型都有自己的领域边界，在领域边界内即限界上下文。限界上下文内限制了统一的通用语言（在限界上下文当中在事件风暴过程中，通过团队交流达成共识的，能够简单、清晰、准确描述业务涵义和规则的语言就是通用语言），以及领域对象等。一般来说，在微服务架构中，不考虑其他因素的话（如技术异构、团队沟通等）可以按照限界上下文来定义微服务。\n（三）聚合、实体、值对象、聚合根 实体和值对象是聚合的基础单元，聚合是构成领域模型的基础单元。在事件风暴中，我们会根据一些业务操作和行为找出实体（Entity）或值对象（ValueObject），进而将业务关联紧密的实体和值对象进行组合，构成聚合，再根据业务语义将多个聚合划定到同一个限界上下文（Bounded Context）中并在限界上下文内完成领域建模。\n聚合 聚合就是由业务和逻辑紧密关联的实体和值对象组合而成的 实体 用来描述业务操作和行为的基础载体，有唯一标识符。 值对象 实体有可能聚合了值对象，值对象相当于一个多个字段的数据载体，依赖于实体存在，不包含业务逻辑，没有唯一标识符，在业务中只涉及整体替换和数据初始化。在数据库建模中有两个方式，一个方式是将一个字段设计为大字段存放值对象的Json,一个方式是将值对象的所有字段都与实体的其他属性字段并列来创建新的字段。 聚合根 也被称为根实体，聚合之间的通信通过聚合根来管理，它以聚合根 ID 关联的方式接受外部任务和请求，也就是说，一个聚合内如果需要访问其它聚合的实体，就要先访问聚合根，再导航到聚合内部实体，外部对象不能直接访问聚合内实体。 （四）领域事件 DDD提倡聚合之间产生的业务协同使用领域事件的方式来完成，领域事件就是将上游聚合处理完成这个动作通过事件的方式抽象，封装完成下游聚合所需的数据，通过消息队列中间件（跨微服务）或本地应用中的消息总线框架（同一个微服务跨聚合）来完成聚合之间业务的解耦。\n二、DDD 分层架构以及实战中代码分层 （一）四层架构 img.png\r用户接口层 用于提供统一对外的接口，同样一个应用服务可能用于不同的端，入参和出参不太一样，所以在上游添加一个用户接口层对不同格式入参出参进行处理，提供对应用服务层的复用。\n应用层 对各个领域服务或其他微服务的提供的接口进行编排，协作完成业务操作。应用服务粒度较粗，需要注意可复用性。应用服务还可以进行安全认证、权限校验、事务控制、发送或订阅领域事件等。另外，应用层代码较薄，不包含业务规则或逻辑，主要是控制业务流程走向，逻辑部分委托给领域层。\n领域层 主要包含实体类、值对象类、事件类、领域服务。领域服务供应用层调用，实现业务的规则和逻辑，但对于只与单一实体自己相关的一些动作，这部分逻辑由实体类自己来实现。\n基础层 基础层是贯穿所有层的，它的作用就是为其它各层提供通用的技术和基础服务，包括第三方工具、驱动、消息中间件、网关、文件、缓存以及数据库等。\n（二）代码分层参考 img_1.png\rimg_2.png\r1、api （方便deploy,供同步调用） 主要包括:\ndto包，即数据传输对象，即内部接口之间的数据传输字段封装。包括 requestDTO 和 responseDTO 两部分。 api接口，即api接口，供feignClient接口继承。 2、Interfaces（用户接口层） 二级目录包括：\nAssembler 实现 DTO 与领域对象之间的相互转换和数据交换。一般来说 Assembler 与 DTO 总是一同出现。 Facade 提供较粗粒度的调用接口，将用户请求委派给一个或多个应用服务进行处理。 dto 接口间数据传输的封装 3、Application（应用层） 二级目录有：\nService（应用服务）：对多个领域服务或其他微服务应用服务进行封装、编排和组合，对外提供粗粒度的服务。可以将所有应用服务放在一个应用服务类里，也可以把一个应用服务设计为一个应用服务类，以防应用服务类代码量过大。 4、Domain（领域层） 由一个或多个聚合包构成，共同实现领域模型的核心业务逻辑。聚合包内的目录：\nentity（实体） 存放聚合根、实体、值对象以及工厂模式（处理DO到PO的初始化,DO到PO的初始化，指当聚合根被创建时，聚合内所有依赖的对象将会被同时创建，将所有依赖的DO对象一次性转换为PO对象）相关代码。 event（事件） 存放事件实体以及与事件活动相关的业务逻辑代码，分为publish和subscribe目录，publish存放事件发布相关代码，listener存放事件订阅相关代码。 service(领域服务) 存放领域服务代码。一个领域服务是多个实体组合出来的一段业务逻辑 repository（仓储）仓储实现本应该属于基础层代码，但在微服务架构中的演进过程中会涉及到聚合的拆分，所以这里把仓储层放在这是为了与聚合进行整体迁移方便，其中包括facade、mapper、persistence包，facade为统一为其他层提供服务定义的接口，mapper是各个框架中需实现的mapper接口，persistence用于PO与查询结果和查询条件间的适配。 5、Infrastructure（基础层） 二级目录包括：\nconfig 主要存放配置相关代码。 util 主要存放平台、开发框架、消息、数据库、缓存、文件、总线、网关、第三方类库、通用算法等基础代码 client 存放feignClient。 三、中台建设与DDD （一）什么是中台 我对中台的粗浅的理解是中台首先是一种基础的理念架构，没有很固定的规范，核心思想就是构建一个个小基础服务，联通起来共同支撑上游的业务，最大的好处就是能力的复用。\n（二）用DDD思想完成中台建模 首先，用DDD去理解中台架构的话，对应关系可以如图示：\nimg_3.png\r我们完成了DDD领域建模，也就完成了中台建模，那么具体如何做勒？ 有两种策略：\n自顶而下的策略,自顶而下逐级梳理领域，最后细分到领域，划定中台边界,适用于推倒重建，或新开发的情况。\n自底向上的策略，先用事件风暴（后文会说）分别梳理原来各个系统的各个聚合，划分限界，构成领域模型（对应上图中的领域模型，一个领域模型对应一个中台服务，由多个聚合构成），然后对梳理的领域模型进行归类，对齐业务域（对应上图中的子域，，比领域模型更抽象，比如用户认证、权限共同属于用户业务域，划分业务域的意义在于划分不同的中台产品，不同产品的人力投资及其他可能不一样），然后对同一业务域中的领域模型的差异和共同点进行分析（粒度控制在聚合，尽量不要在聚合内部划分），将通用功能进行沉淀，重新定义限界上下文，划分领域模型，然后再来根据进行领域模型归类（通用中台，核心中台），然后这样重构后，原有系统中的聚合该合并的合并，该迁移的迁移。\n四、事件风暴实践-构建领域模型的关键 （一）事件风暴的核心 以产品愿景为核心，分析业务场景，集中关注业务场景中的事件、命令和实体等领域对象，找出实体、聚合根、划分聚合、构建限界上下文，拆分为微服务。\n（二）事件风暴的准备工作 1、事件风暴的参与者 可以包括：\n领域专家，即对业务或问题域有深刻见解的主题专家， DDD 专家 架构师 产品经理 项目经理 开发人员 测试人员等 2、 准备材料 即时贴和水笔，用于“刷墙”，即将不同颜色的即时贴贴在墙上\nimg_4.png\r3、场地 足够长的墙和足够大的空间。\n（三）事件风暴的过程 1、梳理产品愿景 由领域专家、业务需求方、产品经理、项目经理和开发经理参与，目的是对产品顶层价值进行设计，使产品目标用户、核心价值、差异化竞争点等信息达成一致，避免产品偏离方向。比如梳理成以下愿景墙，以设计一个用户中台产品为例，每个人对每一个点发表意见，贴在墙上：\nimg_5.png\r2、业务场景分析 由领域专家、产品经理、需求分析人员、架构师、项目经理、开发经理和测试经理参与，从用户视角出发的，根据业务流程或用户旅程，采用用例和场景分析，探索领域中的典型场景，找出领域事件、实体和命令等领域对象，支撑领域建模。以设计用户中台为例，用户中台有这样三个典型的业务场景：第一个是系统和岗位设置，设置系统中岗位的菜单权限；第二个是用户权限配置，为用户建立账户和密码，设置用户岗位；第三个是用户登录系统和权限校验，生成用户登录和操作日志。一步一步搜寻用户业务流程中的关键领域事件，比如岗位已创建，用户已创建等事件。再找出什么行为会引起这些领域事件，这些行为可能是一个或若干个命令组合在一起产生的，比如创建用户时，第一个命令是从公司 HR 系统中获取用户信息，第二个命令是根据 HR 的员工信息在用户中台创建用户，创建完用户后就会产生用户已创建的领域事件。当然这个领域事件可能会触发下一步的操作，比如发布到邮件系统通知用户已创建，但也可能到此就结束了。梳理如图：\nimg_6.png\r3、领域建模 由领域专家、产品经理、需求分析人员、架构师、项目经理、开发经理和测试经理参与，根据场景分析过程中产生的领域对象，比如命令、事件等之间关系，找出产生命令的实体，分析实体之间的依赖关系组成聚合，为聚合划定限界上下文，建立领域模型以及模型之间的依赖。\n(1) 第一步-找出实体 从前面得到命令和事件中提取产生这些行为的实体，如图，对上面分析得到的命令和事件进行梳理得到对应的实体，用绿色贴纸表示：\nimg_7.png\r（2）第二步-找出聚合根，划分聚合 根据聚合根的管理性质在前面的实体中找到聚合根，然后根据业务依赖和业务内聚原则，将聚合根以及它关联的实体和值对象组合为聚合。比如前面的系统和菜单实体可以组合为“系统功能”聚合，按此方方法，用户中台就有了系统功能、岗位、用户信息、用户日志、账户和认证票据六个聚合。。\n（3）划定限界上下文，将聚合归类 我的理解是就是将前面的聚合根据业务进一步抽象归类，以便划分微服务。如图，将前面得到的聚合进行归类，分为了三个领域模型：\nimg_8.png\r另外：\n由于领域建模的过程中产生的领域对象实在太多了，我们可以借助表格来记录，如图：\nimg_9.png\r4、 划分微服务 理论上一个领域模型可以为一个微服务，但也不绝对，也有其他需要考虑的因素如技术异构，比如用户日志数据量巨大，大到需要采用大数据技术来实现，这时用户信息聚合与用户日志聚合就会有技术异构。这两个聚合就不适合放在一个微服务里面了。另外还有需要考虑比如将敏态与稳态业务的分离、非功能性需求（如弹性伸缩要求、安全性等要求）、团队组织和沟通效率、软件包大小以及技术异构（使用的技术环境不一样）等非业务因素。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2020-02-04T00:00:00Z","permalink":"https://runningccode.github.io/2020/ddd%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E8%AF%BBddd%E5%AE%9E%E6%88%98%E8%AF%BE%E5%BD%92%E7%BA%B3/","title":"DDD领域驱动设计思想——读《DDD实战课》归纳"},{"content":"各种线程协作工具 常见线程协作工具： 读写锁ReadWriteLock 信号量Semaphore 倒计时门栓CountDownLatch 循环栅栏CyclicBarrier 线程本地变量ThreadLocal 主要讲信号量Semaphore，倒计时门栓CountDownLatch， 循环栅栏CyclicBarrier 然后根据两个LeetCode题目来应用一下。\n第一题： 三个不同的线程将会共用一个 Foo 实例。\n线程 A 将会调用 one() 方法 线程 B 将会调用 two() 方法 线程 C 将会调用 three() 方法 请设计修改程序，以确保 two() 方法在 one() 方法之后被执行，three() 方法在 two() 方法之后被执行。\n原生解法 应用场景及用法 基于notify/wait，所有的线程间通信类似通知的机制本质上都是notifyAll，多能用这个来实现。\nclass Foo { private static AtomicInteger flag = new AtomicInteger(0); private static Object Lock1 = new Object(); public Foo() { } public void first(Runnable printFirst) throws InterruptedException { synchronized (Lock1){ printFirst.run(); flag.set(2); Lock1.notifyAll(); } } public void second(Runnable printSecond) throws InterruptedException { synchronized (Lock1){ while (flag.get() != 2){ Lock1.wait(); } printSecond.run(); flag.set(3); Lock1.notifyAll(); } } public void third(Runnable printThird) throws InterruptedException { synchronized (Lock1){ while (flag.get() != 3){ Lock1.wait(); } printThird.run(); flag.set(4); Lock1.notifyAll(); } } } 倒计时门栓解法 应用场景及用法 同时开始。初始化CountDownLatch计数为1，子线程与主线程共享CountDownLatch变量，先启动子线程，然后调用await方法，主线程调用countDown方法，即所有子线程同时开始。 主从协作。主线程依赖子线程运行结果，初始化CountDownLatch计数为开辟的子线程个数，然后调用await方法等待，子线程运行完逻辑之后调用countDown方法，达到主线程等待所有子线程完毕之后再继续运行的目的 class Foo { private CountDownLatch c2; private CountDownLatch c3; public Foo() { c2 = new CountDownLatch(1); c3 = new CountDownLatch(1); } public void first(Runnable printFirst) throws InterruptedException { printFirst.run(); c2.countDown(); } public void second(Runnable printSecond) throws InterruptedException { c2.await(); printSecond.run(); c3.countDown(); } public void third(Runnable printThird) throws InterruptedException { c3.await(); printThird.run(); } } 信号量解法 应用场景及用法 用法: 传入许可数新键Semaphore对象，可设置是否公平\n获取许可方法，有阻塞和非阻塞方式，有响应和不响应中断的方式\n方法执行完释放许可\n应用场景 限制并发访问数量 也可用于线程间构建屏障，因为释放许可并不需要当前线程释放，任何线程都能调用release()方法释放许可。 public class Foo { //声明两个 Semaphore变量 private Semaphore spa,spb; public Foo() { //初始化Semaphore为0的原因：如果这个Semaphore为零，如果另一线程调用(acquire)这个Semaphore就会产生阻塞，便可以控制second和third线程的执行 spa = new Semaphore(0); spb = new Semaphore(0); } public void first(Runnable printFirst) throws InterruptedException { printFirst.run(); //只有等first线程释放Semaphore后使Semaphore值为1,另外一个线程才可以调用（acquire） spa.release(); } public void second(Runnable printSecond) throws InterruptedException { spa.acquire(); printSecond.run(); spb.release(); } public void third(Runnable printThird) throws InterruptedException { spb.acquire(); printThird.run(); } } 第二题： 现在有两种线程，氢 oxygen 和氧 hydrogen，你的目标是组织这两种线程来产生水分子。\n存在一个屏障（barrier）使得每个线程必须等候直到一个完整水分子能够被产生出来。\n氢和氧线程会被分别给予 releaseHydrogen 和 releaseOxygen 方法来允许它们突破屏障。\n这些线程应该三三成组突破屏障并能立即组合产生一个水分子。\n你必须保证产生一个水分子所需线程的结合必须发生在下一个水分子产生之前。\n换句话说:\n如果一个氧线程到达屏障时没有氢线程到达，它必须等候直到两个氢线程到达。 如果一个氢线程到达屏障时没有其它线程到达，它必须等候直到一个氧线程和另一个氢线程到达。 书写满足这些限制条件的氢、氧线程同步代码。\n示例 1:\n输入: \u0026ldquo;HOH\u0026rdquo; 输出: \u0026ldquo;HHO\u0026rdquo; 解释: \u0026ldquo;HOH\u0026rdquo; 和 \u0026ldquo;OHH\u0026rdquo; 依然都是有效解。 示例 2:\n输入: \u0026ldquo;OOHHHH\u0026rdquo; 输出: \u0026ldquo;HHOHHO\u0026rdquo; 解释: \u0026ldquo;HOHHHO\u0026rdquo;, \u0026ldquo;OHHHHO\u0026rdquo;, \u0026ldquo;HHOHOH\u0026rdquo;, \u0026ldquo;HOHHOH\u0026rdquo;, \u0026ldquo;OHHHOH\u0026rdquo;, \u0026ldquo;HHOOHH\u0026rdquo;, \u0026ldquo;HOHOHH\u0026rdquo; 和 \u0026ldquo;OHHOHH\u0026rdquo; 依然都是有效解。\n限制条件:\n输入字符串的总长将会是 3n, 1 ≤ n ≤ 50； 输入字符串中的 “H” 总数将会是 2n； 输入字符串中的 “O” 总数将会是 n。\n循环栅栏的用法及应用场景 用法 初始化CyclicBarrier，传入栅栏需拦住的线程数量（也可以再传入一个Runnable接口实现，由最后一个到达集合点的线程执行）,\n应用场景 多个线程互相等待，到达一个集合点，然后执行后续任务.\n解法： class H2O { Semaphore semaphore4H = new Semaphore(2); Semaphore semaphore4O = new Semaphore(1); CyclicBarrier cyclicBarrier = new CyclicBarrier(3, new Runnable() { @Override public void run() { semaphore4H.release(2); semaphore4O.release(1); } }); public H2O() { } public void hydrogen(Runnable releaseHydrogen) throws InterruptedException { try { semaphore4H.acquire(); releaseHydrogen.run(); cyclicBarrier.await(); } catch (BrokenBarrierException e) { e.printStackTrace(); } } public void oxygen(Runnable releaseOxygen) throws InterruptedException { try { semaphore4O.acquire(); releaseOxygen.run(); cyclicBarrier.await(); } catch (BrokenBarrierException e) { e.printStackTrace(); } } } 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-12-20T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E7%BA%BF%E7%A8%8B%E5%8D%8F%E4%BD%9C%E5%B7%A5%E5%85%B7%E4%B9%8Bsemaphorecountdownlatchcyclicbarrier/","title":"线程协作工具之Semaphore、CountDownLatch、CyclicBarrier"},{"content":"定义 Allow an object to alter its behavior when its internal state changes.The object will appear to change its class.（当一个对象内在状态改变时允许其改变行为，这个对象看起来像改变了其类。）\nUML类图 img.png\r角色解释 Context,环境，封装各种状态的变化，并且向外部提供所有需要的接口，使得外部不用关注状态的变化，只需要关注抽象的行为，以及初始状态。 State，状态，抽象所有状态的行为，并且依赖Context，提供setContext方法。 ConcreateState,状态实现，总结下就是每个状态只关注自己的行为实现，针对不属于自己的行为可以进行抛错。 代码示例 已开门状态和关门状态举例：\nContext类：\npublic class DoorContext { public static final DoorState OPEN_STATE = new OpenState(); public static final DoorState CLOSE_STATE = new CloseState(); /** * 当前状态 */ private DoorState currentState; /** * 获得当前状态 */ public DoorState getCurrentState() { return currentState; } public void setCurrentState(DoorState doorState) { doorState.setDoorContext(this); this.currentState = doorState; } public void close() { this.currentState.close(); } public void open() { this.currentState.open(); } public void enter() { this.currentState.enter(); } public void out() { this.currentState.out(); } public void knock() { this.currentState.knock(); } } State抽象类：\npublic abstract class DoorState { protected DoorContext doorContext; public void setDoorContext(DoorContext _Door_context) { this.doorContext = _Door_context; } public abstract void open(); public abstract void close(); public abstract void enter(); public abstract void out(); public abstract void knock(); } 关门状态实现类：\n@Override public void open() { System.out.println(\u0026quot;开门\u0026quot;); //切换状态 super.doorContext.setCurrentState(DoorContext.OPEN_STATE); } @Override public void close() { System.out.println(\u0026quot;已关门\u0026quot;); } @Override public void knock() { System.out.println(\u0026quot;关着的门用力敲\u0026quot;); } @Override public void enter() { throw new RuntimeException(\u0026quot;门关了，进门失败\u0026quot;); } @Override public void out() { throw new RuntimeException(\u0026quot;门关了，出门失败\u0026quot;); } 开门状态实现类：\npublic class OpenState extends DoorState { @Override public void enter() { System.out.println(\u0026quot;进入\u0026quot;); } @Override public void out() { System.out.println(\u0026quot;出来\u0026quot;); } @Override public void knock() { System.out.println(\u0026quot;开着的门轻轻敲\u0026quot;); } @Override public void open() { System.out.println(\u0026quot;门已经开着了\u0026quot;); } /** * 涉及到切换到其他状态 */ @Override public void close() { System.out.println(\u0026quot;关门\u0026quot;); super.doorContext.setCurrentState(DoorContext.CLOSE_STATE); } } 场景类1：\npublic static void main(String[] args) { DoorContext doorContext = new DoorContext(); //给个初始状态 doorContext.setCurrentState(DoorContext.CLOSE_STATE); //敲门 doorContext.knock(); //开门 doorContext.open(); //进门 doorContext.enter(); //关门 doorContext.close(); //进门 doorContext.open(); //出门 doorContext.out(); //关门 doorContext.close(); } 观察场景类可以知道，当场景类调用的时候，根本不需要在意状态，只需要关注需要实现的行为，而具体能不能实现行为，具体怎么实现这个行为，状态的流转，则被封装在了Context里。\n应用场景 行为与状态有相互依赖的关系。 也就是说，行为受状态影响（状态会改变行为），而状态也受行为影响（行为会改变状态），状态模式就是将每个状态抽出来，然后针对每个行为进行实现，遇到切换状态的行为的时候将当前场景Context里面的状态切换就行了。而对于Context来说，只需要将外部的调用转发个当前场景内部的状态去处理就行了。 优点 屏蔽了状态切换，并且又拥有良好的健壮性，没有忽视行为受状态的影响，只是封装在了状态内部去去处理了。 符合单一职责，如果不应用状态模式，那么状态和行为是耦合在一个类中，不符合单一职责原则。 符合开闭原则，高层模块不关注具体状态实现，如果需要扩展状态，只需要在底层添加一个状态实现类就是了。 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-11-22T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E7%8A%B6%E6%80%81%E6%A8%A1%E5%BC%8F/","title":"设计模式之状态模式"},{"content":"定义 Represent an operation to be performed on the elements of an object structure. Visitor lets you define a new operation without changing the classes of the elements on which it operates. （封装一些作用于某种数据结构中的各元素的操作，它可以在不改变数据结构的前提下定义作用于这些元素的新的操作。）\nUML类图 img.png\r角色：\nObjectStruture 结构对象，大白话就是容器如List,Set，Queue，Map等。 Element 元素抽象，就是容器类的元素 ConcreateElement 元素的具体实现 Vistor 访问者接口 ConcreateVistor 具体访问者 应用场景 个人觉得是常用于迭代当中针对统一抽象不同的具体类，做出不同的操作，比如以下代码：\nfor(Element element: elementList){ element.accept(visitor1); element.accept(visitor2); } 注意了accept的代码基本上都是直接把自己传入visitor，即visitor.visitorElement(this)。\n这里乍一眼看很普通的一个调用，对调用类来说好像每个元素就是添加了一个观察者而已，貌似是走的一套逻辑，这里是如何实现针对不同的Element实现类进行不同的操作的勒？原理是对重载和重写的合理运用。element的accept由不同的子类重写。内部调用了visitor重载的不同方法visitorElement（ConcreateElement element），所以这里实际上根据子类类型做了动态绑定，走的不同的逻辑。也许大家又会说那么为什么要引入一个访问者勒，直接在子类里实现重写方法，不一样实现了动态绑定。确实是可以这么做，但访问者模式的意义在引入了了访问者，封装了这种不同，而不是将这种不同分散在一个个实现类当中。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-11-17T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BF%E9%97%AE%E8%80%85%E6%A8%A1%E5%BC%8F/","title":"设计模式之访问者模式"},{"content":"定义 Define a one-to-many dependency between objects so that when one object changes state,all its dependents are notified and updated automatically.（定义对象间一种一对多的依赖关系，使得每 当一个对象改变状态，则所有依赖于它的对象都会得到通知并被自动更新。）\n类图 img.png\r角色 Subject 被观察者接口，定义添加观察者，减少观察者，通知观察者的方法。 ConcreateSubject 被观察者实现类，实现观察者接口的方法，在适当业务场景代码中调用notify来通知观察者。 其中notify的代码如：\nprivate List\u0026lt;Observer\u0026gt; obsList = new CopyOnWriteArrayList\u0026lt;\u0026gt;(); //通知所有观察者 public void notify(){ for(Observer o:this.obsList){ o.update(); } } Observer 观察者接口，定义收到信息后的处理方法update。 ConcreateObserver 观察者实现类，实现update方法。 高级应用 这种在JDK中已经实现了一套如java.util.Observable实现类和java.util.Observer接口 上述的例子中通知多个观察者是采用的循环中顺序执行的方法，在这种情况下，一个观察者如果效率非常慢，将影响其他观察者的处理进度，所以可以采取队列+多线程的方式进行处理。 消息中的发布订阅就是观察者模式的高级应用 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-11-17T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F/","title":"设计模式之观察者模式"},{"content":"定义 Provide a unified interface to a set of interfaces in a subsystem.Facade defines a higher-level interface that makes the subsystem easier to use.（要求一个子系统的外部与其内部的通信必须通 过一个统一的对象进行。门面模式提供一个高层次的接口，使得子系统更易于使用。）\nUML类图 img.png\r解释 门面模式的类图很简单，Subsystem Classes是子系统所有类的简称，它可能代表一个类，也可能代表几十个对象的集合，门面模式就是提供一个门面Facaed接口，封装所有子系统的类，要求与外部通信必须经过这个类进行。简化耦合关系，避免外部类直接与子系统的各个类进行耦合。这就是门面模式。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-11-17T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E9%97%A8%E9%9D%A2%E6%A8%A1%E5%BC%8F/","title":"设计模式之门面模式"},{"content":"定义 Avoid coupling the sender of a request to its receiver by giving more than one object a chance to handle the request.Chain the receiving objects and pass the request along the chain until an object handles it.（使多个对象都有机会处理请求，从而避免了请求的发送者和接受者之间的耦合关 系。将这些对象连成一条链，并沿着这条链传递该请求，直到有对象处理它为止。）\nUML类图 img.png\r角色：\nHanlder 抽象处理者 ConcreateHander 处理者实现 应用场景 针对一类请求，有多个处理方法，如果不用责任链模式代码会类似为：\nif(type = 1){ //这么处理 }else if(type = 2){ //这么处理 }else if(type =3){ //这么处理 }else{ //这么处理 } 针对这种if-elseif-else结构的分支，可以将处理方式进行抽象封装为Hanlder。用链的形式传递请求到各个实现类。 比如针对一个请求Request，抽象的Hanlder处理逻辑为：\npublic final void hanlderMessage(Request request){ //如果自己能够处理 if(request.getType().equals(this.getType)){ //自己进行处理 this.response() }else{ if(this.getNextHanlder() == null){ //如果没有接替者,按默认情况处理 this.hanlderMessageDefault(request); }else{ //交给自己的接替者进行处理 this.getNextHanlder().hanlderMessage（request）； } } } 大概就是这种结构，其中运用了模板方法模式，hanlderMessage就是每个ConcreateHanlder类继承的方法。继承类只需要实现resonse方法来按照自己的逻辑实现处理，这个抽象Hanlder只需要提供对外的方法setType用来定义自己的处理级别与请求的类型相对应，以及hanlderMessage来处理请求。\n优点 优点是封装了if-elseif-else这种的逻辑分支，将请求与处理分了开来，请求只需要关心抽象的Hanlder。而具体实现ConcreateHanlder也不用关心具体的请求全貌，只需要处理自己的方式。如果需要扩展处理只需要多实现一个Hanlder就可以了。\n缺点 缺点也很明显，这种链表的调用形式，如果调用链过长会浪费栈内存空间。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-11-17T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%B4%A3%E4%BB%BB%E9%93%BE%E6%A8%A1%E5%BC%8F/","title":"设计模式之责任链模式"},{"content":"定义 Attach additional responsibilities to an object dynamically keeping the same interface.Decorators provide a flexible alternative to subclassing for extending functionality.（动态地给一个对象添加一些额外的职责。 就增加功能来说，装饰模式相比生成子类更为灵活\nUML类图 img.png\r角色 Component 抽象构件，就是装饰器与被装饰类共有的抽象。 ConcreteComponent 具体构件，就是被装饰类 Decorator 装饰器抽象，就是装饰器的抽象 ConcreteDecorator 装饰器的具体实现 应用 应用中通常代码如：\n装饰类实现：\n//构造方法，初始化成员变量为装饰类 public ConcreteDecorator1(Component component){ //抽象装饰器中定义的构造方法，初始化被装饰类到成员变量 super(component); } public void operate(){ //这里可以添加一些功能增强 this.component.operate() //这里可以添加一些功能增强 } 场景类应用：\nComponent component = new ConcreteComponent(); //第一次修饰 component = new ConcreteDecorator1(component); //第二次修饰 component = new ConcreteDecorator2(component); //修饰后运行 component.operate(); 思考 装饰器模式最大的好处是动态扩展一类对象的功能，并且可以应用嵌套装饰无限扩展下去。 这是单单用继承来增强父类行为这种方式来扩展远比不了的吗，因为继承是静态的，扩展哪个类都已经在定义类的时候决定了。\n与代理模式的对比 代理模式在前篇梳理过： 设计模式之代理模式。\n总结下两者的区分：\n代理模式对于整个系统来说是控制了真实对象的访问，我们往往只需要使用代理类完成相关业务功能就行了，被代理类相对透明。 如果是装饰器模式，那么我们需要关注的是被装饰的类需要添加什么装饰这个过程，系统中的代码还是得依赖被装饰对象。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-11-17T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/","title":"设计模式之装饰器模式"},{"content":"定义 将对象组合成树形结构以表示“部分-整体”的层次结构，使得用户对单个对象和组合对象的使用具有一致性。\nUML类图 组合模式有两种不同的实现，一种是安全模式的组合模式，叶子节点没有实现树枝节点独有的方法，UML图为：\nimg.png\r另一种是透明模式的组合模式，将树枝节点的方法定义在了节点抽象里面，只是子类的实现是抛出一个异常（所以是不安全的）：\nimg_1.png\r角色 Component,树枝节点与树叶节点统一抽象，主要是被树枝节点类关联。 Composient,树枝节点类 Leaf,树叶节点类 思考 组合模式主要是通过把树枝节点和树叶节点统一抽象为了一个类，也就是树枝节点的getChildren方法并不关心返回的是树枝还是叶子类对象，返回的是Component。该模式主要是一种反映树状模型的设计。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-11-17T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E7%BB%84%E5%90%88%E6%A8%A1%E5%BC%8F/","title":"设计模式之组合模式"},{"content":"定义 Encapsulate a request as an object,thereby letting you parameterize clients with different requests,queue or log requests,and support undoable operations.（将一个请求封装成一个对象，从而让你使用不同的请求把客户端参数化，对请 求排队或者记录请求日志，可以提供命令的撤销和恢复功能。）\n命令模式的类图 img.png\r其中的角色有：\nClient 客户端。只依赖于调用者Invoker、接收者Receiver、以及Command（网上找的图片这里没有画出来）,不用关注接收者如何执行命令，只需要告诉调用者需要执行什么命令，以及会用到什么接收者去执行，但是这里得注意，虽然类图中同时依赖了接收者，但是实践中，通常没有依赖接收者，而是利用Command的命名处理与接收者的耦合关系，如在实例化Command实现类默认构造函数自动为自己设置1个或多个默认的接收者，同时也对外提供自定义接收者的构造方法。这样Client就只需要关心Comand然后交给Invoker去执行，而不需要关新接收者是谁。 Invoker 调用者，调用者主要是对Command的抽象进行执行。抽象一类Command的调用。只关注执行什么命令。 Command 命令，最重要的角色，是命令模式的核心，命令模式正是将客户端的需求抽象为了一个个命令，当需要扩展命令时只需要增加一个Command实现就行了。Command只关注如何组织接收者完成业务。 Receiver 接收者，执行命令的角色。是业务逻辑执行的角色。只关注本角色需要执行什么业务。 个人理解：命令模式最大的优点是类间解耦。Invoker和Receiver之间没有任何依赖关系，Receiver只需要关注怎么实现自己的功能，Invoker只需要关注调用什么Command，然后将Receiver之间的组织封装在Command内部，实现高内聚，职责非常清晰，并且Command可以随时扩展。实在是一个非常优秀的设计。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-11-14T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%91%BD%E4%BB%A4%E6%A8%A1%E5%BC%8F/","title":"设计模式之命令模式"},{"content":"一、前言 前几天阿里云服务器（低配）86块一年，于是赶紧入手了，主要是拿来练练手，今天就是在这个这台服务器上练了下手搭了一个docker和nexus，当自己的私服用，想着平时可以把一些自己写的一些常用的工具、配置类搞上去。现在梳理一下搭建的流程把。\n二、安装docker （一） 安装依赖 yum install -y yum-utils yum install -y device-mapper-persistent-data yum install -y lvm2 (二) 安装 Docker 运行以下命令：\nyum install docker Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，CentOS 系统上可以使用这套脚本安装：\ncurl -fsSL get.docker.com -o get-docker.sh sh get-docker.sh 然后执行docker version查看是否安装完成，如图：\nimg.png\r然后启动 Docker：\nsystemctl enable docker systemctl start docker （三）配置镜像加速 另外，我们知道，类似maven，如果我们在国内每次都从官方仓库里面去下镜像是很慢的，所以一般也需要配置镜像加速。\nvi /etc/docker/daemon.json 打开配置为：\n# 填写自己的加速器地址 { \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;https://xxxxxx.mirror.aliyuncs.com\u0026quot;] } 注意了，我是使用的阿里云镜像加速，实际上还有其他站点，阿里云加速地址需要去阿里云登录后获取，获取地址为阿里云镜像加速地址获取，如图： img_1.png\r把这个地址复制到daemon.json就行了。然后重启daemon和docker服务：\nsystemctl daemon-reload systemctl restart docker （四）docker常用命令总结 概念性的知识我就不介绍了，我也正在学习，这里总结一下常用命令：\ndocker search 关键词:搜索镜像 docker pull xxx:下载镜像 docker images:查看本地有哪些镜像 docker rm container_name/container_id:删除镜像 docker run -t -i container_name/container_id /bin/bash:运行容器中的镜像，并且调用镜像里面的 bash docker ps -a:查看有哪些容器在运行 docker start container_name/container_id:运行容器 docker stop container_name/container_id:停止容器 docker restart container_name/container_id:重启容器 docker attach container_name/container_id:进入容器 exit:退出容器 docker rm container_name/container_id:删除容器 三、docker下安装nexus3 （一）查找nexus3镜像 docker search nexus3 我是下的这个：\nimg_2.png\r（二）拉取镜像 docker pull docker.io/sonatype/nexus3 看拉下来没有可以执行：\ndocker images img_3.png\r（三）执行镜像 docker run -d -p 8081:8081 --name nexus3 --restart=always sonatype/nexus3 这里的参数分别表示：\n-d：表示在docker守护线程运行这个镜像； -p：表示绑定端口，前面的端口表示宿主机端口，后面的表示容器端口，如果宿主机的8081端口已经被占用了，那么可以改为8082：8081，为什么后面的是8081端口？这是Nexus服务自己设定的，可以修改，但是跑docker的话没必要了，下面都是以8081为nexus的端口； \u0026ndash;restart=always：这个指定docker重启启动容器，当服务器或者docker进程重启之后，nexus容器会在docker守护进程启动后由docker守护进程启动容器，容器的重启策略很多，大家可以自己去看看对应的资料； \u0026ndash;name ：这里是指定了容器建立后的名称； 最后面的sonatype/nexus3是镜像名。 如果需要查看日志执行：\ndocker logs nexus3 (四)进入nexus 访问你的服务器地址+端口8081进入nexus管理后台。\n这里可能会遇到两个问题：\n第一个问题是阿里云服务器端口必须要配置一下外网才可以访问。\n第二个问题是密码错误的问题。\n默认nexus的账号和密码是admin/admin123，但是不知道为什么可能是版本原因我这里下的nexus需要默认密码要进入容器内去找，怎么查看勒,执行：\ndocker exec -it 容器id或者容器名 /bin/bash 容器id或容器名通过docker ps -a查看。\n然后cd到nexus-data目录，cat admin.password，查看密码：\nimg_4.png\r将密码复制出来，进行登录就可以了。\n（五）创建仓库 然后登录nexus，根据需要创建自己的仓库或者就用默认的仓库也行，我是创建了一个新的仓库，如果需要创建依次点击如图：\nimg_5.png\r选择maven2(hosted)\nimg_6.png\r不同的类型区别是：\nhosted：本地存储。像官方仓库一样提供本地私库功能 proxy：提供代理其它仓库的类型 group：组类型，能够组合多个仓库为一个地址提供服务 然后创建仓库，用默认设置就行：\nimg_7.png\r（六）将仓库地址copy出来 接下来把刚刚的仓库地址copy出来就行了。\nimg_8.png\r四、设置maven的setting.xml以及pom文件并上传jar包 （一）配置setting.xml img_9.png\r注意了，这里的id必须与设定下载jar主仓库的标签和设定发布仓库的标签内的id保持一致。\n设置下载Jar仓库可以在Pom里面写也可以在setting.xml里面写，在setting.xml写表示全局，我是在setting.xml里面写的：\n标签下添加：\n\u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;my-nexus\u0026lt;/id\u0026gt; \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;my-nexus\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;my-nexus\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;刚刚复制的仓库地址\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; \u0026lt;/profile\u0026gt; 标签下添加：\n\u0026lt;activeProfile\u0026gt;my-nexus\u0026lt;/activeProfile\u0026gt; 然后如果需要上传到私服，配置pom：\n\u0026lt;distributionManagement\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;my-nexus\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;你的仓库地址\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/distributionManagement\u0026gt; 以上保证repository标签下的Id和server下的id一样就行。\n配置好了就可以使用maven打包了，我是用的idea自带的maven插件直接deploey就行了： img_10.png\r本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-11-04T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E5%AE%89%E8%A3%85docker%E5%B9%B6%E5%9C%A8docker%E4%B8%8B%E7%94%A8nexus3%E6%90%AD%E5%BB%BAmaven%E7%A7%81%E6%9C%8D/","title":"安装docker，并在docker下用nexus3搭建maven私服"},{"content":"之所以学习UML类图，是因为想把前面看的《设计模式之禅》再看一遍，但文章都用到了uml类图，前面其实都一知半解，觉得自己挺low的，所以觉得还是得看明白才行。才浏览了网上几篇博客，现将知识点整理如下：\n博客来源：\nJava 大白话讲解设计模式之 \u0026ndash; UML类图\n类之间的关联关系和依赖关系\n依赖、关联、聚合、组合\n看懂UML类图和时序图\n一、UML类图中一个类的基本表示 img.png\r如图，Person类被分为三部分，从上到下依次是：\n类名 成员变量，表示为：权限 属性名：类型 [ = 默认值 ] 方法，表示为：权限 方法名称(参数列表) [ : 返回类型] 其中，权限的表示方法简写为了+、#、~、-,分别表示public、protected、default、privite。\n二、UML图中的关系表示 （一）泛化（继承）关系 img_1.png\rA指向B，表示B是A的泛化，即继承。\n（二）实现关系 img_2.png\rA指向B，表示A实现了B\n（三）依赖关系 img_3.png\rA指向B，表示A依赖于B，即在A中使用到了B对象，与关联关系相区分，依赖比关联关系要弱一些，依赖关系一般是类B以参数的形式传入类A的方法（参数依赖）、类B以局部变量的形式存在于类A的方法中（局部依赖），类A调用类B的静态属性或方法（静态依赖）,而关联关系通常是指一个类对象作为另一个类对象的成员变量。\n（四）关联关系 关联又分为单向关联，双向关联。其中：\n1.单向关联 img_4.png\rA指向B表示，一般用于表示B作为成员变量在A中，是一种较强的关系。\n2.双向关联 img_5.png\r表示A和B都作为成员变量在对方类中，代码中应该尽量避免双向关联。\n（五）特殊的关联关系 1.聚合关系 一种特殊的关联关系，用以表示整体由部分构成的语义，但与组合关系相区分的是，其中整体和部分又相对独立，可以分开，如学生与选修课，选修课由学生一起选修组成，但是选修课没有了，不代表学生这个实体就失去意义了。\nimg_6.png\r菱形那一边表示整体，这里的图表示B由A组成，但就算没了B，A也有自己的意义。\n2.组合关系 也是一种特殊的关联关系，与聚合关系类似，表示的也是整体由部分构成，但是区别的是即使聚合的整体被破坏，部分则失去了意义，换句话说部分的生命周期依赖于整体的生命周期，如公司与部门，公司不存在了，则部门也不存在了。\nimg_7.png\r菱形那一边表示整体，这里的图表示B由A组成，也表示没了A，B也不存在（注，与聚合关系图对比这里是实心的菱形）。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-11-01T00:00:00Z","permalink":"https://runningccode.github.io/2019/uml%E7%B1%BB%E5%9B%BE%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/","title":"UML类图知识整理"},{"content":"一、前言 Sprinboot中配置SpringMVC主要是继承WebMvcConfigurerAdapter（1.x版本）或者WebMvcConfigurationSupport（2.x版本）。这次主要介绍下web应用的一些常用配置。\n二、开始配置 （一）配置参数解析器 参数解析器的作用，通俗来说，参数解析器的作用是将请求中的参数映射到我们Controller方法参数,比如说通过参数解析器，我们可以将前端传过来的token参数做一下处理，从redis中取出用户信息，直接映射为一个userInfo对象，然后Controller方法的参数就直接是UserInfo类型的对象就可以了。如何使用勒？下面是一个简单范例，这里只贴出伪代码：\n首先我们创建一个解析器类，并且实现HandlerMethodArgumentResolver接口。\npublic class TokenHandlerMethodArgumentResolver implements HandlerMethodArgumentResolver { private RedissonClient redissonClient; private UserDao userDao; public TokenHandlerMethodArgumentResolver(RedisClient redisClient, UserDao userDao) { this.redissonClient = redisClient; this.userDao = userDao; } @Override public boolean supportsParameter(MethodParameter methodParameter) { return User.class.isAssignableFrom(methodParameter.getParameterType()); } @Override public Object resolveArgument(MethodParameter methodParameter, ModelAndViewContainer modelAndViewContainer, NativeWebRequest nativeWebRequest, WebDataBinderFactory webDataBinderFactory) throws Exception{ HttpServletRequest nativeRequest = (HttpServletRequest) nativeWebRequest.getNativeRequest(); String token = nativeRequest.getHeader(\u0026quot;token\u0026quot;); RBucket\u0026lt;String\u0026gt; userIdBucket = redissonClient.getBucket(token); if(StringUtils.isNotBlank(userIdBucket.get())){ User user = userDao.getById(userIdBucket.get()); } return user; } } 然后创建类MyWebConfig，继承WebMvcConfigurerAdapter并实现ApplicationContextAware接口，为什么实现ApplicationContextAware，是为了从IOC容器当中取出redissonClient和userDao，用于构造TokenHandlerMethodArgumentResolver。\npublic class MyWebConfig extends WebMvcConfigurerAdapter implements ApplicationContextAware { private UserDao userDao; private RedissonClient redissonClient; @Override public void addArgumentResolvers(List\u0026lt;HandlerMethodArgumentResolver\u0026gt; argumentResolvers) { argumentResolvers.add(new TokenHandlerMethodArgumentResolver(redissonClient,userDao)); super.addArgumentResolvers(argumentResolvers); } @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { userDao = applicationContext.getBean(UserDao.class); redissonClient = applicationContext.getBean(RedissonClient.class); } } 如上，HandlerMethodArgumentResolver最重要的两个方法是boolean supportsParameter(MethodParameter methodParameter) 和 Object resolveArgument(MethodParameter methodParameter, ModelAndViewContainer modelAndViewContainer, NativeWebRequest nativeWebRequest, WebDataBinderFactory webDataBinderFactory),前者是如果返回ture表示使用该解析器进行解析，后者就是返回处理后的方法参数。另外例子中比较关键的方法有通过NativeWebRequest获取HttpServletRequest的方法 nativeWebRequest.getNativeRequest()。\n这里的逻辑是，如果参数中有参数的类型是User类型，那么直接通过token去获取注入在这里。对于我们来说，如果需要用户信息就只需要在Controller中加个User参数，就自动有了，不需要自己查，就可以很方便的引用用户的相关信息。\n（二）配置数据序列化 配置数据序列化有两种方式，一个是通过添加Formatter，一个是添加Converter，两者区别不大，SpingMVC内部处理Formatter时也是包装了一层Converter。同时这里值得注意的是，Formatter和Converter是在SpringMVC使用默认RequestParamMethodArgumentResolver或ServletModelAttributeMethodProcessor参数解析器情况下使用的，如果你自定义了参数解析器，那么其接管的参数，转换规则由自定义参数解析器里面的逻辑来确定。\n另外主要被应用于form表单参数或query参数字段，Json传参不是用这个，Json传参默认参数解析器是：RequestResponseBodyMethodProcessor，针对的主要是@RequestBody修饰的方法参数,调用的消息转换器会用Jackson的ObjectMapper来序列化或反序列化，所以如果是JSON传参，配置这个东西没有用。\n1.配置Formatter 以配置一个LocalDateTime类与字符串之间的转换为例：\n首先新建一个类LocalDateTimeFormatter如下：\npublic class LocalDateTimeFormatter implements Formatter\u0026lt;LocalDateTime\u0026gt; { private static final DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern(\u0026quot;yyyy-MM-dd HH:ss:mm\u0026quot;); @Override public LocalDateTime parse(String s, Locale locale) throws ParseException { return LocalDateTime.parse(s, dateTimeFormatter); } @Override public String print(LocalDateTime localDateTime, Locale locale) { return dateTimeFormatter.format(localDateTime); } } 其中parse方法主要是将字符串转换为对象的逻辑，print方法是将对象转换为字符串的逻辑。\n然后注册该Formatter,在MyWebConfig重写public void addFormatters(FormatterRegistry registry) 方法：\n@Override public void addFormatters(FormatterRegistry registry) { registry.addFormatter(new LocalDateTimeFormatter()); super.addFormatters(registry); } 这样，当不是json传参的时候，默认情况下会使用这个自定义的格式化器进行字符串和对象的转换。\n2.配置Converter 一般情况下我们使用Formatter替代Converter,两者作用差不多。Converter的好处是因为有两个泛型参数，可以限制需要转换的类型和要转换为的类型,但翻看源码发现在引用converter的时候判断选用哪个converter传入的来源类型貌似都是String（就算query参数是一个数字），感觉formatter已经够用了，这里存疑把。\nconverter的用法很简单，只需要往spring容器里面注册一个实现了org.springframework.core.convert.converter的bean，除了这种形式，也可以直接注册ConverterFactory实现获取converter的工厂方法\u0026lt;T extends R\u0026gt; Converter\u0026lt;S, T\u0026gt; getConverter(Class\u0026lt;T\u0026gt; targetType)，用在一个来源类型，多个目标类型的场景。另外还有GenericConverter，需实现Object convert(@Nullable Object source, TypeDescriptor sourceType, TypeDescriptor targetType)方法，用于多个来源类型，多个目标类型的场景，返回值直接为转换后的值。\n（三）配置静态资源映射 配置静态资源重写的方法为：void addResourceHandlers(ResourceHandlerRegistry registry),如重写方法为：\n@Override public void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\u0026quot;/upload/**\u0026quot;).addResourceLocations(\u0026quot;classpath:/upload/\u0026quot;); super.addResourceHandlers(registry); } 其中addResourceHandler(\u0026quot;/upload/**\u0026quot;).addResourceLocations(\u0026quot;classpath:/upload/\u0026quot;)的意思表示将URL:项目访问url+upload/xxx映射到classpath下的upload目录里面名为XXX的静态资源，其中addResourceLocations参数为变长参数，可以映射多个路径，也可以前面加\u0026rsquo;file:\u0026rsquo;，映射磁盘上任意目录，如：file:/D://upload/，表示映射到d盘的upload目录。\n（四）配置过滤器 添加过滤器只需要注册一个FilterRegistrationBean类对象到spring容器即可，如在测试环境注册一个允许跨域的过滤器：\n@Conditional(value = {TestCondition.class}) @Bean public FilterRegistrationBean corsFilter() { UrlBasedCorsConfigurationSource urlBasedCorsConfigurationSource = new UrlBasedCorsConfigurationSource(); CorsConfiguration corsConfiguration = new CorsConfiguration(); corsConfiguration.addAllowedOrigin(\u0026quot;*\u0026quot;); corsConfiguration.addAllowedHeader(\u0026quot;*\u0026quot;); corsConfiguration.addAllowedMethod(\u0026quot;*\u0026quot;); urlBasedCorsConfigurationSource.registerCorsConfiguration(\u0026quot;/**\u0026quot;, corsConfiguration); FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(); filterRegistrationBean.setOrder(10); filterRegistrationBean.setFilter(new CorsFilter(urlBasedCorsConfigurationSource)); filterRegistrationBean.setName(\u0026quot;corsFilter\u0026quot;); filterRegistrationBean.addUrlPatterns(\u0026quot;/*\u0026quot;); return filterRegistrationBean; } @Conditional注解标识在上面表示在测试环境即引用的aplication-test.yml|properties，这里的value应该传入一个org.springframework.context.annotation.Condition接口实现类的Class对象。这里传入的是TestCondition。代码如下：\n@Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) { Environment environment = conditionContext.getEnvironment(); String[] activeProfiles = environment.getActiveProfiles(); if (null != activeProfiles) { for (String x : activeProfiles) { if (\u0026quot;test\u0026quot;.equals(x)) { return true; } } } return false; } 另外对于自定义的过滤器，常规操作如下：\n继承OncePerRequestFilter抽象类，实现doFilterInternal方法。 将这个Filter对象注入到filterRegistrationBean，并配置其他信息，如order，已经过滤的Url。 如：\n@Bean public FilterRegistrationBean myFilter() { FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(); filterRegistrationBean.setFilter(new MyFilter()); //order越小，优先级越高 filterRegistrationBean.setOrder(1); filterRegistrationBean.addUrlPatterns(\u0026quot;/*\u0026quot;); filterRegistrationBean.setName(\u0026quot;myFilter\u0026quot;); return filterRegistrationBean; } (四）配置拦截器 拦截器与过滤器的区别在于过滤器的优先级比拦截器高，Filter是作用于Servlet前，而Interceptor则相对于Filter更靠后一点。另外Filter不可以使用IOC容器资源，Interceptor则可以。过滤器能完成的功能，通过Interceptor都可以完成，通常情况下，推荐使用Interceptor。\n配置拦截器的步骤是：\n1.创建类继承HandlerInterceptorAdapter。 HandlerInterceptorAdapter有三个方法可以重写，未重写前不做任何处理。三个方法是：\n//在业务处理器处理请求之前被执行 public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler)throws Exception { return true; } //在业务处理器处理请求返回响应之前执行 public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView)throws Exception { } //返回响应之后执行 public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex)throws Exception { } 2.注册拦截器 在MyWebConfig类重写方法void addInterceptors(InterceptorRegistry registry)，如：\n@Override public void addInterceptors(InterceptorRegistry registry) { // 可以多个拦截器组成一个拦截器链 // addPathPatterns 用于添加拦截规则 // excludePathPatterns 用于排除拦截 registry.addInterceptor(new MyInterceptor()) .addPathPatterns(\u0026quot;/**\u0026quot;) .excludePathPatterns(\u0026quot;/swagger*/**\u0026quot;); super.addInterceptors(registry); } 三、小结 以上就是基于Springboot下的SpringMVC常用配置方法，基本上能满足常用项目配置需求，其他就暂时不作了解了。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-10-12T00:00:00Z","permalink":"https://runningccode.github.io/2019/springboot%E4%B8%8B%E7%9A%84springmvc%E9%85%8D%E7%BD%AE%E8%A7%A3%E6%9E%90-webmvcconfigureradapter%E5%92%8Cwebmvcconfigurationsupport/","title":"Springboot下的SpringMVC配置解析 ——WebMvcConfigurerAdapter和WebMvcConfigurationSupport"},{"content":"一、前言 字符集表示的是存储的二进制与字符如何映射的关系，比较规则指的是字符如何排序的规则，比如字符如果使用order by到底按什么规则进行排序。\n二、查看命令 查看支持的字符集命令是：SHOW (CHARACTER SET|CHARSET) [LIKE 匹配的模式],CHARACTER SET|CHARSET同意，两者都可以用。\n查看支持的比较规则命令是：SHOW COLLATION [LIKE 匹配的模式]。\n注意，比较规则的命名方式有一定规律，一般来说，满足：\n比较规则名称以与其关联的字符集的名称开头 后边紧跟着该比较规则主要作用于哪种语言，比如utf8_polish_ci表示以波兰语的规则比较，utf8_spanish_ci是以西班牙语的规则比较，utf8_general_ci是一种通用的比较规则。 名称后缀意味着该比较规则是否区分语言中的重音、大小写啥的，具体可以用的值如下: 后缀 英文释义 描述 _ai accent insensitive 不区分重音 _as accent sensitive 区分重音 _ci case insensitive 不区分大小写 _cs case sensitive 区分大小写 _bin binary 以二进制方式比较 比如我们常用的utf8_general_ci这个比较规则是以ci结尾的，说明不区分大小写进行比较。\n三、关于utf8与utfmb4 utf8和utfmb4是我们常用的字符集，这两者有什么区别勒？实际上真正的UTF-8 是1-4个字节，但是mysql里面的utf8不是指的这个，而是指的utf8mb3，其中mb表示的是最多占用多少个字节，mysql最开始为了节省空间资源偷偷把utf-8给阉割了，用1-3个字节表示，实际上1-3个字节也足够表示我们平常使用的字符了。而实际上utfmb4才是真正的utf8，能映射所有的unicode码。\n四、字符集和比较规则的级别 MySQL有4个级别的字符集和比较规则，包括服务器级别、数据库级别、表级别、列级别，对于一个表的列，这几个级别粒度越具体的越优先使用，在创建数据库、表、列的时候，如果没有具体指定用什么字符集和比较规则，自动引用上一级别的配置。我们接下来看看各个级别的字符集和比较规则具体怎么设置。\n服务器级别 系统变量 描述 character_set_server 服务器级别的字符集 collation_server 服务器级别的比较规则 如图，服务器级别的字符集和比较规则由系统变量character_set_server和collation_server控制，查看和修改命令上一篇文章介绍过。我们可以通过启动选项、配置文件、运行时更改来设置它。\n数据库级别 数据库级别的字符集和比较规则的系统变量为：\n系统变量 描述 character_set_database 当前数据库的字符集 collation_database 当前数据库的比较规则 如果想查看当前数据库使用的字符集和比较规则，可以使用以上的变量值进行查看（前提是使用USE语句选择当前默认数据库，如果没有默认数据库，则变量与相应的服务器级系统变量具有相同的值）。\n另外，这两个变量是只读的，我们不能直接修改这两个值就把数据库的字符集和比较规则改变了。这两个值只能通过DDL语句去改变。语法格式为：\nCREATE DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; ALTER DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; 表级别 编辑和修改：\n`` CREATE TABLE 表名 (列的信息) [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称]]\nALTER TABLE 表名 [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称] ``\n列级别 编辑和修改：\nCREATE TABLE 表名( 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称], 其他列... ); ALTER TABLE 表名 MODIFY 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称]; 另外 由于字符集和比较规则是互相有联系的，如果我们只修改字符集，则比较规则将变为修改后的字符集默认的比较规则。，只修改比较规则，则字符集将变为修改后的比较规则对应的字符集。\n五、Mysql中字符集的转换 我们知道从客户端发往服务器的请求本质上就是一个字符串，服务器向客户端返回的结果本质上也是一个字符串，而字符串其实是使用某种字符集编码的二进制数据。这个字符串可不是使用一种字符集的编码方式一条道走到黑的，从发送请求到返回结果这个过程中伴随着多次字符集的转换，在这个过程中会用到3个系统变量:\n系统变量 描述 character_set_client 服务器解码请求时使用的字符集 character_set_connection 服务器处理请求时会把请求字符串从character_set_client转为character_set_connection character_set_results 服务器向客户端返回数据时使用的字符集 img.png\r多次转码流程如上图，注意，如果某个列使用的字符集和character_set_connection代表的字符集不一致的话，还需要进行一次字符集转换。一般情况下要使用保持这三个变量的值和客户端使用的字符集相同，免得不必要的编解码开销。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-10-07T00:00:00Z","permalink":"https://runningccode.github.io/2019/mysql%E7%B3%BB%E5%88%97%E4%B8%89-mysql%E5%AD%97%E7%AC%A6%E9%9B%86%E5%92%8C%E6%AF%94%E8%BE%83%E8%A7%84%E5%88%99/","title":"Mysql系列（三）—— Mysql字符集和比较规则"},{"content":"系统变量 什么是系统变量 系统变量，就是Mysql针对自己程序运行的一些参数配置。例如通过系统变量我们可以指定诸如允许同时连入的客户端数量、客户端和服务器通信方式、表的默认存储引擎、查询缓存的大小等设置项。\n系统变量的分类 GLOBAL：全局变量，影响服务器的整体操作。 SESSION：会话变量，影响某个客户端连接的操作。（注：SESSION有个别名叫LOCAL） 注：\n在服务器启动时，会将每个全局变量初始化为其默认值（可以通过命令行或选项文件中指定的选项更改这些默认值）。然后服务器还为每个连接的客户端维护一组会话变量，客户端的会话变量在连接时使用相应全局变量的当前值初始化。 并不是所有系统变量都具有GLOBAL和SESSION的作用范围。有一些系统变量只具有GLOBAL作用范围，比方说max_connections，表示服务器程序支持同时最多有多少个客户端程序进行连接。有一些系统变量只具有SESSION作用范围，比如insert_id，表示在对某个包含AUTO_INCREMENT列的表进行插入时，该列初始的值。有一些系统变量的值既具有GLOBAL作用范围，也具有SESSION作用范围，比如我们default_storage_engine（存储引擎），而且其实大部分的系统变量都是这样的。 如何查看系统变量 命令：SHOW [GLOBAL|SESSION] VARIABLES [LIKE 匹配的模式]（不写GLOBAL或SESSION等同于SESSION）;\n如何设置系统变量 通过启动选项设置，如命令：mysqld --default-storage-engine=MyISAM --max-connections=10，就是配置默认存储引擎为MyISAM，最大连接数为10。 注：在类Unix系统中，启动脚本有mysqld、mysqld_safe、mysql.server，其中mysqld代表直接启动mysql服务器程序，mysqld_safe会在此基础上启动一个监控进程，它会将服务器程序的出错信息和其他诊断信息重定向到某个文件中，产生出错日志，mysql.server也可以启动Mysql,使用命令mysql.server start,效果跟mysqld_safe一样，mysqld_multi是用于单机多个mysql服务端进程的启动，停止脚本。\n每个MySQL程序都有许多不同的选项。例如，使用mysql \u0026ndash;help可以看到mysql程序支持的启动选项，mysqld_safe \u0026ndash;help可以看到mysqld_safe程序支持的启动选项。查看mysqld支持的启动选项有些特别，需要使用mysqld \u0026ndash;verbose \u0026ndash;help。\n在my.cnf配置文件中添加启动选项 配置文件形如：\n[server] (具体的启动选项...) [mysqld] (具体的启动选项...) [mysqld_safe] (具体的启动选项...) [client] (具体的启动选项...) [mysql] (具体的启动选项...) [mysqladmin] (具体的启动选项...) 像这个配置文件里就定义了许多个组，组名分别是server、mysqld、mysqld_safe、client、mysql、mysqladmin。每个组下边可以定义若干个启动选项。\n如在server组下面配置：\n[server] default-storage-engine=MyISAM max-connections=10 表示默认存储引擎为MyISAM，最大连接数为10。\n服务器程序运行过程中设置 命令： SET [GLOBAL|SESSION] 系统变量名 = 值 或 SET [@@(GLOBAL|SESSION).]系统变量名 = 值（不写GLOBAL或SESSION等同于SESSION）;\n例如：\n语句一：SET GLOBAL default_storage_engine = MyISAM;\n语句二：SET @@GLOBAL.default_storage_engine = MyISAM;\n注：如果某个客户端改变了某个系统变量在GLOBAL作用范围的值，并不会影响该系统变量在当前已经连接的客户端作用范围为SESSION的值，只会影响后续连入的客户端在作用范围为SESSION的值。\n二、状态变量 什么是状态变量 MySQL服务器程序中维护了好多关于程序运行状态的变量，它们被称为状态变量，由于状态变量是用来显示服务器程序运行状况的，所以它们的值只能由服务器程序自己来设置，我们程序员是不能设置的。与系统变量类似，状态变量也有GLOBAL和SESSION两个作用范围的。比方说Threads_connected表示当前有多少客户端与服务器建立了连接，Handler_update表示已经更新了多少行记录。\n查看状态变量命令 SHOW [GLOBAL|SESSION] STATUS [LIKE 匹配的模式]（不写GLOBAL或SESSION等同于SESSION;\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-10-06T00:00:00Z","permalink":"https://runningccode.github.io/2019/mysql%E7%B3%BB%E5%88%97%E4%BA%8C-%E7%B3%BB%E7%BB%9F%E5%8F%98%E9%87%8F%E4%B8%8E%E7%8A%B6%E6%80%81%E5%8F%98%E9%87%8F/","title":"Mysql系列（二）—— 系统变量与状态变量"},{"content":"写在前面 最近在学习作者小孩子的掘金专栏《MySQL是怎样运行的：从根儿上理解MySQL》,现将学习心得总结梳理为一个系列。\nimg.png\rMysql查询流程图 img_1.png\rMysql查询流程 处理连接 Mysql有三种连接方式，包括：\nTCP/IP协议，远程连接使用，常规可采用ip端口建立连接。 命名管道或共享内存，windows下可以采用命名管道或共享内存进行进程间通信方式，但要求服务器端与连接客服端在同一台机器。 Unix域套接字，类Unix操作系统可以使用Unix域套接字文件来进行进程间通信。 Mysql会限制同时连接服务器的数量，系统变量（后面我们在解释什么是系统变量、还有状态变量）：max_connections表示的是最大连接数（默认151）。\n查询缓存 Mysql8.0之前会对查询结果建立缓存，第二次使用同样的语句查询时会先看缓存里面有没有，如果有直接返回查询结果。\n注：从MySQL 5.7.20开始，不推荐使用查询缓存，并在MySQL 8.0中删除（个人觉得，项目小并发量不是很高的可以用，但是并发量不高，相对来说缓不缓存其实影响也不大。但是项目大，数据变更频繁，缓存的开销就会很大，所以无论项目大小，保持关闭状态都是可以的）\n缓存的命中条件 前后查询语句必须一样，两个查询请求在任何字符上的不同（例如：空格、注释、大小写），都会导致缓存不命中。 如果查询请求中包含某些系统函数、用户自定义变量和函数、一些系统表，如 mysql 、information_schema、 performance_schema 数据库中的表，那这个请求就不会被缓存。 缓存的删除条件 MySQL的缓存系统会监测涉及到的每张表，只要该表的结构或者数据被修改，如对该表使用了INSERT、 UPDATE、DELETE、TRUNCATE TABLE、ALTER TABLE、DROP TABLE或 DROP DATABASE语句，那使用该表的所有高速缓存查询都将变为无效并从高速缓存中删除。\n语法解析 这一阶段主要是针对未命中缓存的查询语句进行语法检查，并且将查询语句中使用的表、各种查询条件都提取出来放到MySQL服务器内部使用的一些数据结构上来。\n查询优化 mysql会针对我们书写的查询语句进行优化，生成一个执行计划（可以在查询语句前面加explain查看执行计划，具体执行计划怎么看，后面有空再总结吧），这个执行计划可以看到将使用哪些索引进行查询，表之间优化后的连接顺序等东西。\n存储引擎执行 MySQL从2007年开始提供了插件式的存储引擎API，一般来说，连接管理、查询缓存、语法解析、查询优化这些并不涉及真实数据存储的功能我们称为MySQL server的功能，但真正执行查询的是存储引擎的功能，存储引擎真正执行数据存取操作，存储引擎有很多，都是基于mysql的API规范进行开发的。包括下列：\nimg_2.png\r其中，InnoDB和MyISAM是我们最常用的，Mysql的默认存储引擎是InnoDB。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-10-06T00:00:00Z","permalink":"https://runningccode.github.io/2019/mysql%E7%B3%BB%E5%88%97%E4%B8%80-%E7%AE%80%E5%8D%95%E6%A2%B3%E7%90%86%E4%B8%80%E4%B8%8Bmysql%E7%9A%84%E6%9F%A5%E8%AF%A2%E6%B5%81%E7%A8%8B/","title":"Mysql系列（一）—— 简单梳理一下Mysql的查询流程"},{"content":"什么是CAS CAS-CompareAndSet，是JDK原子变量类AtomicInteger、AtomicLong、AtomicInteger、AtomicBoolean、AtomicReference等实现的基础，例如对于一个共享变量int，就算是简单的自增操作也不是原子性的，多线程同时自增，可能会导致变量的值比预期结果小。但是可以使用AtomicInteger的incrementAndGet() 方法操作变量，这样结果和预期值一样。跟传统的加锁不同，getAndDecrement()方法并没有给代码加锁。代码类似于：\npublic final int incrementAndGet() { for (;;) { int current = get(); int next = current + 1; if (compareAndSet(current, next)) return next; } } 底层通过sun.misc.Unsafe的本地方法compareAndSwapInt实现，这个方法是原子的。\n与synchronized的对比 乐观锁与悲观锁的区别 性能对比 synchronized是阻塞的，CAS更新是非阻塞的，只是会重试，不会有线程上下文切换开销，对于大部分比较简单的操作，无论是在低并发还是高并发情况下，这种乐观非阻塞方式的性能都要远高于悲观阻塞式方式。\n应用场景 用来实现乐观非阻塞算法，确保当前线程方法体内使用的共享变量不被其他线程改变，CAS广泛运用在非阻塞容器中。 用来实现悲观阻塞式算法，其用在了显式锁的原理实现，如可重入计数中，调用lock()方法时将通过CAS方法将其设为1，调用unlock则设为递减1。如果同时多个线程调用Lock方法那么必然会导致原子修改不成功，保证了锁的机制，排他性。 可能存在的问题 ABA问题，普通的CAS操作并不是原子的，因为有可能另一个线程改了值但是又改回了值，那么乐观锁的方式是不能保证原子性的，若业务需要规避这种情况那么可以使用AtomicStampedReference的compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp)方法，只有值和时间戳都相等的时候才进行原子更新，每次更新都把当前时间修改进原子变量。 JDK8的优化 JAVA8新增了LongAdder、DoubleAdder对原子变量进行进一步优化，主要是利用了分段CAS的机制，如果不用LongAdder，用AtomicLong的话，在高并发情况下，会产生一直自旋，导致效率不高。他将一个数分成若干个数，CompareAndSet方法的参数只是比较的这若干个数中的一个数，从而降低了自旋的概率，提高了效率。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-10-04T00:00:00Z","permalink":"https://runningccode.github.io/2019/jdk%E5%B9%B6%E5%8F%91%E5%8C%85%E6%B8%A9%E6%95%85%E7%9F%A5%E6%96%B0%E7%B3%BB%E5%88%97%E5%9B%9B-cas%E5%8E%9F%E7%90%86%E4%B8%8Ejdk8%E7%9A%84%E4%BC%98%E5%8C%96/","title":"JDK并发包温故知新系列（四）—— CAS原理与JDK8的优化"},{"content":"显式锁-Lock与ReadWriteLock JDK针对Lock的主要实现是ReentrantLock，ReadWriteLock实现是ReentrantReadWriteLock。本文主要介绍ReentrantLock。\nReentrantReadWriteLock 两把锁共享一个等待队列，两把锁的状态都由一个原子变量表示，特有的获取锁和释放锁逻辑。\nReentrantReadWriteLock的基本原理： 读锁的获取,只要求写锁没有被线程持有就可以获取，检查等待队列，逐个唤醒等待读锁线程，遇到等待写锁线程则停止. 读锁的释放,释放后，检查写锁和读锁是否被持有，若都没有被持有则唤醒下一个等待线程. 写锁的获取,只有读写锁都未被持有才会获取写锁。 写锁的释放，唤醒等待队列的下一个线程。 ReentrantLock 主要方法 void lock();获取锁，阻塞，不响应中断，但会记录中断标志位。 void lockInterruptibly() throws InterruptedException;获取锁，响应中断 boolean tryLock();获取锁，不阻塞，实时返回，一般需循环调用 boolean tryLock(long time, TimeUnit unit) throws InterruptedException;在time的时间内阻塞获取锁，响应中断 void unlock();释放锁 Condition newCondition();新建显式条件 注： 这里的响应中断意思是若被其他线程中断（调用interrupt方法）会抛出InterruptedException异常。\n原理支持 依赖CAS方法,可重入实现用的计数就是用的原子变量。 依赖LockSupport中的方法: public static void park()：放弃CPU执行权，CPU不在进行调度，响应中断，当有中断发生时，park会返回，线程中断状态会被设置，另外park也有可能无缘无故的返回，所以一般需要循环检查park的等待条件是否满足。。 public static void parkNanos(long nanos)：在nanos纳秒内放弃CPU执行权 public static void parkUntil(long deadline)：放弃执行权直到deadline时间（距离1970年毫秒数）。 public static void unpark(Thread thread)：重新恢复线程，让其争夺CPU执行权。 实现基础AQS AQS-AbstractQueuedSynchronizer（抽象队列同步器）。\nReadWriteLock在内部注入了AbstractQueuedSynchronizer，上锁和释放锁核心方法都在AQS类当中，AQS维护了两个核心变量，一个是state（当前可重入计数，初始值为0），一个是exclusiveOwnerThread（当前持有锁的线程Thread对象）。另外还维护了一个锁等待队列。\nReentrantLock构造方法传入的boolean值ture为公平锁，false为不公平锁。以不公平锁为例先讲一下上锁和释放锁的原理：\n上锁 如果当前锁状态为0（未被锁），则使用CAS获得锁，并设置当前锁内的线程为自己。 如果不为0，且持有锁的线程不是自己，则添加到队列尾部，并调用LockSupport中的park()方法放弃CPU执行权。直到当锁被释放的时候被唤醒，被唤醒后检查自己是否是第一个等待的线程，如果是且能获得锁，则返回，否则继续等待，这个过程中如果发生了中断，lock会记录中断标志位，但不会提前返回或抛出异常。 如果不为0，但持有锁线程是自己，则直接将state加1。 释放锁 就是将AQS内的state变量的值递减1，如果state值为0，则彻底释放锁，会将“加锁线程”变量也设置为null，同时唤醒等待队列中的第一个线程。\n公平锁 为什么说上面的是不公平锁，释放锁时不是唤醒队列中第一个线程吗？为什么还会出现不公平的情况了，原因在于如果刚好释放锁，此时有一个线程进来尝试获取锁，可能会存在插队的情况。\n公平锁原理 构造方法bollean传入true则代表的是公平锁，在获取锁方法中多了一个检查，意义是只有不存在其他等待时间更长的线程，它才会尝试获取锁。对比不公平锁，其整体性能比较低，低的原因不是这个检查慢，而是会让活跃线程得不到锁，进入等待状态，引起上下文切换，降低了整体的效率，\n与synchrnized的区别 tryLock可避免死锁造成的无限等待 拥有获取锁信息方法的各种API 可以响应中断 可以限时 建议： synchronized以前的效率不如显式锁，但现在的版本两者效率上几乎没有区别，所以建议能用synchronized就用synchronized，需要实现synchronized办不到的需求如以上区别时，再考虑ReentrantLock。\n显示条件 什么是显示条件 与wait和notify对应，用于线程协作，通过Lock的Condition newCondition()方法创建对应显示锁的显示条件;\n方法 主要方法是await()和signal()，await()对应于Object的wait()，signal()对应于notify，signalAll()对应于notifyAll()\n用法示例 public class WaitThread extends Thread { private volatile boolean fire = false; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); @Override public void run() { try { lock.lock(); try { while (!fire) { condition.await(); } } finally { lock.unlock(); } System.out.println(\u0026quot;fired\u0026quot;); } catch (InterruptedException e) { Thread.interrupted(); } } public void fire() { lock.lock(); try { this.fire = true; condition.signal(); } finally { lock.unlock(); } } public static void main(String[] args) throws InterruptedException { WaitThread waitThread = new WaitThread(); waitThread.start(); Thread.sleep(1000); System.out.println(\u0026quot;fire\u0026quot;); waitThread.fire(); } } 当主线程调用fire方法时，子线程才被唤醒继续执行。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-10-04T00:00:00Z","permalink":"https://runningccode.github.io/2019/jdk%E5%B9%B6%E5%8F%91%E5%8C%85%E6%B8%A9%E6%95%85%E7%9F%A5%E6%96%B0%E7%B3%BB%E5%88%97%E4%BA%94-%E6%98%BE%E5%BC%8F%E9%94%81%E4%B8%8E%E6%98%BE%E5%BC%8F%E6%9D%A1%E4%BB%B6/","title":"JDK并发包温故知新系列（五）—— 显式锁与显式条件"},{"content":"前言 线程间通信主要是通过wait、notify来实现的，使用这种机制实现线程通信是非常效率的，相比而言，不知道的同学针对线程通信可能只会想到轮询的方式，下次可别再去轮询共享变量了，把线程协作机制用起来。\n一、线程协作的要素 首先，如果了解显式锁ReentrantLock或显式条件Condition,我们就会知道锁的队列不止有一个等待队列，还有一个等待条件队列，存放等待被唤醒的线程。 对于用声明式编程synchronized关键字来说，底层也是这种原理，对应的针对等待条件队列入队出队方法就是锁资源的wait/notify/notifyAll方法。但光有这种机制只能表示我们线程可以触发其他线程继续执行，前面说了叫等待条件队列，那条件到底是什么勒？一般来说条件就是线程间共享的一个变量，这个变量用于控制线程等待或继续执行。总结来说，notify一般伴随着一个条件共享变量的改变，wait一般伴随着一个条件共享变量的不满足。比如如下代码：\nsynchronized (this) { while (!condition) { wait(); } } 最开始condition不满足，该线程放弃CPU执行权，进入等待条件队列，然后等到其他线程做了其他事后，条件共享变量被改变，然后该线程被唤醒，然后继续执行。\n二、线程协作的场景 线程间的基本协作机制大致分为以下几种：\n生产者/消费者协作模式，生产者线程和消费者线程共享一个队列变量，为了控制队列的长度上限，当队列为满时，限制生产者线程等待，当添加信息到队列时，队列不为空，唤醒消费者队列，当队列为空时限制消费者线程等待，取到信息时，队列不为满，唤醒生产者队列。 同时开始，所有子线程根据一个共享变量等待一个条件，同时开始的意思是主线程改变这个共享变量同时移除所有在等待条件队列中的线程，如模拟仿真程序中，要求多个线程能同时开始。 等待结束，thread.join()方法的底层原理是while（子线程未结束）{wait(0)}，等待子线程结束。更好的方式实现是通过Java的CountDownLatch类来控制主线程的等待，子线程每结束一个线程计数器减1，主线程的等待条件是计数器为0，使用线程计数器也可以实现同时开始，只需要让子线程共享线程计数器变量，等待条件是线程计数器为0，主线程将一个线程计数器传入多个子线程并运行子线程，在一个时刻将线程计数器countDown为0就行了。这个主要是应用在主从协作模式中，主线程将任务分解为若干个子任务，为每个子任务创建一个线程，主线程在继续执行其他任务之前需要等待每个子任务执行完毕。 异步结果，jdk并发包对异步任务进行了封装，主要用于主从协作场景，如果不想手工创建线程，管理线程可以使用Excutors异步任务执行服务，核心是Future接口的get方法，该方法将等待异步任务返回执行结果，然后才会被唤醒并返回结果。 集合点，并发包中的CyclicBarrier栅栏就是这种场景的应用，一组子线程同时等待所有其他线程到一个地方之后再开始执行下面的逻辑，如一些并行计算场景，每个线程负责一部分计算，然后在集合点等待其他线程完成，所有线程到齐后，交换数据和计算结果，再进行下一次计算。 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-10-03T00:00:00Z","permalink":"https://runningccode.github.io/2019/jdk%E5%B9%B6%E5%8F%91%E5%8C%85%E6%B8%A9%E6%95%85%E7%9F%A5%E6%96%B0%E7%B3%BB%E5%88%97%E4%BA%8C-%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8D%8F%E4%BD%9C%E6%9C%BA%E5%88%B6-wait%E5%92%8Cnotify/","title":"JDK并发包温故知新系列（二）—— 线程的基本协作机制: wait和notify"},{"content":"需要进行线程中断的场景 很多线程的运行模式是死循环，比如在生产者/消费者模式中，消费者主体就是一个死循环，它不停的从队列中接受任务，执行任务，在停止程序时，我们需要一种\u0026quot;优雅\u0026quot;的方法以关闭该线程。 在一些用户启动的任务中，线程是用户启动的，比如手动启动批次任务，在任务执行过程中，用户可能会希望取消该任务。 在一些场景中，比如从第三方服务器查询一个结果，我们希望在限定的时间内得到结果，如果得不到，我们会希望取消该任务。 有时，我们会启动多个线程做同一件事，比如类似抢火车票，我们可能会让多个好友帮忙从多个渠道买火车票，只要有一个渠道买到了，我们会通知取消其他渠道。 涉及的线程方法(Thread对象方法) public boolean isInterrupted() 判断线程中断标志位是否为true public void interrupt() 设置线程中断标志位为true，但对于线程不同的状态，不一定能设置成功。 public static boolean interrupted() 返回线程中断标志位，并清空。 线程对中断的反应 分几种情况：\nRUNNABLE状态-线程调用了start()方法，处于等待系统调度或在运行中 这种情况下只设置中断标志位。\nWAITING/TIMED_WAITING-等待状态 当调用如下方法时进入等待状态，包括的方法有：\nWATING：调用了锁资源的wait方法，或调用了join方法。\nTIMED_WAITING：wait(long timeout)，sleep(long millis)，join(long millis)。(wait与sleep的区别:是否释放锁)。\n抛出InterruptedException异常并且线程中断标志位被清空，针对此一般一般是交由上级处理，若希望线程中断，在catch里面执行清理工作或重设线程标志位。\nBLOCKED-线程处于锁等待队列，试图进入同步块 只设置标志位。\nNEW/TERMINATED-线程结束了或还未调用start()方法 不会有任何效果。\n注意 不是说调用了interrupt()方法，线程就终止了，需要线程实现者通过代码实现，如下：\nwhile (!Thread.currentThread().isInterrupted()) { //清理逻辑 } 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-10-03T00:00:00Z","permalink":"https://runningccode.github.io/2019/jdk%E5%B9%B6%E5%8F%91%E5%8C%85%E6%B8%A9%E6%95%85%E7%9F%A5%E6%96%B0%E7%B3%BB%E5%88%97%E4%B8%89-%E7%BA%BF%E7%A8%8B%E7%9A%84%E4%B8%AD%E6%96%AD/","title":"JDK并发包温故知新系列（三）—— 线程的中断"},{"content":"前言 今天回家火车上没事情干，一直在看石杉码农老大的技术博客。其中看了些并发系列的文章，虽然都是以前学习JDK源码的时候都了解到的东西，但是隔久了发现自己不是很清晰了，所以到家后把自己笔记捞出来复习一遍，把知识点再串联一次，让自己理解更深刻一点。\n并发问题 并发问题是指多线程读写共享内存的时候，产生结果不可预期的情况，并发问题的产生的原因可以归结为两种，一是因为竞态条件，二是因为内存可见性、\n竞态条件 什么是竞态条件 竞态条件，官方解释是如果程序运行顺序的改变会影响最终结果,这就是一个竞态条件。\n这句话有点抽象，描述的有点抽象，我个人对竞态条件的理解是多个线程在竞争同一系列资源的使用权，因为使用都是有时间的，不是咔嚓一下完成的，对于每个线程来说它能够正确执行的条件就是要求从获取资源到使用资源完成都是原子性的，不允许其他线程来中途改变资源从而影响了原子性，每个线程都需要这种条件，我把这种多个线程对于共享资源获取到操作完成的原子性的需求叫做竞态条件。\n针对竞态条件问题的应对思想 结合自己所学，总结一下应对竞态条件的思想：\n通过路由避免竞争，这反映在并发包中有ConcurrentHashMap的分段锁机制、Java8对Cas的优化如LongAdder代替AtomicLong等等，很多地方都是运用了这种思想降低竞争资源粒度。 对资源的获取以及使用资源进行串行化，通过锁、CAS、队列来串行化获取资源和操作资源的操作。 写时复制避免读写竞争。比如ArrayList的写操作，没有必要在写的过程不让读，通过写时复制是可以同时进行读的。对应的并发容器CopyOnWriteArrayList就是采用的写时复制原理使得随时都可以读，类似的还有InnoDb的MVCC快照读、各种读缓存机制如eureka的多级缓存机制。等下讲到的JVM针对共享变量主内存、工作内存都是类似的思想。 内存可见性 什么是内存可见性 先看这张图：\nimg.png\r如图，在Java内存模型中，对于共享变量data，两个工作线程都需要读取这个值的时候，实际上是读取和操作的副本，类似高速本地缓存，这样做的原因是使用缓存机制降低对data的读写并发冲突，不然都去读写同一个内存地址，效率是很低的。\n但是这样造成的问题是，各个工作线程的变量不是即时同步的，如线程1将data改为了1，但是对于线程2来说可能他自己的data副本中还没有同步为1，他读取的还是0。这就是内存可见性问题。\n怎么避免内存可见性导致的问题 一是使用synchronized同步锁，锁住共享数据。但是这种对于解决内存可见性来说较重。 二是使用volatile关键字，使用volatile关键字修饰时，可理解为对数据的操作都在主存中进行，相比synchronized同步锁，volatile关键字更轻量级一点。另外volatile除了解决了内存可见性，也禁止指令重排（比如懒汉式单例，在初始化一个对象赋值给一个变量的过程(instance = new Instance()这句代码)，可能分为三步，第一步开辟内存空间，第二步调用构造函数初始化对象，第三步将变量指向分配的内存空间，当不加volatile关键字时无法保证这三步的顺序，有可能执行顺序为1,3,2。那么就导致多线程中有可能变量不为null,但是对象却没有初始化完成,然后返回这个变量不为null但是指向的内存却没有初始化完成的变量，可能导致程序报错）。 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-10-02T00:00:00Z","permalink":"https://runningccode.github.io/2019/jdk%E5%B9%B6%E5%8F%91%E5%8C%85%E6%B8%A9%E6%95%85%E7%9F%A5%E6%96%B0%E7%B3%BB%E5%88%97%E4%B8%80-%E7%AB%9E%E6%80%81%E6%9D%A1%E4%BB%B6%E4%B8%8E%E5%86%85%E5%AD%98%E5%8F%AF%E8%A7%81%E6%80%A7/","title":"JDK并发包温故知新系列（一）—— 竞态条件与内存可见性"},{"content":"JVM中垃圾回收的判定标准 最终目的是将内存中无用的对象回收掉。具体的判定方法有：\n引用计数法，不采用，指的是维护对象被引用的次数，次数为0则意味着是垃圾。 可达性算法-GC Roots tracing，指的是从GC Roots开始往下遍历所有引用的对象，（每个GC Root就是一个树状图），所有被引用到的对象就是需要存活的对象，其他对象可以被回收。GC Root指的是，虚拟机栈(栈帧中的本地变量表)中引用的对象，方法区中非基本类型的类静态变量（一个地址）所引用的对象，本地方法栈中JNI(即一般说的Native方法)引用的对象。 JVM中内存相关参数 -Xms Java堆内存初始大小 -Xmx Java堆内存最大大小 -Xmn Java堆内存中的新生代大小，扣除它就是老年代大小 -XX:PermSize（1.8之后：-XX:MetaspaceSize） 永久代初始大小 -XX:MaxPerSize（1.8之后：-XX:MaxMetaspaceSize） 永久代最大大小 -Xss 每个线程的栈内存大小 注：通常情况下，Xms和Xmx，-XX:PermSize和-XX:MaxPerSize都会设置为一样。\n-XX:MaxTenuringThreshold 多少岁进入老年代-默认15 -XX:PretenureSizeThreshold 超过多少字节的大对象直接进入老年代 -XX:HandlePromotionFailure MinorGC时，如果老年代剩余空间小于新生代对象总大小，但是如果大于之前平均进入老年代对象的大小，是否尝试进行MinorGC(默认开启) -XX:SurvivorRatio=8 Eden区的比例 上面看不懂的参数不要深究，等下提到回过头再来看，这里只是将所有参数罗列出来方便查找。\nJVM中的内存分代模型 JVM中，将对象在内存中分为了三代：\n年轻代：很快被回收的对象，存在于堆，具体还在内存中分为了1个eden区，和2个survivor区。 老年代：长期存在的对象，存在于堆 永久代：指的就是方法区（存放Class元数据），回收条件较苛刻，需满足：该类所有实例对象所有已经从堆内存被回收，该类classLoader已经被回收，该类Class对象没有任何引用 为什么要分代勒，因为针对每个年龄代，都有不同的垃圾回收算法，以及内存分配机制。如果将所有对象放在一起，第一是会造成频繁遍历判断回收的开销，第二是会造成复制、移动的开销，为什么会有复制、移动，因为回收内存必然会造成内存碎片，而内存碎片会导致空间浪费，所以必须通过复制、移动来清理随便，使得空闲内存连续。\nJVM中具体的内存分配模型 img.png\r如上图，至于年轻代为什么要如此分配，与特定的回收算法有关。\n对象在内存分代中如何流转 年轻代 大部分对象刚创建的时候都会分配在年轻代的Eden区，只要年轻代空间不够就会触发MinorGC(只回收年轻代内存)，minorGC采取复制算法进行回收，当JVM运行触发第一轮minorGC时，会将eden区存活的对象先复制到一个suprivor区。然后删除eden区对象，当触发下一轮minorGC时，又把suprivor区和eden区的存活的对象转移到另一个suprivor区，然后删除这两个区的所有对象。依次类推。至于为什么要用复制算法，包括老年代的标记整理算法，这是考虑到了避免内存碎片。如果对象内存不连续，会造成很多的空间浪费。\n老年代 老年代的对象都是从年轻代根据一定的规则流转过来的。 具体有几类流转方式：\n超过指定年龄（参数-XX:MaxTenuringThreshold 配置，默认15），这里年龄指的是没有被垃圾回收，存活下来一次理解为增加一岁。流转到老年代。\n大对象直接进入，超过参数指定字节数（-XX:PretenureSizeThreshold）设置的字节数的大对象会直接进入老年代，这是因为对象越大，复制开销就越大。\n动态年龄判断规则进入，意思是不一定要到指定年龄再流转到15，如果某一年龄以上的对象到达一定大小，也会提前进入老年代。当躲过一轮GC的对象加起来超过surrvivor区50%，如年龄1+年龄2+年龄n一直累加，直到年龄n的时候发现加起来超过了surrvivor空间的50%，则年龄n以上的对象直接进入老年代\nminorGC发生时，suprivor区放不下，则所有存活对象转移到老年代。这里涉及一个老年代分配担保规则，指的是每次MinorGC发生时，都会判断老年代可用内存大不大于，年轻代存活对象内存之和，如果大于则直接进行minorGC,如果小于则要看参数XX:HandlePromotionFailure是否启用（默认启用），如果启用则对老年代这次需要承载的转移对象内存进行预估（取前面minorGC被转移的平均内存大小），若大于则也进行MinorGC,若意料状况外转移内存超出了老年代可用空间，则进行FullGC,若fullGC还是不够，则抛出OOM错误。FullGC是采取的标记整理算法，指的是移动存活对象，让内存连续，然后删除需要回收的对象，为什么使用标记整理？因为认为老年代对象存活几率高，复制算法不划算。\n永久代 永久代存放的是元数据信息，当类加载时，类元数据信息写入永久代，fullGC时永久代数据被回收，回收条件是：该类所有实例对象所有已经从堆内存被回收，该类classLoader已经被回收，该类Class对象没有任何引用。\n附图：\nimg_1.png\r谈一个JVM优化实例 现一个日处理量上亿数据的计算系统，不断从Mysql和其他数据中间件中提取数据进行计算处理。每分钟执行500次数据提取和计算任务，每次任务处理耗时10秒，每次处理1万条数据（每条数据20个字段），但是集群部署，共5台机器，1台机器每分钟处理100次任务，每台机器是4核8G内配置，JVM分了4G，3G堆内存，1.5G年轻代，1.5G老年代。\nimg_2.png\r我们先来估算一下内存占用：\n每条数据20个字段，可以估算一条数据为1KB大小左右 每次计算1W条，那么一个任务占用内存就是1KB*1W=10MB数据左右，一台机器每分钟处理100次任务，暂用内存约为1G左右，基本上一分钟多点后Eden区就被占满了。 实际生产环境是怎么样的勒？\n一分钟之后的第一次GC，此时的内存情况: img_3.png\r每个任务处理10S，意味着还有大概六分之一的数据应该存活，算200M，但是200M放不进Survivor区，所以会尝试往老年代放，老年代现在大于1.2G所以直接放就是了。\n每次MinorGC都会有大概200M进入老年代，当进行到第三次时 img_4.png\r此时老年代可用容量小于年轻代对象总内存，默认判断老年代剩余空间是否大于平均每次MinorGC转移过来的老年代对象容量，这里是大于，所以还是继续MInorGC。\n当进行到第八次时 img_5.png\r此时会触发FullGC清理老年代，于是老年代的对象被全部清理掉了：\nimg_6.png\r然后继续回到原来的第一次，每8次进行一次FullGC,也就是8分钟进行一次 优化策略 重新调整新生代老年代比例，扩大新生代内存为2GB，老年代1GB 此时一个Survivor区有200MB，每次MinorGC后都能存放的下存活对象，不用往老年代转移（当然还是有转移，这只是避免了suprivor区过小被迫转移的对象）。JVM优化的策略最核心的就是减少FullGC次数，因为扫描对象多了一个老年代和永久代、永久代标记算法略微复杂、老年代整理时由于对象较多比较慢的原因，FullGC效率是远远低于MinorGC的，一般时间是minorGC的10倍以上。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-09-03T00:00:00Z","permalink":"https://runningccode.github.io/2019/jvm%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E4%BA%8C-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6%E4%BB%A5%E5%8F%8A%E5%86%85%E5%AD%98%E5%88%86%E4%BB%A3%E6%A8%A1%E5%9E%8B/","title":"JVM从零开始（二） -垃圾回收机制以及内存分代模型"},{"content":"一般常用的缓存方案有两种：\n第一种 读的时候，先读缓存，缓存没有的话，读数据库，取出数据后放入缓存，同时返回响应。 更新的时候，先删除缓存，在更新数据库。 第二种 读的时候，先读缓存，缓存没有的话，读数据库，取出数据后放入缓存，同时返回响应。 更新的时候，先更新数据库，再删除缓存。 第二种是Cache Aside Pattern的原本思路，第一种也有在用。为什么会造成这两种分歧勒？原因在于：\n第一种方案引入了缓存-数据库双写不一致的问题，即读数据（写缓存）与修改数据（写数据库）并发的情况下，在删除缓存后与修改数据数据库事务提交间隙，此时来了个读请求，而且读请求跑的比较快，一下就执行完了，就会把旧的数据刷到缓存里面，这样就导致了缓存中的数据直到下一次修改数据库之前肯定是与数据库不一致的 第二种方案也会导致双写不一致，此时缓存中无数据，先是一个读数据的请求，在查询和设置缓存间隙突然来了个数据更新请求，而且数据更新请求跑的很快，一下就执行完了，这时在读数据请求这个线程里面设置的值却是较老的值，这样就导致了缓存中的数据直到下一次修改数据库之前肯定是与数据库不一致的。另外第二种方案还有一个情况，在更新的时候，如果删除缓存失败（应用突然宕机或redis不可用），也会引入数据库和缓存不一致的问题。 总结一下两种方案的数据库缓存不一致场景：\n第一种：a线程删除缓存 - b线程读数据库值为A并设置缓存值为A - a线程更新数据库值为B 第二种有两种情况： a线程读数据库值为A - b线程更新数据库值为B - b线程删除缓存 - a线程设置缓存值为A；\na线程更新数据库值为B-由于网络延迟或宕机没有删除缓存-系统恢复后-b线程读缓存值为A。\n另外还有一种导致缓存数据库不一致的原因还有读写分离，由于主从同步延迟，如果采取上面的两种方案，在极端情况下（从库读延迟），也有可能导致读请求写入缓存中的可能是旧数据。\n解决方案 一般来说，我们对缓存的一致性要求并没有很高，只要求最终一致性，在较短的时间内不一致都是能忍受的。不论是前面哪一种方案，就算发生了，再来一次更新请求只要不发生同样的情况，缓存都会被再次刷成一致的。所以解决方案从简易到复杂就有缓存过期时间兜底，保证“更新数据库、删除缓存”和“读数据库并设置缓存”的之间串行化。\n1.缓存过期时间兜底 就算更新操作非常少，没有更新操作，也有一个缓存过期时间，在缓存过期之后再次刷新缓存。\n2.串行化更新数据库和写缓存 解决这个目标的关键主要目的是保证“删除缓存、更新数据库”和“读数据库并设置缓存”两者之间要保证串行化。 基于此，可能的优化有以下几种：\n更新数据以及更新缓存整个过程用消息队列或加锁实现，即修改数据的时候通过mq通知修改，更新数据库、更新缓存。（适用于预热场景，对某些数据进行预热）。 更新数据的时候发送消息队列，更新数据库并删除缓存，读数据的时候如果没命中缓存先从数据库查出来返回，在发送消息队列，读数据库并设置缓存。 3.如果引入了读写分离 通过消费binlog日志消息，再次发送消息到mq去删除缓存，读数据若没有缓存的时候也发送消息到mq读数据并设置缓存。 通过延迟删缓存处理，但需控制延时时间，不能太长，导致这段时间缓存一直延迟。 今天看到的延时双删 网上的延时双删方案：\n读的时候，先读缓存，缓存没有的话，读数据库，取出数据后放入缓存，同时返回响应。 更新的时候，先删除缓存，在更新数据库,然后延时删缓存。 个人思考： 这种方案在写线程更删除缓存到更新数据库这段时间内，插入读请求，则到下一次延时双删之前会导致数据库缓存不一致。\n那么是否能够修改成：\n读的时候，先读缓存，缓存没有的话，读数据库，取出数据后放入缓存，同时返回响应。 更新的时候，先更新数据库,再删除缓存，然后再延时删缓存。 相比前面谈的先更新数据库再删除缓存，第一是缓解数据库读库延迟的影响，第二是用延时删缓存缓解前面谈到的先更新数据库再删除缓存的缓存不一致情况，即缓存没有的情况下，读请求读数据库和删缓存操作中间来了个写请求一下子执行完了，导致缓存脏数据。使用延时双删，可以在延时后把脏数据删掉（一般延时时间比缓存过期时间短的多），除非读请求线程太慢太慢了，延时的时间过了，都还没有设置缓存（本事读请求一般比写请求快，所以这种情况我们可以一定程度忽略）。第三是为了控制避免不必要的延时，前面先即时删一次缓存，而不是次次都延时。\n个人觉得这样，貌似更合理一些。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-09-02T00:00:00Z","permalink":"https://runningccode.github.io/2019/redis%E7%BC%93%E5%AD%98%E7%BB%B4%E6%8A%A4%E6%96%B9%E6%A1%88-%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%E7%BC%93%E5%AD%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8D%E4%B8%80%E8%87%B4/","title":"Redis缓存维护方案-怎么解决缓存与数据库不一致"},{"content":"一、前言 自从入行以来，一直没有深入学习过JVM，虽然看过好些书、博客也有一点了解，但都不是专门讲JVM的，所以对JVM停留在浅显的了解上，没有深入了解过，一直想深入学习，不过想学的东西太多了，这就导致JVM成了自己的短板，前些日子在网上看到了救火大队长的《从零开始带你成为JVM实战高手》专栏正在更新中，决定借此机会好好深入学习一下，顺便把学习心得结合以前学习的知识记录下来，加深自己的理解。\n二、JVM加载类的过程 加载，加载为按需加载，主线程需要用到一个类才开始加载一个类。 验证，简而言之就是校验class内容是否符合指定规范 准备，为类变量分配内存空间，并初始默认值。 初始化，执行类的初始化代码（静态代码块，静态赋值代码），注：初始化会检验父类是否初始化，没有则必须先初始化父类。 使用 卸载 三、类加载器 种类 启动类加载器-Bootstrap ClassLoader，用于加载Java核心类库-java安装目录下的lib目录下的class。 扩展类加载器-Extension ClassLoader，用于加载java下的一些扩展类库-java安装目录lib\\ext下的class。 应用程序类加载器-Application ClassLoader，加载项目ClassPath下所的class。 自定义类加载器，自己定义的类加载器 双亲委派机制 意思就是即优先让父ClassLoader去加载。原因是避免重复加载，保证应用的classPath下的Class对象在内存中的唯一性。\n层次图 img.png\rtomcat破坏双亲委派 tomcat作为java编码的web容器，本身也是在jvm中运行，怎么让基于tomcat容器的各个web应用正常运行，使得各个web应用包含的应用的class不会乱套，必须破坏双亲委派机制，为每一个web应用的class都用不同的类加载器去加载。\ntomcat的类加载器层次图 img_1.png\r其中：\ncommon、catalina、shared负责加载tomcat自己的核心类库，webApp加载web应用的class（隔离web应用引用的三方类库），jsp加载器加载JSP(每个JSP文件都对应一个Jsp类加载器，jsp加载器支持热加载，即Jsp只要修改，就重新加载一次覆盖原来的jsp)。\n通过tomcat的类加载机制，可以达成以下目标：\ntomcat支持各个web引用中不同版本的三方类库相互隔离。 同一个第三方类库的相同版本在不同web应用可以共享。 tomcat自身依赖的类库需要与应用依赖的类库隔离 。 jsp需要支持修改后不用重启tomcat即可生效 为了上面类加载隔离和类更新不用重启，定制开发各种的类加载器。 自定义类加载器的作用 实际中，可以从Web服务器、数据库或缓存服务器获取bytes数组，这就不是系统类加载器能做到的了。 可以从不同的路径中获取同一个类的不同class对象，可以实现隔离，一个复杂的程序，内部可能按模块组织，不同模块可能使用同一个类，但使用的是不同版本，如果使用同一个类加载器，它们是无法共存的，不同模块使用不同的类加载器就可以实现隔离，Tomcat使用它隔离不同的Web应用，OSGI使用它隔离不同模块。 可以实现热部署。使用同一个ClassLoader，类只会被加载一次，加载后，即使class文件已经变了，再次加载，得到的也还是原来的Class对象，而使用自定义类加载器，则可以先创建一个新的ClassLoader，再用它加载Class，得到的Class对象就是新的，从而实现动态更新。 四、JVM内存区域划分 方法区（1.8之后叫metaspace-元数据空间） 存放类的元数据信息，可以理解为反射获取的Class对象的哪些信息。\n程序计数器 记录当前执行的class文件中字节码指令的位置 与线程是一对一的关系，每个线程都会有自己的一个程序计数器 栈 一个程序计数器对应一个线程、一个线程对应一个栈、一个栈由一个个栈帧组成，一个方法对应一个栈帧，栈帧包含局部变量表、操作数栈、动态链接、方法出口（相当于结束方法时需要跳转的位置）。栈帧在调用方法的时候进栈，方法结束的时候出栈。\n堆 存放对象数据（上述栈中只存在对象的引用（即堆内存中对象的地址）或基本数据类型的值） 所有线程共享 当引用变量地址数据出栈时，堆中的内存释放由JVM控制回收 其他内存区域 执行native方法会有线程对应的本地方法栈。 NIO中ByteBuffer类方法：allocateDirect（int capacity）-堆外分配内存空间返回DirectBuffer，通过DirectBuffer的Api可以操作堆外内存。 图示 img_2.png\r四、小结 这篇文章是JVM总结的第一篇博客，可能会觉得有点Low，不过写这篇博客也是想从零开始学习总结一番，就这样吧。 文中大部分内容来、图片（我也自己画过，但是太丑了，就不贴了）来源于救火大队长的《从零开始带你成为JVM实战高手》专栏文章所学笔记心得，地址在下面，有兴趣的朋友可以去看看（是收费专栏，不过较便宜）。\nimg_3.png\r本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-08-31T00:00:00Z","permalink":"https://runningccode.github.io/2019/jvm%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E4%B8%80-jvm%E7%9A%84%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F/","title":"JVM从零开始（一）-JVM的类加载、内存区域"},{"content":"前言 本文主要介绍Hystrix的基本功能，原理。Hystrix协调分布式系统中各个服务高可用的框架，SpringCloud体系重要的一员。Hystrix主要解决了服务雪崩问题、服务监控问题等。\nHystrix功能介绍 资源隔离与限流 作用 主要在于预防依赖服务崩溃影响到自己服务资源被耗光，使用隔离的方式限制调用依赖服务所能使用的最大线程资源。如果一个服务崩溃，不控制好资源隔离的话，会引发服务雪崩，众多上游服务集体崩溃。 另外在并发量很大的情况下，也可以降低对依赖服务请求的流量，避免依赖服务过度负载。 原理 Hystrix将对外部服务的调用和一系列处理封装为一个Commond对象，具体分为HystrixCommand（直接调用命令），HystrixObservableCommand（观察者订阅命令），后者可以注册回调事件。两者都可以以异步或同步的方式调用（异步返回Future，后续可以调用get阻塞获取结果）。\nCommond对象对资源隔离进行隔离有两种方式，一种是使用线程池（常用），第二种是使用信号量。两者都是对Commond被调用的线程并发数量进行限制，区别在于信号量只是对command被调用并发许可的一个线程，不能记录超时信息等等。\n配置项 选择Commond资源隔离的策略，线程池（常用）或信号量 指定command名称、组。名称默认为类名（一个command通常对应一个依赖服务的接口调用），group下的所有command默认情况下共用一个线程池或用一个信号量隔离（group一般对应一个依赖服务）。 指定command线程池,HystrixThreadPool的key默认为组名称，也可以手动指定使用的HystrixThreadPool。 指定线程池核心线程数大小，默认为10。 指定线程池队列长度，默认-1 无界队列，设置值之后超过则拒绝请求。 指定线程池拒绝阀值，默认5，超过就拒绝请求。 信号量隔离下可设置最大并发访问量，默认10。 requestCache-请求缓存 原理 在一个web调用主线程中，为该主线程调用服务的command建立缓存，使得调用依赖服务相同参数的情况下直接从内存中获取结果，由于本人认为其作用不是很大，所以就不多介绍了，主要是在拦截器中初始化HytrixRequestContext（这个缓存数据目测应该是在线程本地变量里面），然后Command中实现getCacheKey方法返回cache的key，当key一样时，直接尝试获取之前调用后返回相同key的结果。\nfallback-服务降级 原理 通过Command实现getFallBack方法（HystrixCommand）或resumeWithFallback方法(HystrixObservableCommand),实现降级逻辑，本地返回降级之后的结果。\n降级的条件 线程池或信号量拒绝 调用超时 执行报错 断路器被开启 降级的通常逻辑 取本地缓存中较老的数据 返回默认值 注：降级逻辑可以设置参数限制并发线程数量（基于信号量） circuit breaker-断路器 原理 打开条件 调用超时达到一定比例 执行报错达到一定比例 线程池或信号量拒绝达到一定比例 半开状态 一定时间之后会切换到半开状态，让1个请求通过，看是否能够正常返回\n开启状态 如果半开状态允许通过的请求成功了，就关闭断路器\n相关可配置项 是否打开断路器（默认打开） 断路器时间滚动窗中最小的请求数（默认20） 多少异常（包括超时、报错、拒绝）比例（默认50%）打开断路器 多长时间进入半开状态（默认5000毫秒） 是否强制打开断路器 是否强制关闭断路器 超时时长配置（默认1000毫秒） 是否打开超时机制（默认打开） 小结 在实践中，通常用了hystrix，一般的架构体系都是SpringCloud那一套，因为feign组件封装了hystrix可以实现熔断、服务隔离。本文是在学习原生Hystrix所总结的，涉及到的使用和配置方法可能会与当前实践相偏离，但是看官着重hystrix的功能原理就好，具体应用时可以再查阅相关资料。 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-08-30T00:00:00Z","permalink":"https://runningccode.github.io/2019/hystrix%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"Hystrix学习笔记"},{"content":"一、为什么要用分库分表 当不使用分库分表的情况下，系统的性能瓶颈主要体现在：\n当面临高并发场景的时候，为了避免Mysql崩溃（MySql性能一般的服务器建议2000/s读写并发以下），只能使用消息队列来削峰。 受制于单机限制。数据库磁盘容量吃紧。 数据库单表数据量太大，sql越跑越慢 而分库分表正是为了解决这些问题，提高数据库读写并发量，磁盘容量大大提高，单表数据量降低，提高查询效率。\n二、垂直拆分和水平拆分 以表的维度来说： 垂直拆分 指根据表的字段进行拆分，其实很常见，有时候在数据库设计的时候就完成了，属于数据库设计范式，如订单表、订单支付表、商品表。 水平拆分 表结构一样，数据进行拆分。如原本的t_order表变为t_order_0,t_order_1，t_order_3 以库的维度来说： 垂直拆分 指把原本的大库，按业务不同拆到不同的库（微服务一般都是这么设计的，即专库专用） 水平拆分 一个服务对应多个库，每个库有相同的业务表，如库1有t_order表，库2也有t_order表。业务系统通过数据库中间件或中间层操作t_order表，分库操作对于业务代码透明。 所以，我们平常说的分库分表，一般都是指的水平拆分\n三、分库分表工具 主要关注MyCat和sharding-jdbc。\n两者对比：\nMyCat:基于中间件的形式，提供读写分离、分库、分表功能。只不过好久都没更新了，我使用的是1.6版本，并不支持一个逻辑表的分库、分表同时存在。 sharding-jdbc：基于jar包中间层的形式，提供读写分离、分库、分表功能。社区较活跃。支持功能强大。 四、分库分表的两种策略 hash分法，按一个键进行hash取模，然后分发到某张表或库。优点是可以平摊每张表的压力，缺点是扩容时会存在数据迁移问题。 range分法，按范围或时间分发，比如按某个键的值区间、或创建时间进行分发，优点是可以很方便的进行扩容，缺点是会造成数据热点问题。从分表上说还好，如果是分库，将导致某一个库节点压力过大，节点间负载不均。 这里，我认为最好的分法是hash分库、range分表。因为对库来说重要得是负载要均衡，对表来说重要的是可以动态扩容。\n五、分库分表数据迁移方案 停机分库分表 方案很简单，就是停机维护时，用后台临时程序基于数据库中间件将老库数据直接分发到需要迁移的数据库\n此方案的主要缺点是一定会出现几个小时的停机，如果没搞定要回滚，第二天继续搞，开发人员心是慌得。\n不停机双写方案 主要步骤：\n修改系统中的所有写库的代码，同时写老库和数据库中间件（包括新增、更新、删除操作） 然后后台用工具将老库之前的老库数据迁移到新的数据库中间件，注意比对修改时间，若id一样，按修改时间决定是否覆盖。 使用后台工具比对一次数据，看是否完全一样，不一样，则后台再用工具进行一次迁移。（避免有些数据因为网络问题没有迁移成功，或业务上的bug导致），这个过程通常需要好几天。 以此循环几次，数据完全一样时，就切换为只读写新库 该方案解决了停机造成的服务不可用。\n六、分库分表下的动态扩容问题 在分库分表的情况下，如何在已经分库分表的基础上进一步分库分表提高系统效率，是一个麻烦的问题。特别是基于hash分片的服务器，再次分库分表，一般只能对服务器进行停机，然后将所有数据又基于新的规则插入到不同的库与表。为了避免这种问题，可以在第一次分库分表的时候就将库切分的较细，避免二次扩容。比如：\n最开始就将库分为32个库，最开始业务量没那么大，可以将多个库放在同一台机器上，以后按照2-4-8-16-32来进行扩容，如最开始2台机器能够满足数据库读写并发，此时一台服务器上有16个库，后来不够了，就扩容为4台机器，每台机器8个库。。。依次类推，这样做的好处是只需要迁移需要迁移的库，并且是按照整个库进行迁移，不需要重新进行分发，同时分库分表的分片机制也不用修改，只需要修改其数据源就行了。 对于分表来说，也可以分的多一些，推荐分为32张表。 以上，分为了32个库32张表，总共数据量可以达到32 * 32 = 1024张表，按每张表500万正常的容量来算，可以容纳约50亿数据，足以满足大部分过扩容需求。 另外这种方案推荐扩容方案为2-4-8-16-32倍数进行扩容，深层原因是32是这些数的公倍数，按照约数进行扩容更容易让每个机器负载的库都一样。 需要注意的是如果按照同一分片键进行同样的分片策略分库分表，会导致数据只会达到某库的某表比如1库的1表，2库的2表，（因为库和表的数量也是一样）所以分库我们可以按32取模策略，分表的话我们可以按整除之后的余数再对32取模进行分表。 七、全局id的生成策略 几种生成id的方式对比：\n通过数据库自增 往公用的一张表（这张表是自增主键）插入一条数据，获取id的返回值，用这个id再去插入中间件当中去。oracle可以通过自增序列。\n缺点：不适合并发高的场景，毕竟不管是自增序列还是采取自增键的方式来生成，会并发竞争写锁，效率太低。\nUUID 缺点：uuid太长了，不规则\n时间戳 一般联合其他业务字段拼接作为一个Id，如时间戳+用户id+业务含义编码\n缺点：并发高容易重复\n雪花算法 img_1.png\r原理：前面1位为定值0+41位为时间戳+5位机房id+5位为机器id+12位为序号,唯一需要保证同步的地方是生成一个序号，锁粒度较低。另外这个算法可用于分布式环境中。最大的优点是不需要依赖任何中间件，核心原理是用5位机房id，5位机器id标志了唯一一台机器，所以不需要分布式锁去保证不同机器生成id的同步性，只需要在当前机器保证生成的序号不一样就行了。\nredis中间件生成 原理：利用redis单线程工作线程属性去维护一个自增变量。\n八、读写分离 为什么要读写分离 理论上来说读写请求不要超过2000/s，如果加了缓存之后，到数据库请求还是超过2000以上考虑读写分离 使得读请求可以在不同机器并发，用了读写分离之后可以通过动态扩展读服务器增加读效率，这与redis中的主从架构读写分离、copyOnWrite机制的并发容器、以及数据库MVCC机制有点相识，都是通过读请求的数据备份增加读写并发效率。 适用于业务场景中，读请求大于写请求的情况，读写分离使得系统能够更多的容纳读请求并发。 读写分离的实现方式 一般来说是基于mysql自带的主从复制功能。mysql主从复制的流程图如下：\nimg.png\r总结mysql的主从复制过程大体是主库有一个进程专门是将将记录的Binlog日志发送到从库，从库有一个io线程（5.6.x之后IO线程可以多线程写入relay日志）将收到的数据写入relay日志当中，另外还有一个SQL进程专门读取relay日志，根据relay日志重做命令(5.7版本之后，从可以并行读取relay log重放命令（按库并行，每个库一个线程）)。\n主从同步的三种模式： 异步模式（mysql async-mode） 异步模式如下图所示，这种模式下，主节点不会主动push bin log到从节点，这样有可能导致failover的情况下，也许从节点没有即时地将最新的bin log同步到本地。\nimg_2.png\r半同步模式（mysql semi-sync） 这种模式下主节点只需要接收到其中一台从节点的返回信息，就会commit；否则需要等待直到超时时间然后切换成异步模式再提交；这样做的目的可以使主从数据库的数据延迟缩小，可以提高数据安全性，确保了事务提交后，binlog至少传输到了一个从节点上，不能保证从节点将此事务更新到db中。性能上会有一定的降低，响应时间会变长。如下图所示：\nimg_3.png\r同步模式（mysql semi-sync） 全同步模式是指主节点和从节点全部执行了commit并确认才会向客户端返回成功。\n读写分离场景下主从延迟可能导致的问题 在代码中插入之后，又查询这样的操作是不可靠，可能导致插入之后，查出来的时候还没有同步到从库，所以查出来为null。如何应对这种情况了？其实并不能从根本上解决这种情况的方案。只能一定程度通过降低主从延迟来尽量避免。\n降低主从延迟的方法有：\n拆主库，降低主库并发，降低主库并发，此时主从延迟可以忽略不计，但并不能保证一定不会出现上述情况。 打开并行复制-但这个效果一般不大，因为写入数据可能只针对某个库并发高，而mysql的并行粒度并不小，是以库为粒度的。 但这并不能根本性解决这个问题，其实面对这种情况最好的处理方式是：\n重写代码，插入之后不要更新 如果确实是存在先插入，立马就能查询到，然后立马执行一些操作，那么可以对这个查询设置直连主库（通过中间件可以办到） 参考：深度探索MySQL主从复制原理\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-08-26T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E4%B8%AD%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/","title":"分布式环境中分库分表、读写分离相关问题总结"},{"content":"分布式锁 分布式系统中，常见的分布式锁有两种，一种是基于Redis实现的分布式锁，一种是基于ZooKeeper锁。本篇文章简要介绍下其原理及方案。\nRedis锁 redis锁简单版本 上锁 先说上锁的命令，上锁的命令是：set {lockName} {randomVal} nx px 30000。\n其中，nx 参数的意思是不存在锁的时候设置，px参数表示毫秒数，该条命令表示当不存在lockName键的时候，为其设置值为randomVal，并设置过期时间为30000毫秒。当redis中存在该键是redis返回nil。其他线程（包括其他机器）来获取锁的时候，可以用轮询来判断是否上锁成功，达到阻塞其他线程的目的。\n解锁 解锁，我们需要实现的需求是不能删除掉其他线程设置的锁。因为某些情况比如锁超时，其他线程还是会获取到锁。所以必须先判断锁是不是自己设置的再进行删除，由于redis没有提供一个原子命令判断当前值是什么再进行删除，所以必须向redis传入lua脚本以确保解锁操作的原子性。解锁的原理是传入随机值进行解锁，脚本中会判断当前Key存不存在，存在的话再判断值是否与传入的随机值相等，相等则将其删除。\n缺点 这种上锁方案有很明显的的缺点，如：\n一是这种方案并不是很可靠，被上锁的redis宕机后容易丢数据，就算是配置了哨兵，也存在主备切换的时候可能丢数据。 二是不能实现公平锁。 三是轮询阻塞这种方式开销有点大。 四是不可实现线程重入。 没有续约机制 针对第一点，redis官方建议使用基于redisCluster的redlock（红锁）方案。这种方案核心要点就是需要在大多数redis节点上获取锁成功才算成功。但这就意味着开销变大了，并且针对红锁这种方案，网络上也有些大佬们提出质疑。\n解决方案 这种简单版本的redis分布式锁方案并不能解决这些问题，如果要解决可以使用redission框架，redission运用了队列、redis发布订阅机制，看门狗机制，较为复杂的加锁、释放锁脚本解决了这些问题。redission支持可重入锁、公平锁、红锁等，并且将redis锁按JDK的Lock接口进行了封装，操作简单易用。\nZK锁 简单版本的不公平ZK锁 上锁 以在ZooKeeper中成功创建临时节点为标识，创建成功则获取锁成功，创建失败则监听该节点，直到节点删除在尝试创建临时节点。 为什么使用临时节点？避免服务宕机，导致死锁问题。\n解锁 删除节点即解锁成功。\n方案缺点 这种方案的缺点是锁是不公平的，并且节点删除唤醒的其他监听线程比较多，效率没有接下来介绍的使用临时顺序节点的方案只唤醒下一个监听节点的方式高。\n基于临时顺序节点的公平ZK锁 上锁 每次尝试获取锁都尝试创建一个临时顺序节点，并且获取当且父节点下的所有临时顺序节点，如果前面还有节点，则获取锁不成功，此时将主线程阻塞，监听前面一个节点被删除，如果被删除再唤醒主线程。反之如果当前创建的临时顺序节点前面没有节点则获取锁成功。\n解锁 删除当前临时顺序节点即解锁成功。\n解决的问题 这种方案实现的是公平锁，以前的并发竞争ZK临时节点创建，改为依次唤醒，降低了一定开销。\n两种方案的对比 个人觉得对于分布式系统来说，redisCluster红锁的设计不是很优雅，感觉基于zookeeper集群高可用的zk锁更优雅一些。所以如果做技术选型的话，个人倾向zk锁。但是如果技术架构中没有搭建zookeeper，可能选择的是springcloud那一套，选择redisssion封装的redis锁也行。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-08-20T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E4%B9%8Bredis%E9%94%81%E5%92%8Czk%E9%94%81/","title":"分布式锁之Redis锁和ZK锁"},{"content":"一、流程简图 dubbo的流程简图：\nimg.png\r二、通信协议 dubbo支持多种通信协议，包括：\ndubbo协议（默认）。单一长连接（消费者和生产者一直保持单一的长连接）+ NIO异步通信（provider作为服务端轮询socket）+ hessian序列化协议。适用传输数据量很小，但是并发量很高，消费者远远大于生产者。 rmi协议。java序列化+短连接,适用于消费者和提供者数量差不多，适用于文件传输，一般较少用。 hessian协议。hessian序列化 + 短连接。适用于生产者数量比消费者数量还多，多用于文件传输，一般较少用 http协议。json序列化 webservice。soap文本序列化 三、功能介绍 （一）负载均衡 可配置负载均衡策略有：\nrandom loadbalance（默认，随机）。随机，按权重设置随机概率 roundrobin loadbalance（轮询）。按公约后的权重设置轮询比率。存在慢的提供者累积请求的问题 leastactive loadbalance（最少活跃调用数）。最小活跃数负载均衡。活跃调用数越小，表明该服务提供者效率越高，单位时间内可处理更多的请求。此时优先将请求分配给该服务提供者。在具体实现中，每个服务提供者对应一个活跃数 active。初始情况下，所有服务提供者活跃数均为0。每收到一个请求，活跃数加1，完成请求后则将活跃数减1 consistanthash loadbalance（一致性 Hash）。相同参数的请求总是发到同一提供者。 （二）集群容错 可配置的集群容错策略有：\nFaliover Cluster（默认）。失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过 retries=\u0026ldquo;2\u0026rdquo; 来设置重试次数(不含第一次)。 Failfast Cluster。快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。 Failsafe Cluster。失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。 Failback Cluster。失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。 Forking Cluster。并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=\u0026ldquo;2\u0026rdquo; 来设置最大并行数。 Broadcast Cluster。广播调用所有提供者，逐个调用，任意一台报错则报错。通常用于通知所有提供者更新缓存或日志等本地资源信息。 （三）动态代理 dubbo动态生成代理类主要有两种方式，一种是基于Javassist方式动态生成代理类（默认）-基于，另一种是用jdk动态代理。个人也不是很了解Javassist、Cglib、JDK动态代理的底层原理区别。只能说这三种方式都能生成代理类，区别在于Javassist相比较而言比较灵活，API接近底层。JDK的动态代理限制较大，必须实现接口，Cglib生成的动态代理类直接是被代理类的父类。\n（四）SPI机制-Service Provider Interface SPI，我的理解就是框架方提供接口，使用方自己实现接口，提高框架扩展性的一个机制。意思就是框架方通过接口约定方法的职责实现了一套逻辑。并没有明确表明这个接口一定要按照那种具体方式实现。所以就用SPI机制让我们自己可以在不改变框架的基础上去自己定义具体的实现。JDK提供了SPI机制的实现方式，但是dubbo不是直接使用的JDK实现的SPI，而是采用了自己的实现。下面介绍两种方式的用法：\n1.JDK实现SPI 第一步：在jar包或工程的meta-inf/service文件夹下，创建文件，文件名为提供的接口名（包含全路径） 第二步：编辑文件内容，文件内容为实现类的全路径。 第三步：框架方，加载实现类： ServiceLoader\u0026lt;DriverService\u0026gt; serviceLoader = ServiceLoader.load(DriverService.class); for (DriverService driverService: serviceLoader){ System.out.println(driverService.getName()); } DriverService为接口，遍历得到的是在Jar包内找到的实现类。\n2、Dubbbo实现SPI 名词解释： 扩展点,称 Dubbo 中被 @SPI 注解的 Interface 为一个扩展点。 扩展,被 @SPI 注解的 Interface 的实现称为这个扩展点的一个扩展。 注解解释： @SPI：@SPI 注解标识了接口是一个扩展点 ， 属性 value 用来指定默认适配扩展点的名称。 @Activate:@Activate 注解在扩展点的实现类上 ，表示了一个扩展类被获取到的的条件，符合条件就被获取，不符合条件就不获取 ，根据 @Activate 中的 group 、value 属性来过滤。 @Adaptive：@Adaptive 注解在类上,这个类就是缺省的适配扩展。@Adaptive 注解在扩展点 Interface 的方法上时 ，dubbo动态的生成一个这个扩展点的适配扩展类（生成代码 ，动态编译实例化 Class ），名称为扩展点 Interface 的简单类名 + $Adaptive ，例如 ： ProxyFactory$Adpative 。这么做的目的是为了在运行时去适配不同的扩展实例 ， 在运行时通过传入的 URL 类型的参数或者内部含有获取 URL 方法的参数 ，从 URL 中获取到要使用的扩展类的名称 ，再去根据名称加载对应的扩展实例 ，用这个扩展实例对象调用相同的方法 。如果运行时没有适配到运行的扩展实例 ，那么就使用 @SPI 注解缺省指定的扩展。通过这种方式就实现了运行时去适配到对应的扩展。 注意： 若@Adaptive标志了实现类，和@SPI标志的默认实现Key，以及SPI接口方法上也标了 @Adaptive接受Url参数解析实现类，优先级次序是 @Adaptive标志的实现类大于Url参数大于@SPI标志的默认实现. 扩展dubbo框架中SPI接口的方式 dubbo自带了一些spi接口，这里介绍下怎么实现这些SPI接口。\n第一步：添加文件关联实现类 路径可以是：META-INF/services/（扩展点接口的全类名）、 META-INF/dubbo/（扩展点接口的全类名 ）、 META-INF/dubbo/internal/（扩展点接口的全类名）。\n文件内容，键为扩展名，值为扩展实现类路径，类似：\nadaptive=com.alibaba.dubbo.common.extension.factory.AdaptiveExtensionFactory spi=com.alibaba.dubbo.common.extension.factory.SpiExtensionFactory spring=com.alibaba.dubbo.config.spring.extension.SpringExtensionFactory 第二步：为dubbo框架中的可扩展点指定实现类 如 \u0026lt;dubbo：protocol name=\u0026lsquo;my\u0026rsquo; prot=\u0026ldquo;20000\u0026rdquo;/\u0026gt; 可以修改protocol的实现为指定扩展名为my的扩展。\ndubbo框架中的可扩展点： 包括动态代理方式（ProxyFactory）、负载均衡策略（LoadBalance）、RPC协议（Protocol）、拦截器（Filter）、容器类型（Container）、集群方式（Cluster）和注册中心类型（RegistryFactory）等。\n（五）服务降级 配置方式例 消费者配置文件：\n\u0026lt;dubbo:reference id=\u0026quot;iUser\u0026quot; interface=\u0026quot;com.dubbosample.iface.IUser\u0026quot; timeout=\u0026quot;10000\u0026quot; check=\u0026quot;false\u0026quot; mock=\u0026quot;return null\u0026quot;\u0026gt; \u0026lt;/dubbo:reference\u0026gt; 其中，关键属性为mock，其有以下几种属性值：\nfalse，不使用mock true/default/fail，mock调用名为接口+Impl类的对应mock方法 {mockClass}，mock调用${mockClass}对应方法 return xxx，直接返回xxx的Mock数据，xxx支持json数据 throw xxxException，直接抛出异常 force xxx xxx为接口实现类类名，mock表达式只含有force的话，直接走接口相同路径下类名为“接口+Impl类\u0026quot;对应相同方法的逻辑，不掉接口，相当于直接屏蔽接口。 （六）服务重试 配置方式例 生产者端：\n使用注解：@Service(retries = 1,timeout = 2000)。 使用配置：\u0026lt;dubbo:provider retries=\u0026ldquo;0\u0026rdquo; timeout=\u0026ldquo;3000\u0026rdquo;/\u0026gt; 四、可能导致的问题 （一）幂等性问题 一些写操作接口需要保证幂等性，特别是包含支付逻辑的接口。\n1.场景 单机接收到重复请求 相同服务不同机器接收到重复请求 2.解决方案 思路 针对每个请求都需要标识一个唯一id，每次处理之后必须有一个记录标识这个请求处理过，每次接收到请求时判断是否处理过，如果处理过就直接忽略。\n具体 基于数据库唯一索引 单机可直接基于本地MAP或SET 分布式的可以基于缓存如redis （二）顺序性问题 针对顺序性的业务，比如先修改后删除的操作，如果执行顺序错了，那么业务就错了。\n1.场景 消费者异步调用的生产者的一组服务接口没有阻塞等待确保顺序问题。\n2.解决方案 使用一致性hash算法负载均衡策略。若这一组服务接口位于同一个服务，可以用一致性hash算法负载均衡策略使得调用的服务被分发到一台机器上，然后这台机器又采取内存队列的形式进行消费。 使用分布式锁机制。锁的key为对应业务单号，值为顺序号，每个生产者服务接口获取锁的时候判断下业务Key的value中的顺序值然后判断是不是该轮到自己。若是则获取锁。若不是则阻塞一段时间再去获取锁。 五、小结一下设计rpc框架的思路 依赖zookeeper或其他做一个注册中心 比如zookeeper，可以为每一个接口的每一个方法注册一个临时节点，然后key为接口方法的唯一标识（包含class路径、方法名称，参数签名），data为服务地址列表\n消费者调用服务 调用服务应该设计为动态代理，该动态代理类处理，拉取服务信息、负载均衡、序列化参数、发送请求。具体可以设计为先根据调用的接口去查本地缓存有没有该服务地址列表，如果有直接用一定的算法比如轮询取其中之一的地址，然后地址有了之后，将参数和请求id（因为需要将获取的响应关联请求Id）封装为一个对象比如名叫Invoker类，选择一定的序列化协议将数据发送给生产者，同时将Invoker对象放入一个本地内存并发容器中如名叫inProgressInvoker，用requestId作为Key，然后用Invoker对象的wait方法阻塞自己（这里先配置发送通信方式可以使用netty，设定接收到返回的回调方法，接收到生产者响应之后首先压回队列然后通过notifyAll唤醒后台队列的消费线程(1个或多个，可以通过参数配置)）。\n后台消费线程的设计是先从队列取出消息（如果没有取到，则调用上面回调方法里notifyAll中的锁资源的wait方法阻塞自己），如果有消息，则先反序列化接收到生产者返回的内容，然后根据requestId去上面的inProgressInvoker取出Invoker对象，将响应设置到Invoker对象中，同时调用notifyAll或notify方法，唤醒消费那里被阻塞的主线程，\n生产者处理请求 生产者这边的netty监听到事件之后，通过线程池处理请求，将请求数据反序列化解析为对象，通过接口方法唯一标识，以及参数信息来反射调用真正的接口实现类，处理好之后连带请求id在通过一定序列化协议返回给消费者。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-08-19T00:00:00Z","permalink":"https://runningccode.github.io/2019/dubbo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"dubbo学习笔记"},{"content":"一、前言 一直很好奇，公司脚手架具体是如何自动在插入或修改数据库的时候自动去改修改时间，创建时间，如何去生成业务主键。今天仔细看了下mybatis的源码和公司的源码大概了解了些。现将心得总结如下。\n二、MyBatis的插件机制 MyBatis是用了JavaSDK的动态代理机制对Executor、StatementHandler、PameterHandler和ResultSetHandler对象进行了动态的代理，具体是用interceptorChain.pluginAll(executor)方法去生成的代理对象：\npublic Object pluginAll(Object target) { for (Interceptor interceptor : interceptors) { target = interceptor.plugin(target); } return target; } 这个类InterceptorChain，保存了所有Intercpetor拦截器，这个拦截器就是我们接下来需要实现的类。而这个plugin方法就是返回的代理类对象，我们往往就是通过Mybatis提供的Plugin.wrap(target, this)方法来直接返回的生成的代理类，这个方法封装了生成代理类的过程。为了便于理解我们来看看Plugin.wrap的源码：\npublic static Object wrap(Object target, Interceptor interceptor) { Map\u0026lt;Class\u0026lt;?\u0026gt;, Set\u0026lt;Method\u0026gt;\u0026gt; signatureMap = getSignatureMap(interceptor); Class\u0026lt;?\u0026gt; type = target.getClass(); Class\u0026lt;?\u0026gt;[] interfaces = getAllInterfaces(type, signatureMap); if (interfaces.length \u0026gt; 0) { return Proxy.newProxyInstance( type.getClassLoader(), interfaces, new Plugin(target, interceptor, signatureMap)); } return target; } 这里具体前面几行就不细看了，大概意思就是获取我们实现的拦截器上面的@Signature注解里面的参数（这里可能还不知道到拦截器的@Signature注解是什么，等会解释），获取被代理类需要实现的接口，然后后面就是生成动态代理类的标准写法了，这里顺便解释下newProxyInstance的各个参数把：\n第一个参数表示的是动态生成的代理类的类加载器。 第二个参数表示的动态生成的代理类需要实现的接口，这些接口的方法在动态代理类中统统调用InvocationHander的invoke方法进行处理。 第三个参数就是InvocationHander的实现,这里Plugin就是Mybatis专门实现的一个类。 顺带一提：在JavaSDK动态生成的代理类做的事情实际就是初始化了真实类的所有Method对象为成员变量，然后实现了被代理类所有的接口，接口方法实现的逻辑很简单，就是把对应Method对象，以及实现的这个方法上面的参数，和自己交给Proxy.newProxyInstance传入的InvocationHander实例的invoke方法处理。\n我们这里再看看Plugin类的invoke方法：\npublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable { try { Set\u0026lt;Method\u0026gt; methods = signatureMap.get(method.getDeclaringClass()); if (methods != null \u0026amp;\u0026amp; methods.contains(method)) { return interceptor.intercept(new Invocation(target, method, args)); } return method.invoke(target, args); } catch (Exception e) { throw ExceptionUtil.unwrapThrowable(e); } } 大概意思就是根据刚刚取到的拦截器上面的@Signature注解信息标志的方法，执行我们自己拦截器的intercept方法，传入intecepter方法的是Mybatis自己封装的一个Invocation，其包装了动态代理类实例，被执行方法，以及方法参数，有一个proceed方法，就是通过反射真实执行被代理的方法。\n由上，我们可以知道，我们实现插件其实只需要创建Interceptor接口，然后Mybatis会自动将被代理的target一层层代理。其他源码诸如insert,update怎么执行的，上面的4大对象在Mybatis里面到底是怎么样的存在，我就没深究了，源码太复杂了。下面直接就来看怎么通过拦截Executor的update方法，修改传入参数，然后实现自动更新自己的业务主键和时间的把。\n三、实现插件 /** * @author chenzhicong * @time 2019/8/14 22:11 * @description * 首先我们必须要知道我们拦截的是Executor的update方法， 这个方法有两个参数，一个是MappedStatement， * 其维护了xml中一条\u0026lt;select|update|delete|insert\u0026gt;节点的封装,另一个则是这个节点的parameterType对应的参数， * 我们这里实现的就是若这个参数是我们定义的实体稍微修改一下，把修改时间和业务Code加上 * 注： * 1.Signature注解参数解释： * Type:需要拦截的类，可以为四大对象Executor、StatementHandler、ParameterHandler和ResultSetHandler。 * Method：需要拦截的方法的名称 * Args：需要拦截的方法有哪些参数 * 2.Executor 的 int update(MappedStatement ms, Object parameter) 方法 是处理所有insert和update的方法入口 */ @Component @Log4j2 @Intercepts({ @Signature( type = Executor.class, method = \u0026quot;update\u0026quot;, args = {MappedStatement.class, Object.class}) }) public class MyInterceptor implements Interceptor { @Override public Object intercept(Invocation invocation) throws Throwable { MappedStatement mappedStatement = (MappedStatement) invocation.getArgs()[0]; // 只处理参数为MyEntity子类的情况 Object param = invocation.getArgs()[1]; if (param == null || !MyEntity.class.isAssignableFrom(param.getClass())) { return invocation.proceed(); } MyEntity myEntity = (MyEntity) param; // 获取Sql执行类型 SqlCommandType sqlCommandType = mappedStatement.getSqlCommandType(); switch (sqlCommandType) { case INSERT: if (myEntity.getCreateTime() == null) { myEntity.setCreateTime(LocalDateTime.now()); } if (myEntity.getUpdateTime() == null) { myEntity.setUpdateTime(LocalDateTime.now()); } if (StringUtils.isEmpty(myEntity.getCode())) { // 这里小demo用uuid就行了，具体根据业务来， // 我们生产是用的redis生成的全局唯一键 myEntity.setCode(UUID.randomUUID().toString()); } break; case UPDATE: if (myEntity.getUpdateTime() == null) { myEntity.setUpdateTime(LocalDateTime.now()); } break; } return invocation.proceed(); } @Override public Object plugin(Object target) { // 这里就直接用MyBatis为我们提供的Plugin.wrap返回代理类 return Plugin.wrap(target, this); } /** * 这个方法用于在Mybatis配置文件中指定一些属性的。 * 具体也不是太懂，这里就算了不深究了 */ @Override public void setProperties(Properties properties) {} } 以上就是我们自定义的插件，我们测试一下：\n@Test public void test2() { UserJpaTest userJpaTest = new UserJpaTest(); userJpaTest.setNickName(\u0026quot;123\u0026quot;); userJpaTestMapper.insert(userJpaTest); } 看一看数据库效果：\nimg.png\r说明我们自定义的插件生效了，另外基于这个插件还可以做很多补充，比如插入是实体集合的情况入参就是DefaultSqlSession.StrictMap（StrictMap时Mybatis对参数为集合或者数组进行的处理，里面有三个键分为collection,list,array），还有就是TkMybatis封装的updateByExampleSelective方法，这个方法的入参就变成了了MapperMethod.ParamMap类(这个具体也不是太懂，当其键为record或param1时有可能是我们传入的record实体),这些在我们脚手架都是处理了的，由于是公司的代码，就不好放出来了。\n四、小结 基于Mybatis的插件功能，我们可以实现很多丰富易用的插件，比如我们常用的PageHelper就是基于此开发的。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-08-14T00:00:00Z","permalink":"https://runningccode.github.io/2019/mybatis%E6%8F%92%E4%BB%B6%E8%BF%90%E7%94%A8%E4%B9%8B%E5%AE%9E%E4%BD%93%E8%87%AA%E5%8A%A8%E4%BF%AE%E6%94%B9%E8%87%AA%E5%B7%B1%E7%9A%84%E4%B8%9A%E5%8A%A1%E4%B8%BB%E9%94%AE%E5%92%8C%E6%97%B6%E9%97%B4/","title":" MyBatis插件运用之实体自动修改自己的业务主键和时间"},{"content":"一、TypeHander介绍 typeHander就是mybatis处理Java类型和Jdbc类型时转换策略。mybatis在查询设置参数和从查询结果映射到Java类型时会调用相应的typeHander的方法完成转换。这里我写了一个小demo以完成自定义List到数据库的转换关系，mybatis自动注册了很多typeHander，比如Java中的基本类型，包装类型，mybatis都为我们注册了，当我们不指定具体的typeHander时，mybatis会自动选择合适的typeHander以完成java类型和jdbc类型的相互转换。\n二、过程 1.首先创建List的TypeHander /** * @author chenzhicong * @time 2019/8/13 20:37 * @description */ public class ListTypeHandler extends BaseTypeHandler\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; { @Override public void setNonNullParameter(PreparedStatement ps, int i, List\u0026lt;String\u0026gt; parameter, JdbcType jdbcType) throws SQLException { if (jdbcType == null) { ps.setString(i, StringUtils.collectionToCommaDelimitedString(parameter)); } else { ps.setObject(i, parameter, jdbcType.TYPE_CODE); } } @Override public List\u0026lt;String\u0026gt; getNullableResult(ResultSet rs, String columnName) throws SQLException { String s = rs.getString(columnName); return s==null ? null : new ArrayList\u0026lt;String\u0026gt;(Arrays.asList(s.split(\u0026quot;,\u0026quot;))); } @Override public List\u0026lt;String\u0026gt; getNullableResult(ResultSet rs, int columnIndex) throws SQLException { String s = rs.getString(columnIndex); return s==null ? null : new ArrayList\u0026lt;\u0026gt;(Arrays.asList(s.split(\u0026quot;,\u0026quot;))); } @Override public List\u0026lt;String\u0026gt; getNullableResult(CallableStatement cs, int columnIndex) throws SQLException { String s = cs.getString(columnIndex); return s==null ? null : new ArrayList\u0026lt;\u0026gt;(Arrays.asList(s.split(\u0026quot;,\u0026quot;))); } } 首先继承BaseTypeHandler,指定其泛型参数为我们需要转换的java类型，BaseTypeHandler运用了模板方法模式，封装了很多空值判断，异常处理。说明一下：这里这个类型转换器没有指定具体的jdbcType，所以mybatis会自动判断当java类型为List的时候就匹配这个这个转换器。\n2.然后注册到Mybatis的TypeHandlerRegistry 注册到TypeHandlerRegistry是为了全局自定义效果，如果不注册，需要在Mapper，xml里面的resultMap去指定typeHander如：\n\u0026lt;resultMap id=\u0026quot;personMap\u0026quot; type=\u0026quot;person\u0026quot;\u0026gt; \u0026lt;id property=\u0026quot;id\u0026quot; column=\u0026quot;id\u0026quot;/\u0026gt; \u0026lt;result property=\u0026quot;name\u0026quot; column=\u0026quot;name\u0026quot;/\u0026gt; \u0026lt;result property=\u0026quot;sex\u0026quot; column=\u0026quot;sex\u0026quot;/\u0026gt; \u0026lt;result property=\u0026quot;hobbys\u0026quot; column=\u0026quot;hobbys\u0026quot; typeHandler=\u0026quot;com.sankuai.lkl.typeHandler.ListTypeHandler\u0026quot;/\u0026gt; \u0026lt;result property=\u0026quot;date\u0026quot; column=\u0026quot;data_time\u0026quot;/\u0026gt; \u0026lt;/resultMap\u0026gt; 注册的思路是直接在spring容器加载完bean的时候从容器里面取出TypeHandlerRegistry，然后注册我们自定义的转换器：\n/** * @author chenzhicong * @time 2019/8/13 22:19 * @description */ @Component public class CustomTypeHandlerParser implements ApplicationContextAware { @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { //从spring容器获取sqlSessionFactory SqlSessionFactory sqlSessionFactory = applicationContext.getBean(SqlSessionFactory.class); //获取typeHandler注册器 TypeHandlerRegistry typeHandlerRegistry = sqlSessionFactory.getConfiguration().getTypeHandlerRegistry(); //注册List的typeHandler typeHandlerRegistry.register(List.class, ListTypeHandler.class); } } 三、测试 @Test public void test() { UserJpaTest userJpaTest = new UserJpaTest(); List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); list.add(\u0026quot;phoneNumber1\u0026quot;); list.add(\u0026quot;phoneNumber2\u0026quot;); userJpaTest.setPhoneNumber(list); userJpaTestMapper.insert(userJpaTest); } 四、效果 img.png\r可以看到，在数据库中已经把集合转换为了逗号分隔的字符串。\n五、进阶-创建枚举的TypeHandler TypeHander还经常应用于枚举与数据库字符串的转换策略，业务中，我们枚举通常有三个变量，一个是给前端的展示字符，一个是存入数据库的字符，另一个是我们在java系统的字符。为了让数据库只识别我们其中存入数据库的字符，同时把数据库中字符转换为我们想要的枚举。我们可以用自定义的转换器来便利的实现。下面介绍下过程：\n首先我们可以把所有枚举抽象为一个接口。如下，我们抽象了所有枚举为一个接口枚举的EntityEnumIFace接口，接口的getDbCode方法为需要转换的数据库字符串，我们通过这个转换器去处理这一类枚举抽象的转换：\npublic final class EntityEnumIFaceHandler\u0026lt;T extends Enum\u0026lt;T\u0026gt; \u0026amp; EntityEnumIFace\u0026gt; extends BaseTypeHandler\u0026lt;T\u0026gt; { private Class\u0026lt;T\u0026gt; type; public EntityEnumIFaceHandler(Class\u0026lt;T\u0026gt; type) { if (null == type) { throw new IllegalArgumentException(\u0026quot;type参数不能为空\u0026quot;); } else { this.type = type; } } @Override public void setNonNullParameter(PreparedStatement preparedStatement, int i, T t, JdbcType jdbcType) throws SQLException { if (jdbcType == null) { preparedStatement.setString(i, ((EntityEnumIFace)t).getDbCode()); } else { preparedStatement.setObject(i, ((EntityEnumIFace)t).getDbCode(), jdbcType.TYPE_CODE); } } @Override public T getNullableResult(ResultSet resultSet, String s) throws SQLException { String value = resultSet.getString(s); return value == null ? null : valueOf(this.type, value); } @Override public T getNullableResult(ResultSet resultSet, int i) throws SQLException { String value = resultSet.getString(i); return value == null ? null : valueOf(this.type, value); } @Override public T getNullableResult(CallableStatement callableStatement, int i) throws SQLException { String value = callableStatement.getString(i); return value == null ? null : valueOf(this.type, value); } private static \u0026lt;E extends Enum\u0026lt;E\u0026gt; \u0026amp; EntityEnumIFace\u0026gt; E valueOf(Class\u0026lt;E\u0026gt; enumClass, String dbCode) { E[] enumConstants = enumClass.getEnumConstants(); if (null != enumConstants) { for (E e : enumConstants) { if (e.getDbCode().equals(dbCode)) { return e; } } } throw new BusinessException(\u0026quot;ENUM_NOT_EXIST\u0026quot;, enumClass.getSimpleName() + \u0026quot;枚举中没有\u0026quot; + dbCode, false); } } 可能看了上面会奇怪，为什么前面定义的转换器实例化没有有参构造方法，这里却定义了有参构造方法？这个实例化过程又是怎么样的？别急，等下面我们再看，这里我们先将其注册到容器，我们怎么注册到容器勒？这里可以通过扫描指定包路径然后获取所有枚举的class对象再用typeHandlerRegistry.register注册到mybatis,如：\ntry { Set\u0026lt;Class\u0026lt;?\u0026gt;\u0026gt; set = ClassUtil.listClass(BUSINESS_ENUM_PACKAGE); set.forEach(o-\u0026gt;{ if(EntityEnumIFace.class.isAssignableFrom(o.getClass())){ typeHandlerRegistry.register(o.getClass(),EntityEnumIFaceHandler.class); } }); }catch (Exception e){ log.error(e.getMessage(),e); } 其中ClassUtil.listClass方法逻辑就是扫描包路径，然后用类加载器加载class文件获取class对象。\n我们可以看到这里和注册ListTypeHandler没有两样都是调用的同一个方法register（register(Class\u003c?\u003e javaTypeClass, Class\u003c?\u003e typeHandlerClass)），我们先进入这个方法看一看源码，源码调用了register(javaTypeClass, getInstance(javaTypeClass, typeHandlerClass)，我们在进去getInstance方法看一看，怎么实例化的typeHandler，源码如下：\npublic \u0026lt;T\u0026gt; TypeHandler\u0026lt;T\u0026gt; getInstance(Class\u0026lt;?\u0026gt; javaTypeClass, Class\u0026lt;?\u0026gt; typeHandlerClass) { if (javaTypeClass != null) { try { Constructor\u0026lt;?\u0026gt; c = typeHandlerClass.getConstructor(Class.class); return (TypeHandler\u0026lt;T\u0026gt;) c.newInstance(javaTypeClass); } catch (NoSuchMethodException ignored) { // ignored } catch (Exception e) { throw new TypeException(\u0026quot;Failed invoking constructor for handler \u0026quot; + typeHandlerClass, e); } } try { Constructor\u0026lt;?\u0026gt; c = typeHandlerClass.getConstructor(); return (TypeHandler\u0026lt;T\u0026gt;) c.newInstance(); } catch (Exception e) { throw new TypeException(\u0026quot;Unable to find a usable constructor for \u0026quot; + typeHandlerClass, e); } } 我们可以看到，实例化typeHandler会首先调用有参构造方法，将typeHandlerRegistry.register参数的javaTypeClass传入typeHandlerClass创建typeHandlerClass的实例，若没有反射获取到构造方法即抛了NoSuchMethodException异常，将进行忽略，进而调用无参的构造方法创建typeHandlerClass实例。所以我们ListTypeHandler和EntityEnumIFaceHandler都通过此方法成功实例化了，我想这个方法之所以这么设计应该是想让TypeHandlerClass能够知晓需要处理的具体Class类型然后去做更通用的抽象去处理这一类Class的转换方式吧。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-08-13T00:00:00Z","permalink":"https://runningccode.github.io/2019/mybatis%E8%87%AA%E5%AE%9A%E4%B9%89%E5%85%A8%E5%B1%80typehander/","title":"Mybatis自定义全局TypeHander"},{"content":"一、基础知识 （一）Redis的线程模型 img.png\r其中：\n1.多路IO复用器。 负责轮询监听服务器套接字和客户端套接字，这里是NIO的机制，所以每个套接字中间的可读，可写状态没有阻塞，避免了线程花费时间等待可读、可写状态。多路IO复用器监听之后生成了各个事件，分为可读和可写两类并携带对应套接字，然后遍历事件将其push到一个队列中去，由文件事件分派器去异步处理。\n2. 文件事件分派器。 它是单线程的，处理一个set命令的业务操作是原子的。文件事件分派器负责根据事件分派到不同的事件应答器去处理。这里分派的逻辑是：\n如果判断是服务器套接字产生的可读事件则交由应答处理器处理，应答处理器会创建一个客户端套接字，创建这个套接字代表客户端就可以向服务器发送命令了，当下次这个客户端套接字被监听到可读事件时代表服务器接收到了命令。 如果判断是客户端套接字产生的可读事件则交由命令请求处理器处理，处理器执行redis真正的业务操作，修改内存数据。之后命令请求处理器会将客户端套接字与命令回复处理器关联，以便下次监听到客户端尝试读取的命令的时候进行回复。 如果判断是客户端套接字产生的可写事件则交由命令回复处理器处理，处理器向套接字写入成功回复，然后接触套接字与命令回复器的关联。 （二）Redis的数据类型 包括string，list（说起是List，其实就是JAVA中的有序队列），hash散列表，set(无序，元素不重复)，sortedset（有序，元素不重复，set时需要指定分数，排序根据分数来排）。\n（三）Redis过期删除策略 过期删除有三种情况：\n1.定期删除 默认每隔100ms随机抽取一些设置了过期时间的key删除。\n2.惰性删除 后面在获取某个key时，redis会检查key是否设置了过期时间是否过期，如果过期就删除，不返回任何东西。\n3.内存淘汰机制时删除 指的是，内存不够了，redis会运行淘汰部分键，这里根据参数不同采取不同的策略，其中有：\nnoeviction,新写入会报错 allkeys-lru,把最近最少使用的key删除 allkeys-random,随机移除key volatile-lru，在设置了过期时间的键中，移除最近最少使用的key volatile=random，在设置了过期时间的键中，随机删除 volatile-ttl，在设置了过期时间的key中，删除马上要过期的key （四）持久化 Redis持久化有两种：RDB机制和AOF机制。其中RDB机制是定时全量在磁盘中备份内存中的数据。AOF机制是以命令日志的方式在磁盘中备份数据。当master节点宕机重启后，会根据持久化的数据还原到内存中。如果RDB和AOF被同时开启，将优先用AOF来进行数据恢复。\n两种机制的对比：\nQPS影响，AOF对QPS的影响是持续性的，RDB则只是间隔性的。 恢复速率，由于RDB不用重放命令，所以恢复速率RDB较快。 数据丢失量，由于AOF是近乎实时的，所以比RDB丢失的要少的多。 冷备份简易度，RDB非常适合做冷备份，一般的做法是用脚本将RDB文件自动同步到云上，这样可以使得Master节点磁盘文件打不开或者丢失，或者机器都重启不了也能使用冷备份文件复制到其他节点进行故障恢复， 二、redis的架构模式 redis架构模式有两种，一种是主从架构，一种是rediscluster架构。下面先总结下主从架构。\n（一）主从架构 主从架构相比rediscluster的优点是可以读写分离，但缺点是不可以通过加服务器的方式横向扩展redis内存空间。\n1.节点结构 主从架构由1个master节点，n个slave节点构成，master节点负责写，slave节点负责读。\n2.节点数据同步策略 （1）full resynchronization（全量复制） 全量复制场景一般是在slave第一次启动或者长时间离线导致数据和master节点相差过大的时候产生的。其过程是master节点后台开启线程将内存中最新的数据全量生成一份rdb文件，同时这段时间内写入命令也缓存在内存中。slave收到rdb文件之后先持久化到自己磁盘，然后写入自己的缓存，然后master把前面内存中缓存的写命令发送给slave节点，slave再执行一遍写命令。这个过程叫做全量复制。\n注：\n若一段时间内master收到多个slave节点的sync命令（相对PSYNC增量同步，SYNC代表请求全量复制），那么master只会生成一份rdb快照文件，不会重复生成。 2.8开始后支持断点续传，master和slave都会保存一个replica offset,用于记录断点的位置，所以如果短时间slave重连不会请求全量复制，默认发送PSYNC命令，请求增量复制。 支持无磁盘化复制，开启master将在内存中生成rdb文件。涉及的参数是repl-diskless-sync no（默认no，不使用diskless同步方式 ），repl-diskless-sync-delay 5（默认5秒，等待时间内多个slave连接的话都传这一个rdb）。 （2）异步复制 异步复制指的是后续节点保持同步是通过异步复制完成的，master节点收到写命令，写完成后返回客户端写成功，然后异步的复制给slave节点。\n3.哨兵架构 要保证主从架构的高可用必须配置哨兵集群，故障转移是通过哨兵架构来完成的，哨兵是不同的进程，也是一个集群。下面主要介绍下哨兵架构。\n（1）哨兵集群通信 其内部通信机制基于Redis的发布订阅，一个哨兵集群会订阅redis同一个channel。通过同一个channel交换信息。\n（2）sdown和odown sdown-主观宕机，描述的master节点状态被部分哨兵节点认为宕机，但是没有真正触发到主从切换的条件。\nodown-客观宕机，描述的是master节点状态触发了主从切换的条件。\n注：当哨兵节点Ping master节点如果延迟超过了is-master-down-agter-millseconds指定毫秒数之后，该节点就认为Master节点宕机\n（2）主从切换的条件 配置文件的quorum参数，quorum配置的是执行主从切换需要多少个哨兵节点认为master节点宕机。\n（3）主从切换的过程 当判断master客观宕机后，如果哨兵在线节点超过majority数量（哨兵集群的半数或半数以上），则会选举一个哨兵节点出来执行主从切换，这是这个哨兵节点又会根据slave节点跟master断开连接的时长、slave配置的优先级、复制offset、run id（slave节点的唯一标识）来进行筛选和排序，选举一个最优的slave节点出来继任master。其中断开连接时长如果超过down-after-millseconds（允许master正常延迟时间极限）的10倍加上millseconds_since_master_is_in_sdown_state（master宕机时长）的话，就会排除选举范围之外，剩下的依次根据slave priority（slave配置的优先级）、再根据slae offset排序（越靠后越高）、再根据run id排序（越小优先级越高）来进行排序。为了保证短时间内多次切换，其他哨兵节点不知道master节点时哪个节点，每次执行主从切换时，哨兵会从要切换的从节点获取一个版本号，成功后将版本号发布到所有哨兵节点监听的channel上，其他哨兵依次更新master节点的信息。\n（4）哨兵集群的高可用性 由于哨兵和redis机器是分不开的，哨兵集群作为redis高可用保证，自身也应该保证是高可用的。所以一般至少需要3个哨兵节点，因为如果只有2个哨兵节点，挂的master节点的机器上又有哨兵进程，那么这时哨兵节点就只剩下两个，而2的majority数又是2，这时是无法完成主备切换的。\n4.主从+哨兵的数据丢失问题 （1）场景 异步复制导致的数据丢失，master在正在准备异步向slave节点同步数据的情况下宕机，slave节点被选举为master,会丢掉没有完成异步复制的数据。 集群脑裂导致的数据丢失，由于网络分区故障，master与哨兵断了联系，但和客户端又保持着联系，导致新的Master节点被选举，当网络分区故障恢复后，新的master往原来的Master节点同步数据，导致在故障期间原来的master节点收到的数据被丢失。 （2）解决方案 一是需要注意两个参数，一个是min-slaves-to-write(默认1)，表示slave节点最少要保证有多少个存活，min-slaves-max-lag(默认10)，表示所有slave节点延迟不能同时超过10秒。当这两个参数不满足的时候，master节点就会拒绝客户端的写请求，以此控制丢失的数据量不会太高。 二是客户端要进行对应的降级处理，如写到本地数据库或发送到消息队列中，然后每隔一段时间重试连接master，当master节点恢复后，在限流的往master写数据，避免流量涌入过多超过master节点写负载。 （二）redis cluster集群架构 redis cluster集群架构与主从架构+哨兵模式的区别是，redis cluster集群架构针对的是大数据量场景，其可以横向扩展多个master节点，但是redis官方默认不支持redis cluster的读写分离，其slave只用于故障转移。但可以通过readonly命令，将slave设置成可读，然后通过slave获取相关的key，达到读写分离。如果在redis cluster中强行采用读写分离可能导致读到过期的数据，在Java的Redis操作类库中，如Jedis和Lettuce都默认只针对Master进行读写，若需要修改Jedis则只能做源码级别的改动，Letture修改相比较简单一些，开放了setReadFrom方法，可以通过connection.setReadFrom(ReadFrom.SLAVE)指定读往slave节点读。\n1.节点结构 多master节点，多slave。\n2.数据分布算法 数据分布算法一般有三种常用算法：\n普通的hash取模算法。即通过key的hash值再与master节点数量取模，模为多少就写入或读取哪个master节点。但这种方法有个缺点就是，如果有一个节点失效，master节点数量被改变，我们如果要取某个数据，计算的模打到的节点就与之前的节点不一样了，这就导致以前写入的节点基本上全部失效，必须通过大量的计算重新取模在把数据重新存入剩下的节点。 一致性hash算法。一致性hash算法主要是将hash值的整个空间组成一个圆环，每个节点均匀排列在一个圆环上，一个key对应的节点时通过计算Key值的hash值然后判断在圆环的哪个位置，顺时针找到的第一个节点就是我们写入或读取的节点。这种算法可以解决如果一个节点失效，则失效的数据只是该节点的数据，但该失效节点的负载数据请求相当于全部打在了顺时针下一个节点上，优化策略是在圆环上均匀分布大量的虚拟节点替代原本的真实节点，每个真实节点用相同数量的虚拟节点用来表示。由于虚拟节点顺序也是随机的，所以如果失效了某个节点，那么这个节点原本所承载的hash值空间也会均匀分布到剩下的节点当中，这个优化可以使得失效节点的数据请求均匀分布到剩下的节点中。 hashSlot算法。redis采用的就是hashSlot算法,每个节点均匀接管0-16384的模（称为hashslot），然后key来了之后，先判断与16384的模是多少，获取到模（hashSlot）之后，判断hashSlot是被哪个节点接管，再寻找该节点写入或读取，这能保证当某个节点宕机之后，失效的只是该节点接管的hashslot对应的数据。redis会自动将该失效节点接管的hashslot均匀分配到剩下的节点，所以这也能保证该失效节点原本负载的请求量均匀分配到剩下节点。 3.节点间通信机制 在集群内部存在的元数据包括hashslot和Node的映射表、master和slave关系、、故障信息、节点的增加和移除等等是如何通信的勒？redis使用的是Gossip协议来通信维护这些元数据，每个节点将开放两个端口，一个端口是面向客户端提供服务的端口，另一个端口端口号是服务端口+10000集群内部节点通信的端口，称为cluter bus集群总线。Gossip将消息分为4种类型：\nmeet，某个节点发送一个goosip meet消息，通知哪个节点加入集群 ping，每个节点都会频繁的给其他节点发送Ping,其中包含自己的状态还有自己维护的集群元数据，互相通过Ping交换元数据进行更新。 pong, 返回Ping和meet，包含自己的状态和其他信息，也可用于信息广播和更新 fail, 某个节点判断另一个节点fail之后，就发送fail给其他节点，通知其他节点，指定的节点宕机了 4.redisCluster的主备切换 redisCluster的主备切换类似哨兵模式，其主要流程是：\n判断节点宕机。对应哨兵模式的客观宕机，SDown在RedisCluster称为PFail,主观宕机ODown称为Fail。在配置的cluster-node-timeout时间内,某个节点一直没有返回Pone,将会被发送ping消息的节点认为是Pfail。如果一个节点认为某个节点Pfail了，那么会在集群总线中，ping其他节点，如果超过半数的节点都认为pfail了，那么就会变成fail，此时将进行主备切换。 从节点过滤。若slave节点与master节点断开连接时间超过了cluster-node-timeout 乘以 cluster-slave-validity-factor，就会被排除选举资格。 从节点选举。每个master，根据slave的offset指定slave的选举时间，选举时间越靠前，offset最大的slave节点优先开始投票让其他master节点选举，然后获得大部分投票的从节点将成为新的Master节点（投票策略应该也是判断offset）。 5.Jedis类库与RedisCluster交互的模式 使用原生命令与Redis集群交互主要一种是随意连接一个节点，在写入数据时，由该节点计算key的hashSlot值，如果该节点不是对应key的处理节点，该请求将重定向到对应节点。另外，在命令上可以加入hashtag手动指定hashslot，如：set myke1:{hash tag} 值，这样免除了节点实例计算hashSlot.\n在Jedis框架里本地维护一个hashslot映射表缓存，工作流程失在rediscluster初始化的时候随机找一个节点初始化hashslot映射表到本地，后面发送请求的时候根据映射表寻找节点，如果节点返回Moved(表示进行了重定向)，就以该节点元数据再更新一下自己的缓存。另外，如果hashslot正在迁移，将返回ask（表示hashslot正在迁移中），jedicluster会重定向到目标节点尝试请求，但Jedisscluster不会进行更新映射表。\n三、缓存方案-Cache Aside Pattern 指的是：\n读的时候，先读缓存，缓存没有的话，读数据库，取出数据后放入缓存，同时返回响应。\n更新的时候，先更新数据库，在删除缓存\n这里主要涉及两个问题，集中在更新的时候：\n第一是为什么是删除缓存而不是更新，因为如果更新缓存会破坏缓存意义，更新并不代表这个数据是热点访问数据，如果更新可能会破坏缓存的LRU特性，更新缓存应该以访问的动作为驱动。另外有时候数据库数据关联的缓存是经过一定计算才关联的，如果需要去更新缓存可能涉及的计算比较复杂。\n第二是为什么是先更新数据库，在删除缓存，因为这是为了避免缓存与数据库双写不一致，如：若修改数据数据库事务还没提交，但是已经把缓存从redis中删除，此时来了个读请求，会把旧的数据刷到缓存里面，这样就导致了缓存中的数据直到下一次修改数据库之前肯定是与数据库不一致的。\n四、redis缓存常见问题整理 （一）缓存与数据库双写不一致 1.场景 读写并发导致，读的时候没读到缓存，同时写的时候删除了缓存但还没来得及更新数据库，导致读的时候去更新缓存更新成了旧的数据。\n2.应对方案 先更新数据库，在删除缓存，同时用redis主从提高redis可用性，避免redis删除操作失败，另外如果是读写分离的情况可以用阿里的canel组件,监听从库的binlog，再去消费消息删除缓存。\n（二）缓存雪崩 1.场景 由于缓存宕机了，大量请求没有命中缓存，数据库接收大量请求压力\n2.应对方案 事前，缓存必须高可用，加哨兵或者cluseter下配置slave 事中，本地添加ehcahe（spingboot集成）缓存作为二级缓存，如果集成了hytrix组件则在前面两级缓存没命中之后加上限流+降级策略。 事后，开启redis的AOF和RDB,尽快恢复redis可用。 （三）缓存穿透 1.场景 大量恶意攻击（请求数据库没有的数据）请求，直接击穿了缓存，每次都去走数据库。\n2.应对方案 一是要做好事前的参数校验，高级一点就用布隆过滤器（相当于所有数据都用一定算法放在一个字典中，通过数据hash只判断在字典中存不存在）。 二是采取暴力一点的办法没查到就写一个空值到缓存里去（注意设置过期时间），这种有一定限制效果，因为恶意攻击方必须耗费精力准备足够大的参数字典来攻击。\n（四）缓存并发竞争 1.场景 短时间内针对一个key的更新并发，导致缓存更新没有保证顺序，由于顺序错乱，可能缓存的数据就错了。\n2.应对方案 使用分布式锁，可以基于zookeeper或redis自己，串行化某个key的更新操作，拿到锁之后，判断这把锁的时间戳（value）是否比自己的要新，新的话就不更新缓存了。反之将自己的时间戳打上这把锁，并更新缓存。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-08-13T00:00:00Z","permalink":"https://runningccode.github.io/2019/redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"Redis学习笔记"},{"content":"刚来公司时看到很多表都有一个valid_code字段，最开始还不懂是什么含义，问了同事才明白。\n比如一张业务表有id，code，外键code,state,valid_code。state有两个状态表示数据是否存在，删除就是修改这个字段。\nvalid_code我们的规则是如果是有效数据我们设为0，如果删除这条数据我们需要将valid_code置为一个随机数也好uuid也好都行。\n我们的背景是这张业务表只会关联这个外键所关联的实体的一次记录，并且用的是伪删除逻辑，通过改变state状态标志数据的删除或存在。\n需求是需要控制这张表的插入操作的幂等性，。\nvalid_code正是通过数据库的唯一索引机制来控制的，我们将外键code和valid_code构建一条唯一索引，这样就能保证如果valid_code相同，就只有一个关联的外键实体。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-08-13T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E5%85%B3%E4%BA%8E%E4%BC%AA%E5%88%A0%E9%99%A4%E7%9A%84%E8%A1%A8%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E4%BB%A5%E6%BB%A1%E8%B6%B3%E5%A4%96%E9%94%AE%E5%85%B3%E8%81%94%E5%8F%AA%E6%9C%89%E4%B8%80%E4%B8%AA%E4%BF%9D%E8%AF%81%E6%8F%92%E5%85%A5%E5%B9%82%E7%AD%89%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7/","title":"关于伪删除的表如何设计唯一索引以满足外键关联只有一个保证插入幂等的小技巧"},{"content":"一、前言 在分布式搜索引擎中，elasticsSearch逐渐变成一种标准了，其通过简单连贯的RESTful API让全文搜索变得简单并隐藏Lucene的复杂性。但底层还是使用Lucene来实现搜索功能。\n二、核心概念 index: 索引，是一类数据的抽象。 type: 类型，是一类数据的具体抽象。更多情况一个index只对应一个type，type类似数据库中的一张表，并且在逻辑定义上也经常是1对1的关系，如elasticsSearch的type中存订单数据需要被搜索的字段，并且有一个字段是订单号，我们通过字段搜索到订单号后通常会在数据库再查一次，返回详情。 document: 与Lucene里面的Document一样，就是表示可以被搜索的一条数据。 field：与Lucene里面的field一样，表示的是document的每个字段。 shard：elasticsSearch集群中存储数据的基本单位单位，一个索引有多个shard，在集群中不可以再次被分隔。 协调节点：集群中任意节点都可以接受客户端请求，接受请求的节点称为协调节点。 segmentFile: shard中数据持久化的磁盘文件，一个shard对应多个segmentFile。 fsync：Unix系统调用函数, 用来将内存缓冲区buffer中的数据存储到文件系统. 这里具体是指将文件缓存cache中的所有segment刷新到磁盘的操作。 三、基本原理 1.分布式策略 （1）数据分布 索引创建可以指定分片的数量以及副本的数量，分片数量在创建之后无法改变，副本数量在之后可以改变，随着集群中节点的增加与删除，各个分片与副本会重新分配到各个节点中。分片和副本不会分配到一个节点上，分片通过hash算法平均分布在各个节点上，也可以自定义分片分布规则（让在集群的某些节点和某个节点创建分片），如通过自定义分片分布规则实现冷热分离提高性能。因为这种分片机制，我们可以通过增加集群中节点保证一台机器的分片不会太多提高搜索性能。\n（2）高可用 集群中会自动选举一个master节点，master节点的主要作用是管理集群，维护索引元数据等。master挂掉，集群重新选举master节点，master节点然后切换节点的身份为master。\n（3）写和读 写请求被路由到只往primaryShard写，然后会自动同步到replicaShard，读的话primaryShard和replicaShard读都可以。\n2.基本原理 （1）写入过程 协调节点接收到写入请求，将写入请求数据通过哈希算法路由到对应的shard的primaryShard上去。primaryShard的节点接收到请求数据，首先把segment fiel以及transLog（事务日志）写入自己的应用内存buffer当中，然后默认每隔1s,将buffer中的数据refresh数据到osCache（文件系统缓存）中。此时客户端就能查询到数据了。这个过程非常快，因为并没有涉及到数据的持久化（所以是准实时的）。当translog文件过大或达到一定时间（默认30分钟）会触发flush操作，flush操作会将segmentfile统一flush到磁盘文件，同时生成一个commitpoint,记录生成的segmentfile，然后清空translog。\n注意：\n故障恢复时，elasticsSearch将根据当前的commitpoint文件加载segmentFile（恢复搜索功能），然后通过translog事务日志，重做所有操作来恢复数据。 当数据尚且在buffer或osCache、translog也在osCache中时可能会丢数据，也可设定参数保证数据不丢失，但会牺牲吞吐量和性能。Elasticsearch 2.0之后, 每次写请求(如index、delete、update、bulk等)完成时, 都会触发fsync将translog中的segment刷到磁盘, 然后才会返回200 OK的响应; （2）删除数据的过程 删除有点类似伪删除，它先是通过将对应删除的记录写入磁盘上的.del文件，标志那些document被删除（如果此时搜索将会搜索到这些文档但不会返回）。当segment File多到一定程度时候，ES将执行物理删除操作, 彻底清除这些文档。\n（3）修改数据的过程 修改数据是先删后增，将原来的数据标志位deleted状态，然后新写入一个document。\n（4）读数据的过程（传入document的id） 通过document 的id hash到指定分片，然后根据负载均衡算法（默认轮询），路由到该分片节点之一读取数据。\n（5）搜索数据的过程 协调节点，把请求发送到所有拥有该索引的节点上去，但是对于主parimaryShard和replicaShard只会查其中之一，每个shard把查询结果的docId返回给协调节点。接着协调节点根据docId去实际存放数据的节点拉取docment，由协调节点进行合并、排序、分页等操作，然后返回给客户端。\n四、如何性能优化 1.提高osCache覆盖率 elasticsSearch的高性能很大程度依赖于osCache的大小，毕竟走内存肯定比走硬盘快，所以可以提高filesystemCache的大小尽可能覆盖多的segment文件来提高性能。\n2.数据预热 做一个子系统，每隔一段对热点数据搜索一下。因为osCache实际上还是基于LRU缓存的。\n3.冷热分离 将热数据专门写一个索引，冷数据又单独写个索引，通过控制分片规则分放在不同的机器，因为热数据数据量少，没有冷数据的话，可以保证尽可能多的数据都在osCache里面，而因为冷数据不走热数据节点，避免oscache频繁切换数据的开销。\n4.模型设计 写入es模型的就完成Type之间的关联，建立冗余字段（别在es中join）,因为如果在搜索中运用到了索引之间的关联效率是很低的。\n5.避免深度分页 假设查询100页，会有1-100页的数据到协调节点来，然后协调节点才完成排序、筛选、分页，这是深度分页。应对方案有两种，一是我们的系统设计不允许翻那么深的页，或默认翻的越深，性能越差。二是利用elasticsSearch的ScrollAPI，ScrollAPI允许我们做一个初始阶段搜索并且持续批量从Elasticsearch里拉取结果直到没有结果剩下，缺点是只能一页一页往后翻，不能跳着翻。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-08-06T00:00:00Z","permalink":"https://runningccode.github.io/2019/elasticssearch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"elasticsSearch学习笔记"},{"content":"一、市面上流行的消息队列对比 ActivityMQ activityMQ是老牌的消息队列，技术相对成熟，但吞吐量一般，目前行业趋势渐渐用的少了，社区也相比其他mq不够活跃。 RabbitMQ，rabbitMQ是基于erlang语言开发的，国内中小企业比较流行，功能完备，特别值得一提的是管理后台界面足够人性化，功能丰富。并且社区活跃度较高，吞吐量还行。 RocketMQ 吞吐量比RabbitMq高，阿里开发，基于java语言，功能强大。 kafkaMQ 吞吐量比rocketMQ还要高，架构足够轻，易于扩展，是大数据实时计算、日志采集领域使用消息中间件的标准，但功能较少，并没有\u0026quot;事务性\u0026quot;\u0026ldquo;消息传输担保(消息确认机制)\u0026ldquo;\u0026ldquo;消息分组\u0026quot;等企业级特性。 总结看来，技术选型中，activityMQ一般不推荐，中小型公司用rabbitMQ（功能完备，使用较简单，有很好的后台界面）。 rocketMQ适合大型公司，大数据领域适合用kafka.\n二、为什么使用消息队列 1. 解耦 相比调用而言，消息队列有发布订阅模式（PUB/SUB），原来场景中的调用方只需要发送消息，不用考虑谁消费，不用手动维护调用关系，如果需要数据的系统太多维护起来是很复杂的。\n2. 异步 消息队列常用于异步场景，以此减少响应时间。\n3. 削峰 应用消息队列的拉模式能够很好的缓解系统在高峰期的压力，消费者保持自己能够接收限度去拉取消息。\n三、消息队列可能导致的问题 1.可用性问题（MQ崩溃则整个系统崩溃） 以rabbitMQ为例，rabbitMQ有三种模式：单机模式、普通集群模式、镜像集群模式。\n单机模式 只有一个mq服务节点，没有可用性可言\n普通集群模式 一个queue不会存多个副本在每个节点，真实数据只保存在一个节点上，但其他节点会保存该queue的元数据元（存储队列指针，长度等），消费者可以通过集群中的任何节点获取到数据（访问的节点通过元数据取到实际存储队列的节点的数据然后返回给消费者）。但真实数据只在一个节点中，如果配置了持久化，那么实际保存数据的节点蹦了，其他节点不能创建同样的队列，所以不符合高可用性。这种模式优点只是在于分散mq的cpu,内存压力，提高数据存储空间。但也会存在很多集群内部各个节点为了传输消息的网络IO,同时可用性没有保障。\n镜像集群模式 相对普通集群模式，所有节点同步保存队列的真实数据。管理控制台可以配置一个镜像集群策略，指定所有节点或指定数量节点同步节点队列。缺点是降低了系统性能，节点每次同步数据的网络开销大。master提供对外服务，slave节点只提供备份服务，需要注意的是，并不是一个节点就是所有队列的master节点。谈论master和slave是针对队列来谈论的。\n以Kafka为例：\n每个topic有多个partion，每个partion分布于不同的节点，而且partion有副本存在于其他节点，每个partion有leader和follow，读写只存在于leader，follow会自动同步leader的数据，如果leader挂了，将重新选举一个leader。\n2.消息被重复消费 rabbitMq场景：消费确认机制，如果设定了手动提交，消费者收到消息突然蹦了，导致ACK状态码未反馈至MQ\nkafka场景：消费者会定期执行消费到的offset值提交给zookeeper用于给kafka节点从哪个位置发送消息为依据，但是由于极端原因如重启导致没有消费了消息但是没有提交，kafka任会按照上一次offset的位置发送消息给消费者，这就导致了重复消费\n另外一种场景是：原本是想发送广播消息到多个消费者服务中，但是一个服务实例部署了多台，多台重复消费了。\n解决方案：设计消费方法为幂等的。这个思路可以是在redis存储消费过后的一个标志,下次消费就先判断该标志存不存在，存在则不进行操作。或者基于数据库唯一键实现，报错就报错。\n3.消息积压 场景：消费端故障导致消息积压\n解决方案:\n快速排查好消费端故障 改造原来的消费端，写到别的服务器的队列中，然后临时多开几个消费端按照原有逻辑去消费这些队列的数据 如果还设置了过期失效（一般情况下不会这么设置），部分数据丢了怎么办,手动写程序把丢掉的数据查出来再手动发到消息队列中去 4.消息丢失 以rabbitMQ为例，消息丢失可以分为： （1）生产者丢消息 主要是因为，写消息的时候由于网络原因没有到rabbitmq服务器。\n解决方案：\n1.一是可以开启rabbitmq的事务模式\n一次事务交互主要有以下环节：\n客户端发送给服务器Tx.Select(开启事务模式)\n服务器端返回Tx.Select-Ok（开启事务模式ok）\n推送消息\n客户端发送给事务提交Tx.Commit\n服务器端返回Tx.Commit-Ok\n以上就完成了事务的交互流程，如果其中任意一个环节出现问题，就会抛出IoException移除，这样用户就可以拦截异常进行事务回滚，或决定要不要重复消息。但是这样的缺点相比下面说的confirm机制，事务是同步的，吞吐量会低一点。\n2.二是可以用confirm机制\n流程是：\n先把channel设置为confirm模式，发送消息，发完就不管了\n生产者提供一个接口，用于实现成功/失败回调后的方法\n接收失败的话可以直接再重发一次\n（2）MQ丢消息 MQ丢消息的情况可能是：rabbitmq接收到消息，在消费者消费之前挂掉了。\n解决方案（两步）：\n1.设置队列元数据持久化，设为durable；\n2.生产者设置消息持久化，delivery_mode=2。\n另外：\n若设置了持久化且开启了confirm，将在持久化之后才回调生产者。\n若持久化过程中宕机，还是会丢失数据，除非结合confirm机制。\n（3）消费者丢消息 消费者打开了autoAck机制（消费者收到消息自动响应mq消费到了数据），如果正在消费时宕机了，就丢消息了。\n解决方案：关闭ack改为手动ack（如果要在意最终数据一致性，最好是在数据库本地事务之后手动发送ack），如果mq没收到ack，mq将把消息发送给其他消费者。\n以kafka为例： (1) 生产者丢消息 如果设置了ack=all,一定不会丢，同时设置了retries=max，则会无限重试\n(2) MQ丢消息 场景：有可能内存还没同步到日志文件或者没有同步到Follower但是自己down掉了。\n参考解决方案：\ntopic设置repication.factor参数，必须大于1，要求每个Partition必须至少有2个副本。\nkafka服务端设置min.insync.replicas大于1，要求一个leader至少需要感知到一个follower还跟自己保持联系。\n生产者设置acks=all,要求每条数据必须是写入所有replica之后，才能认为是写成功了。\n在producer端设置retries=max（很大的值），要求一旦写入失败，就无线重试。\n（3）消费者丢消息 消费者自动提交了offset，但是处理业务中却异常宕机。取消offset，改为手动提交。\n3.消息顺序变了 消息顺序有时候会影响业务，比如先修改后删除的两条消息，不按顺序来消费就会出错。\n从rabbitMQ上来说，需要保证顺序的队列避免多个消费者同时消费（避免工作队列，只有一个消费者去消费）。\n从kafka上来说，多个消费者同时消费的消息指定到了不同的key分发到了不同的partion导致不同消费者同时消费。因为kafka的一个partion只允许一个消费者，所以只需要把保证顺序的消息发送到同一个pation，让其被同一个消费者消费就行了。\n另外也有可能是消费者自己并发的去消费信息造成的，针对这种情况，我们可以通过对消息的唯一标识进行hash算法分派到不同的内存队列，然后线程只取其中一个内存队列的消息进行消费来控制相关联的消息顺序不会乱。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-07-30T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"消息队列学习笔记"},{"content":"需要自定义Jackson序列化和反序列化有两种方式，一种是全局定义，一种是非全局定义。先来看看全局定义。全局定义的步骤如下，以定义一个localDateTime的序列化和反序列化为例：\n一、创建序列化类 创建一个序列化类然后继承JsonSerializer，重写serialize序列化方法。其中第一个参数localDateTime为JsonSerializer的泛型，表示的是被序列化的类型的值，第二个参数jsonGenerator表示的是用于输出生成的Json内容，第三个参数暂时没明白什么应用场景。重写方法一般是将想要序列化的字符串传入 jsonGenerator.writeString。\npublic final class LocalDateTimeSerializer extends JsonSerializer\u0026lt;LocalDateTime\u0026gt; { public static final LocalDateTimeSerializer INSTANCE = new LocalDateTimeSerializer(); public LocalDateTimeSerializer() { } @Override public void serialize(LocalDateTime localDateTime, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException, JsonProcessingException { jsonGenerator.writeString(DateUtil.format(localDateTime, DateUtil.DateTimeFormatEnum.DATE_TIME_FORMAT_4)); } } 二、创建反序列化类 创建两个类，一个类继承JsonDeserializer，一个类继承KeyDeserializer,重写deserialize反序列化方法。参数jsonParser用于读取json内容的解析，deserializationContext可用于访问此有关反序列化的上下文（暂时也不知道怎么用），返回值则是JsonDeserializer的泛型对象，表示要反序列化的对象。一般用法是通过jsonParser.getText().trim()获取该字段json字符串，然后将该字符串转换为对象返回。\npublic final class LocalTimeDeserializer extends JsonDeserializer\u0026lt;LocalTime\u0026gt; { public static final LocalTimeDeserializer INSTANCE = new LocalTimeDeserializer(); public LocalTimeDeserializer() { } @Override public LocalTime deserialize(JsonParser jsonParser, DeserializationContext deserializationContext) throws IOException, JsonProcessingException { String text = jsonParser.getText().trim(); return LocalTime.parse(text, DateUtil.DATE_TIME_FORMATTER_6); } } public final class LocalDateTimeKeyDeserializer extends KeyDeserializer { public static final LocalDateTimeKeyDeserializer INSTANCE = new LocalDateTimeKeyDeserializer(); public LocalDateTimeKeyDeserializer() { } @Override public Object deserializeKey(String s, DeserializationContext deserializationContext) throws IOException, JsonProcessingException { return StringUtils.isBlank(s) ? null : LocalDateTime.parse(s, DateUtil.DATE_TIME_FORMATTER_4); } } 三、将两个类注册进入jackson核心对象objectMapper @Bean public ObjectMapper objectMapper(){ ObjectMapper objectMapper = new ObjectMapper(); objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); //不注释,会导致swagger报错 //objectMapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); //关闭日期序列化为时间戳的功能 objectMapper.disable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS); //关闭序列化的时候没有为属性找到getter方法,报错 objectMapper.disable(SerializationFeature.FAIL_ON_EMPTY_BEANS); //关闭反序列化的时候，没有找到属性的setter报错 objectMapper.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES); //序列化的时候序列对象的所有属性 objectMapper.setSerializationInclusion(JsonInclude.Include.ALWAYS); //反序列化的时候如果多了其他属性,不抛出异常 objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false); //如果是空对象的时候,不抛异常 objectMapper.configure(SerializationFeature.FAIL_ON_EMPTY_BEANS, false); SimpleModule simpleModule = new SimpleModule(); //json值序列化 simpleModule.addSerializer(LocalDateTime.class, LocalDateTimeSerializer.INSTANCE); //json值反序列化 simpleModule.addDeserializer(LocalDateTime.class, LocalDateTimeDeserializer.INSTANCE); //json键序列化 simpleModule.addKeySerializer(LocalDateTime.class,LocalDateTimeSerializer.INSTANCE); //json键反序列化 simpleModule.addKeyDeserializer(LocalDateTime.class, LocalDateTimeKeyDeserializer.INSTANCE); objectMapper.registerModule(simpleModule); return objectMapper; } 四、总结 以上，通过objectMapper的配置，完成了全局序列化、反序列化的配置，如果不需要全局则通过@jsonserialize或 @JsonDeserialize指定使用的序列化、反序列化类。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-07-27T00:00:00Z","permalink":"https://runningccode.github.io/2019/jackson%E8%87%AA%E5%AE%9A%E4%B9%89%E5%85%A8%E5%B1%80%E5%BA%8F%E5%88%97%E5%8C%96%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/","title":"jackson自定义全局序列化、反序列化"},{"content":"今天无意中发现了一款分布式事务框架，LCN,看了下网上介绍感觉挺强大的。这里做一下笔记。\n一、环境基础 LCN框架基础需要一台服务器作为事务管理器TxManager，此外还需要Redis和Eureka做配合，eureka负责为TxManager注册，redis主要是用于TxManager存放事务组和补偿的信息。\n二、一个事务所涉及的角色 事务发起方服务 TxManager事务管理器 事务下游服务（可能有多个） 三、原理 TxManager是基于本地事务的，这里有一个事务组的概念，事务组是所有涉及的微服务的本地事务的集合。\n它主要原理是对通过重写dataSource的close方法，本身的close方法是需要关闭本地事务的，但是重写后并没有关闭事务，而是把事务信息记录在txmanager的redis中，等待事务发起方服务业务执行完成或异常的时候再发通知给各个服务通知本地事务提交或回滚的。在代理连接池中，并不是都不真实提交事务，它可以自动识别连接的读或写操作，如果全是读操作，那么将返回本地连接对象。如果该方法被重复执行，连接也可以被重用，同时也有超时限制，参与模块等待通知超时会自动提交或者回滚（这里具体不清楚到底是提交还是回滚，暂时存疑，不过看后面的补偿机制说明，好像是自动提交？）\nTxManager类似于二阶段提交，只不过二阶段提交的事务发起方和事务管理器在同一台机器，而TxManager是作为独立的中间件。\n四、事务补偿 事务补偿指的是当事务发起方服务异常或正常执行的时候发送事务组关闭的请求到TxManager，让TxManager通知下游服务回滚或提交事务，二阶段模型中这一步没有解决结束事务操作能否正确提交到资源管理端的问题，在LCN中提供了一种自动补偿机制。\n首先看一看TxManager后台页面：\nimg.png\r这里，有两项值得注意，第一项是补偿回调地址，这正是在TxManager发送通知下游服务回滚或提交事务失败的时候回调事务信息给事务发起方服务的回调地址 第二项是是否开启自动补偿。 不开启补偿的话，TxManager还是会回调。 自动补偿是怎么实现的勒？首先TxManager回调事务发起方服务（携带了事务信息，切面拦截信息），那么事务发起方在回调方法中就写一个重复业务的操作，这个操作中还是会模拟上次的请求。但是针对已经上次通知成功commit的服务，这次就需要回滚了，只有通知失败的服务需要commit。\n五、小结 git地址：https://github.com/syzpig/tx-lcn\ndemo地址：https://github.com/codingapi/springcloud-lcn-demo\n官方文档地址：https://txlcn.org/zh-cn/docs/preface.html\nLcn支持springcloud和dubbo。拉了springcloud的demo代码下来看，看了使用方法，环境搭好以后只需要在事务发起方法里使用@TxTransaction(isStart = true)注解，然后在下游服务方法中使用@TxTransaction就行了，使用方法很简单，代码侵入很低，值得推荐。\n另外，提一点，对于消息队列异步调用的形式，这款框架并不能满足事务一致性，需要结合其他方案。\n在最新版本的Lcn中发现也支持TCC和TXC模式~~~~~~~~\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-07-27T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E5%88%9D%E8%AF%86lcn%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%A1%86%E6%9E%B6/","title":"初识LCN分布式事务框架"},{"content":"一、基本知识概念 （一）数据库事务4大特性：ACID 数据库事务需要实现下面4大特性：\nA-Atomicity-原子性，指的是事务要么成功提交，要么失败回滚 C-Consistency-一致性，指事务操作前后，数据库的完整性不会产生变化，因为事务不可能执行一半，导致数据库内容不正确 I-Isolation-隔离性 事务不能互相影响，简单来说就是事务的中间状态不应该被其他事务察觉 D-Durability-持久性 一旦事务提交则会永久保存在数据库中 其中，只有隔离性根据数据库不同的隔离级别有不同的标准，其他三点都是数据库事务必须具备的。\n（二）分布式事务 单机事务机制可以有数据库本地事务实现良好的控制，但分布式事务由于涉及多个数据源。这种情况主要有两种：\n一种是分库分表导致了一台服务器多数据源，这种情况可以通过二阶段提交实现简单的事务控制。 一种是应用的微服务化，一个请求由多个服务协作完成，而多个服务又配置了不同的数据源。这种情况如果是支付等严格要求数据一致性的场景可以用TCC事务模型控制，如果是要求并发性能，但对数据一致性要求不高可以用最终一致性方案控制。 在介绍事务控制模型之前先介绍一下针对分布式事务，我们需要关注的指标:\n1.CAP理论 C-Consistency -一致性，一个节点数据更新，其他节点保存的数据也应该同步更新 A-Availability-可用性，节点收到请求，合理时间内必须返回基于请求合理的响应 P-Partition tolerance-分区容错性 能够允许部分节点网络故障 CAP理论表示这三个指标必然不能同时满足，最多只能满足其中两个。这里简单推论一下： 同时满足这三个指标的情况相当于承认当一个节点无法与其他节点通信的时候，还能保证在合理时间返回与其他节点一致的合理响应。这显然是不可能的。由此可以得出3个指标不能同时满足。\n因为P是分布式的基础，我们设计分布式系统不可能不考虑P，如果不考虑P那么这个分布式系统是极其不稳定的，所以针对CAP理论，一般选择满足CP或AP。\n选择CP意味着当出现部分节点通信挂了，为了一致性必须阻塞请求，等待数据同步。 选择AP意味着当出现部分节点通信挂了，为了可用性，针对请求，节点只能返回一个不能保证一致性的返回给请求方。 顺带一提，基于此ZooKeeper集群是注重的是CP，当50%的节点挂了之后，那服务发现功能也就挂了。EureKa集群注重CA，注册中心只要有一台能够使用，则不会影响正常的服务发现，另外客户端还会缓存服务、消费者数据。\n2.BASE理论 BASE理论算是对CAP理论的一种指导性意见。\nBA-Basically Available-基本可用，分布式系统出现不可预料故障的时候，要尽量将故障减小到不影响核心功能。 S-Soft state-软状态，允许部分节点的数据存在一定的延时，这个延时不影响可用性，这是对强一致的一种妥协，因为真正的强一致是所有节点数据时时刻刻都保持一致，而CAP理论要求的C忽略了网络通信延迟。 E-Eventually consistent -最终一致性，最终一致是指经过一段时间后，所有节点数据都将会达到一致。 总体上来说BASE理论强调的是用最终一致代替了CAP理论的强一致，用基本可用代替了CAP理论的完全可用。\n二、分布式事务解决方案模型 （一）二阶段提交-2PC- XA Transactions 1.角色 在二阶段提交中涉及的角色有两个：\n事务管理器，对资源管理器的事务调度者，可能是一个组件，也可以是一个中间件。 本地资源管理器，一般指的是数据库服务，如Oracle，Mysql。 2.应用场景 本地资源管理器一致性的管理。\n3.过程 第一阶段，事务管理器发送preCommit消息每个源管理器事务并执行业务逻辑，并阻塞等待每个数据源undo，redo日志写入完毕，就差提交 第二阶段，如果每个数据源都没有发生异常，表示可以提交，则事务管理器发送commit给资源管理器所有数据库的事务，如果有数据源异常了，则回滚所有事务。 强调一点，二阶段提交协议只是事务管理器和本地资源管理器之间的协议，没有明确规定事务管理器的角色，可能在单点服务器多个数据源中事务管理器就是一个组件，用于控制多个数据源的事务，可能在微服务中又是一个中间件。 5.缺点 缺点主要有：\n单点问题,如果事务管理器宕机，资源管理器将阻塞并且锁住资源 同步阻塞,准备就绪到事务管理器发送请求这段时间对于资源管理器来说是不必要的阻塞与占用资源时间(资源管理器通过锁机制保证隔离性)，对于性能上是一种浪费，在微服务链路中时间中阻塞可能耗时更长。 脑裂问题，没有完全解决数据不一致，不能保证事务管理器发出的commit命令操作正确到达本地资源管理器，如果与某个本地资源管理器通信失败了，事务管理器是无法察觉的，结果还是不一致的。 （二）三阶段提交协议 跟二阶段提交协议差不多，只不过在PreCommit之前引入了CanCommit阶段，该阶段不做任何实质性操作，只是测试事务管理器与资源管理器之间的通信是否异常。同时引入超时机制，如果本地事务管理器没有收到第三阶段的消息，默认是事务管理器宕机，自动commit本地事务。（因为通过第一阶段的测试，大概率认定所有资源管理器都收到preCommit消息，此时自动提交大概率不会出错）。\n（三）TCC模型 1.角色 涉及的角色有微服务应用，不同数据源，同一个TCC管理框架，之所以要引用TCC管理框架的原因在于对于Try操作失败成功的感知，如果涉及微服务调用链是极其复杂的。\n1.应用场景 微服务应用，不同数据源，不要求高并发，一致性要求较高。\n2.过程 (1) 第一阶段-Try 根服务调用各个服务的Try方法，各个服务根据业务锁定资源-业务层面上的，没有引入数据库事务，就是将业务涉及的资源存入各个服务的本地方便之后回滚的时候恢复，然后返回给根服务是否执行成功。\n(2) 第二阶段-Confirm或Cancel Confirm,根服务获取各个服务try是否执行成功，若成功则调用各个服务的Confirm方法，confirm开始把锁定的资源正式刷到业务数据中，如果confirm不成功，则一直重试，务必保证confirm成功。 Cancel,如果各个服务的try有一个不成功，则调用各个服务的cancel方法，释放锁定的资源，如果调用不成功，也会重试。 注意：Try阶段成功意味着数据库层面的操作大概率是没有问题的，后续的异常只会发生在极端情况下，所以这是tcc根据try操作，去不停重试confirm或cancel的原因。\n3.缺点 confirm和cancel方法因为有可能被重复调用，所以得从代码上保证幂等，对代码侵入性有点强，复杂度搞。 由于整个TCC过程需要锁定资源所以对高并发场景不太友好。 （四）本地消息表方案模型 1.角色 上游根服务 MQ消息中间件 下游服务 2.思路 不关注MQ丢不丢消息，只关注根服务的本地事务，如果本地事务执行成功，那么记录所需发送的消息表也被插入成功。\n此时唯一需要关注的只是消息能够被消费者消费掉，用轮询本地消息表的方式重复发送消息保证最终消息一致性即可。\n3.具体方案 根服务完成事务后，将需要与其他服务协调的请求消息存入本地消息表并将状态置为待确认。然后有一个后台线程定时轮询消息表，将待确认消息推送给MQ，MQ再推送给消费者，消费者收到消息后，再消费消息。当消费完消息后，消费者通过一定方式如zookepper或接口回调根服务，然后根服务修改消息表状态为已完成。\n3.缺点 强依赖于本地的消息表，数据库压力较大。\n（五）最终一致性方案模型 1.角色 上游根服务 可靠消息服务 MQ消息中间件 下游服务（可能有多个） 2.应用场景 主要应用于不要求强一致，并发要求度高，微服务异步调用场景中。\n3.思路 这个模型主要关注三点：\n在根服务本地事务完成和失败的情况下，确保消息能够正确投递到消费者或取消投递。 在下游服务本地事务失败的情况下，确保消息能够被重发。 不关注各个优秀MQ框架消息可靠性的保证，即默认消息能够被丢失。 为了解决上述问题，引入了可靠消息服务，用以协调上游服务的事务状态和下游事务状态。通过维护消息的三种状态：待确认、已发送、已成功，确保事务的最终一致性。\n3.过程 (1) 上游根服务调用可靠消息服务。 上游根服务同步调用可靠消息服务，参数包含调用下游服务的参数，这些数据由可靠消息服务存入自己的表中，并将这条记录的状态置为待确认。\n(2) 上游根服务执行完自己的业务之后在再次调用可靠消息服务。 上游根服务执行完自己的业务之后，将业务执行结果传入可靠消息服务，可靠消息服务根据业务失败结果，若失败则删除消息表中关于这个业务的所有消息表记录，若成功则通过MQ发送消息给下游服务，然后更新刚刚存入的记录将状态置为已发送。注意可靠消息服务的这个方法中，判断业务成功失败，需安排在同一个本地事务（因为发送消息和更新记录必须保证原子性）。\n（3） 下游服务接收消息之后消费消息，执行业务完成后回调可靠消息服务。 下游服务接收消息之后消费消息，执行业务完成后回调可靠消息服务。可靠消息服务受到请求后，将本地记录状态置为已完成。\n4.注意 (1) 如何保证上游服务对可靠消息服务的100%可靠投递 第一次调用如果失败，这种情况上游根服务会收到可靠消息服务错误返回，这种情况上游根服务直接放弃流程。 第二次调用如果失败，这种情况可靠消息服务后台定时检查记录中为待确认消息的记录，如果待确认状态时间较长，则对上游服务发起请求，看上游服务是否完成了自己的业务逻辑。若完成，则自动更新记录状态，反之删除记录。（上游服务需要维护自己的事务执行状态在本地表中） (2) 如何保证下游服务对可靠消息服务的100%可靠投递 可靠消息服务定时检查记录中已发送状态维持时间超时的记录，然后重复发送请求给下游服务。所以这里同时也要求下游服务方法的幂等性。\n（五）另外几种想到的解决办法 使用RabbitMQ的消息确认机制和消费者开启手动ACK是能够确保消息一定被消费。唯一需要关注的问题在于如何处理本地事务失败、消息者成功消费造成的数据不一致的情况。\n对于这种情况，可以这么做：\n在发送消息的时候，绑定两个监听队列到一个交换机上。一个监听队列负责对这种情况进行补偿，补偿操作为判断本地事务是否成功，若不成功则进行补偿。一个监听队列负责完成下游服务。 三、小结 其实这些模型有很多都有相似的地方，也能做很多变通，很多通过回调、轮询、长连接监听、同步调用的方式其实都可以根据具体场景互相替换或选择不同的优秀架构。另外需要注意的就是，引入分布式事务是需要维护成本和性能成本的，很多情况下，需要根据业务来看需不需要引入分布式事务。不是很严格的情况，用监控、日志记录的方式采取人工补偿的策略有可能成本更小。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-07-27T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E6%A8%A1%E5%9E%8B/","title":"分布式事务相关概念及解决方案模型"},{"content":"最近在学springJpa，照着网上博客想试着配一下Jpa的多数据源，但发现因为springboot版本太高的问题，网上的demo都不适用，导致找了很久才找到解决办法。现在把操作过程记录如下。\n一、yml配置 spring: datasource: test1: driver-class-name: com.mysql.jdbc.Driver password: 123456 #url: jdbc:mysql://localhost:3306/test?useUnicode=true\u0026amp;characterEncoding=UTF-8\u0026amp;serverTimezone=UTC\u0026amp;useSSL=false #springboot2.0以上 jdbc-url: jdbc:mysql://localhost:3306/test?useUnicode=true\u0026amp;characterEncoding=UTF-8\u0026amp;serverTimezone=UTC\u0026amp;useSSL=false username: root test2: driver-class-name: com.mysql.jdbc.Driver password: 123456 #url: jdbc:mysql://localhost:3306/test2?useUnicode=true\u0026amp;characterEncoding=UTF-8\u0026amp;serverTimezone=UTC\u0026amp;useSSL=false #springboot2.0以上 jdbc-url: jdbc:mysql://localhost:3306/test2?useUnicode=true\u0026amp;characterEncoding=UTF-8\u0026amp;serverTimezone=UTC\u0026amp;useSSL=false username: root jpa: ## 是否打印sql show-sql: true properties: hibernate: # 指定引擎为Innodb dialect: org.hibernate.dialect.MySQL5InnoDBDialect hbm2ddl: # create： 每次加载 hibernate 时都会删除上一次的生成的表， # 然后根据你的 model 类再重新来生成新表，哪怕两次没有任何改变也要这样执行， # 这就是导致数据库表数据丢失的一个重要原因。 # create-drop ：每次加载 hibernate 时根据 model 类生成表，但是 sessionFactory 一关闭,表就自动删除。 # update：最常用的属性，第一次加载 hibernate 时根据 model 类会自动建立起表的结构（前提是先建立好数据库），以后加载 hibernate 时根据 model 类自动更新表结构，即使表结构改变了但表中的行仍然存在不会删除以前的行。要注意的是当部署到服务器后，表结构是不会被马上建立起来的，是要等 应用第一次运行起来后才会。 # validate ：每次加载 hibernate 时，验证创建数据库表结构，只会和数据库中的表进行比较，不会创建新表，但是会插入新值。 auto: update 二、注册datasource到spring容器 @Configuration public class DataSourceConfig { @Bean(name = \u0026quot;primaryDataSource\u0026quot;) @Primary @Qualifier(\u0026quot;primaryDataSource\u0026quot;) @ConfigurationProperties(prefix = \u0026quot;spring.datasource.test1\u0026quot;) public DataSource primaryDataSource() { System.out.println(\u0026quot;-------------------- primaryDataSource初始化 ---------------------\u0026quot;); return DataSourceBuilder.create().build(); } @Bean(name = \u0026quot;secondaryDataSource\u0026quot;) @Qualifier(\u0026quot;secondaryDataSource\u0026quot;) @ConfigurationProperties(prefix = \u0026quot;spring.datasource.test2\u0026quot;) public DataSource secondaryDataSource() { System.out.println(\u0026quot;-------------------- secondaryDataSource初始化---------------------\u0026quot;); return DataSourceBuilder.create().build(); } } 三、注册jpa相关对象进入spring容器 数据源1:\n@Configuration @EnableTransactionManagement @EnableJpaRepositories( entityManagerFactoryRef=\u0026quot;entityManagerFactoryPrimary\u0026quot;, transactionManagerRef=\u0026quot;transactionManagerPrimary\u0026quot;, basePackages= { \u0026quot;com.czcstudy.springbootdemo.day1.dao.test1\u0026quot; }) //设置Repository所在位置 public class RepositoryPrimaryConfig { @Autowired @Qualifier(\u0026quot;primaryDataSource\u0026quot;) private DataSource primaryDataSource; @Autowired private JpaProperties jpaProperties; @Autowired private HibernateProperties hibernateProperties; @Primary @Bean(name = \u0026quot;entityManagerFactoryPrimary\u0026quot;) public LocalContainerEntityManagerFactoryBean entityManagerFactoryPrimary( EntityManagerFactoryBuilder builder) { //网上文章大多数都是jpaProperties.getHibernateProperties(dataSource);就直接得到了hibernate的配置map， //但这个方法在springboot2.0+好像就舍弃了，所以这里改成这样。 Map\u0026lt;String, Object\u0026gt; properties = hibernateProperties.determineHibernateProperties( jpaProperties.getProperties(), new HibernateSettings()); return builder.dataSource(primaryDataSource).properties(properties) .packages(\u0026quot;com.czcstudy.springbootdemo.day1.bean.po\u0026quot;).build();//实体包路径 } @Primary @Bean(name = \u0026quot;transactionManagerPrimary\u0026quot;) public PlatformTransactionManager transactionManagerPrimary(EntityManagerFactoryBuilder builder) { return new JpaTransactionManager(entityManagerFactoryPrimary(builder).getObject()); } 数据源2：\n@Configuration @EnableTransactionManagement @EnableJpaRepositories( entityManagerFactoryRef=\u0026quot;entityManagerFactorySecondary\u0026quot;, transactionManagerRef=\u0026quot;transactionManagerSecondary\u0026quot;, basePackages= { \u0026quot;com.czcstudy.springbootdemo.day1.dao.test2\u0026quot; }) //设置Repository所在位置 public class RepositorySecondaryConfig { @Autowired @Qualifier(\u0026quot;secondaryDataSource\u0026quot;) private DataSource secondaryDataSource; @Autowired private JpaProperties jpaProperties; @Autowired private HibernateProperties hibernateProperties; @Bean(name = \u0026quot;entityManagerFactorySecondary\u0026quot;) public LocalContainerEntityManagerFactoryBean entityManagerFactorySecondary( EntityManagerFactoryBuilder builder) { //网上文章大多数都是jpaProperties.getHibernateProperties(dataSource);就直接得到了hibernate的配置map， //但这个方法在springboot2.0+好像就舍弃了，所以这里改成这样。 Map\u0026lt;String, Object\u0026gt; properties = hibernateProperties.determineHibernateProperties( jpaProperties.getProperties(), new HibernateSettings()); return builder.dataSource(secondaryDataSource).properties(properties) .packages(\u0026quot;com.czcstudy.springbootdemo.day1.bean.po\u0026quot;).build();//实体的包路径 } @Bean(name = \u0026quot;transactionManagerSecondary\u0026quot;) public PlatformTransactionManager transactionManagerPrimary(EntityManagerFactoryBuilder builder) { return new JpaTransactionManager(entityManagerFactorySecondary(builder).getObject()); } } 四、使用spring事务例 @Service public class JpaTestServiceImpl implements JpaTestService { @Autowired private UserJpaTest2Dao userRepository2; @Override @Transactional(value = \u0026quot;transactionManagerSecondary\u0026quot;,rollbackFor = RuntimeException.class) public void test(){ List\u0026lt;UserJpaTest\u0026gt; userJpaTestList = userRepository2.findAll(); System.out.println(userJpaTestList); } } 其中指定的value就是前面注册的PlatformTransactionManager对象名称，多数据源时需要指定。\n五、小结 以上就是springboot2.1.5 配置jpa多数据源的方法，启动项目我们可以看到\nimg.png\rHikariPool连接池已经启动了，这是springboot的默认数据库连接池，所以连接池我们这里就不自己配了。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-07-18T00:00:00Z","permalink":"https://runningccode.github.io/2019/springboot-2.1.5-%E9%85%8D%E7%BD%AEjpa%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90/","title":"Springboot 2.1.5 配置JPA多数据源"},{"content":"经常写一些业务代码，学会快速生成项目上业务代码所需的类entity、dao、service类对我们提高工作效率很有帮助，整理步骤如下：\n一、准备工作 在idea中连接数据库 下载idea的CodeMaker插件 二、生成实体类 准备生成实体类的groovy脚本,这里我直接用写好了的脚本，因为不懂groovy，只能是在脚本上猜着改改，但实体类生成都差不多，猜着改改勉强能改到满足自己要求，下面把脚本贴上：\nimport com.intellij.database.model.DasTable import com.intellij.database.model.ObjectKind import com.intellij.database.util.Case import com.intellij.database.util.DasUtil import java.io.* import java.text.SimpleDateFormat import java.lang.*; /* * Available context bindings: * SELECTION Iterable\u0026lt;DasObject\u0026gt; * PROJECT project * FILES files helper */ packageName = \u0026quot;\u0026quot; typeMapping = [ (~/(?i)bigint/) : \u0026quot;Long\u0026quot;, (~/(?i)int|tinyint|smallint|mediumint/) : \u0026quot;Integer\u0026quot;, (~/(?i)bool|bit/) : \u0026quot;Boolean\u0026quot;, (~/(?i)float|double|decimal|real/) : \u0026quot;Double\u0026quot;, (~/(?i)datetime|timestamp|date|time/) : \u0026quot;Date\u0026quot;, (~/(?i)blob|binary|bfile|clob|raw|image/): \u0026quot;InputStream\u0026quot;, (~/(?i)/) : \u0026quot;String\u0026quot; ] FILES.chooseDirectoryAndSave(\u0026quot;Choose directory\u0026quot;, \u0026quot;Choose where to store generated files\u0026quot;) { dir -\u0026gt; SELECTION.filter { it instanceof DasTable \u0026amp;\u0026amp; it.getKind() == ObjectKind.TABLE }.each { generate(it, dir) } } def generate(table, dir) { def className = javaName(table.getName(), true) def fields = calcFields(table) packageName = getPackageName(dir) PrintWriter printWriter = new PrintWriter(new OutputStreamWriter(new FileOutputStream(new File(dir, className + \u0026quot;.java\u0026quot;)), \u0026quot;UTF-8\u0026quot;)) printWriter.withPrintWriter {out -\u0026gt; generate(out, className, fields,table)} // new File(dir, className + \u0026quot;.java\u0026quot;).withPrintWriter { out -\u0026gt; generate(out, className, fields,table) } } // 获取包所在文件夹路径 def getPackageName(dir) { return dir.toString().replaceAll(\u0026quot;\\\\\\\\\u0026quot;, \u0026quot;.\u0026quot;).replaceAll(\u0026quot;/\u0026quot;, \u0026quot;.\u0026quot;).replaceAll(\u0026quot;^.*src(\\\\.main\\\\.java\\\\.)?\u0026quot;, \u0026quot;\u0026quot;) + \u0026quot;;\u0026quot; } def generate(out, className, fields,table) { out.println \u0026quot;package $packageName\u0026quot; out.println \u0026quot;\u0026quot; out.println \u0026quot;import com.yunhuakeji.component.base.annotation.doc.ApiField;\u0026quot; out.println \u0026quot;import com.yunhuakeji.component.base.annotation.entity.Code;\u0026quot; out.println \u0026quot;import com.yunhuakeji.component.base.bean.entity.base.BaseEntity;\u0026quot; out.println \u0026quot;import com.yunhuakeji.component.base.enums.entity.YesNoCodeEnum;\u0026quot; out.println \u0026quot;import lombok.Getter;\u0026quot; out.println \u0026quot;import lombok.Setter;\u0026quot; out.println \u0026quot;import lombok.ToString;\u0026quot; out.println \u0026quot;import io.swagger.annotations.ApiModel;\u0026quot; out.println \u0026quot;import io.swagger.annotations.ApiModelProperty;\u0026quot; out.println \u0026quot;import java.time.LocalDateTime;\u0026quot; out.println \u0026quot;import javax.persistence.Table;\u0026quot; out.println \u0026quot;import javax.persistence.Column;\u0026quot; //out.println \u0026quot;import import java.util.Date;\u0026quot; Set types = new HashSet() fields.each() { types.add(it.type) } if (types.contains(\u0026quot;Date\u0026quot;)) { out.println \u0026quot;import java.time.LocalDateTime;\u0026quot; } if (types.contains(\u0026quot;InputStream\u0026quot;)) { out.println \u0026quot;import java.io.InputStream;\u0026quot; } out.println \u0026quot;\u0026quot; out.println \u0026quot;/**\\n\u0026quot; + \u0026quot; * @Description \\n\u0026quot; + \u0026quot; * @Author chenzhicong\\n\u0026quot; + \u0026quot; * @Date \u0026quot;+ new SimpleDateFormat(\u0026quot;yyyy-MM-dd\u0026quot;).format(new Date()) + \u0026quot; \\n\u0026quot; + \u0026quot; */\u0026quot; out.println \u0026quot;\u0026quot; out.println \u0026quot;@Setter\u0026quot; out.println \u0026quot;@Getter\u0026quot; out.println \u0026quot;@ToString\u0026quot; out.println \u0026quot;@Table ( name =\\\u0026quot;\u0026quot;+table.getName() +\u0026quot;\\\u0026quot; )\u0026quot; out.println \u0026quot;public class $className extends BaseEntity {\u0026quot; out.println \u0026quot;\u0026quot; out.println genSerialID() fields.each() { if(!\u0026quot;universityId\u0026quot;.equals(it.name)\u0026amp;\u0026amp; !\u0026quot;operatorId\u0026quot;.equals(it.name)\u0026amp;\u0026amp; !\u0026quot;createdDate\u0026quot;.equals(it.name)\u0026amp;\u0026amp; !\u0026quot;state\u0026quot;.equals(it.name)\u0026amp;\u0026amp; !\u0026quot;stateDate\u0026quot;.equals(it.name)\u0026amp;\u0026amp; !\u0026quot;memo\u0026quot;.equals(it.name)){ out.println \u0026quot;\u0026quot; // 输出注释 if (isNotEmpty(it.commoent)) { out.println \u0026quot;\\t/**\u0026quot; out.println \u0026quot;\\t * ${it.commoent.toString()}\u0026quot; out.println \u0026quot;\\t */\u0026quot; } if (it.annos != \u0026quot;\u0026quot;) out.println \u0026quot; ${it.annos.replace(\u0026quot;[@Id]\u0026quot;, \u0026quot;\u0026quot;)}\u0026quot; // 输出成员变量 out.println \u0026quot;\\tprivate ${it.type} ${it.name};\u0026quot; } } // 输出get/set方法 // fields.each() { // out.println \u0026quot;\u0026quot; // out.println \u0026quot;\\tpublic ${it.type} get${it.name.capitalize()}() {\u0026quot; // out.println \u0026quot;\\t\\treturn this.${it.name};\u0026quot; // out.println \u0026quot;\\t}\u0026quot; // out.println \u0026quot;\u0026quot; // // out.println \u0026quot;\\tpublic void set${it.name.capitalize()}(${it.type} ${it.name}) {\u0026quot; // out.println \u0026quot;\\t\\tthis.${it.name} = ${it.name};\u0026quot; // out.println \u0026quot;\\t}\u0026quot; // } out.println \u0026quot;\u0026quot; out.println \u0026quot;}\u0026quot; } def calcFields(table) { DasUtil.getColumns(table).reduce([]) { fields, col -\u0026gt; def spec = Case.LOWER.apply(col.getDataType().getSpecification()) def typeStr = typeMapping.find { p, t -\u0026gt; p.matcher(spec).find() }.value if(\u0026quot;Date\u0026quot;.equals(typeStr)){ typeStr=\u0026quot;LocalDateTime\u0026quot; } if(col.getName().toString().startsWith(\u0026quot;PK_\u0026quot;)){ typeStr = \u0026quot;Long\u0026quot; } def comm =[ colName : col.getName(), name : javaName(col.getName(), false), type : typeStr, commoent: col.getComment(), annos: \u0026quot;\\t@Column(name = \\\u0026quot;\u0026quot;+col.getName()+\u0026quot;\\\u0026quot; )\u0026quot;] if(isNotEmpty(col.getComment())){ comm.annos +=\u0026quot;\\r\\n\\t@ApiField(desc = \\\u0026quot;\u0026quot;+col.getComment()+\u0026quot;\\\u0026quot;)\u0026quot; } /* if(col.getComment().startsWith(\u0026quot;pk_\u0026quot;)){ comm.annos +=\u0026quot;\\r\\n\\t@Id\u0026quot; }*/ if(Case.LOWER.apply(comm.name.toString()).startsWith(\u0026quot;pk\u0026quot;)){ comm.annos +=\u0026quot;\\r\\n\\t@Id\u0026quot; } if(\u0026quot;id\u0026quot;.equals(Case.LOWER.apply(col.getName()))){ comm.annos +=[\u0026quot;@Id\u0026quot;]} fields += [comm] } } // 处理类名（这里是因为我的表都是以t_命名的，所以需要处理去掉生成类名时的开头的T， // 如果你不需要那么请查找用到了 javaClassName这个方法的地方修改为 javaName 即可） def javaClassName(str, capitalize) { def s = com.intellij.psi.codeStyle.NameUtil.splitNameIntoWords(str) .collect { Case.LOWER.apply(it).capitalize() } .join(\u0026quot;\u0026quot;) .replaceAll(/[^\\p{javaJavaIdentifierPart}[_]]/, \u0026quot;_\u0026quot;) // 去除开头的T http://developer.51cto.com/art/200906/129168.htm s = s[1..s.size()-1] capitalize || s.length() == 1? s : Case.LOWER.apply(s[0]) + s[1..-1] } def javaName(str, capitalize) { // def s = str.split(/(?\u0026lt;=[^\\p{IsLetter}])/).collect { Case.LOWER.apply(it).capitalize() } // .join(\u0026quot;\u0026quot;).replaceAll(/[^\\p{javaJavaIdentifierPart}]/, \u0026quot;_\u0026quot;) // capitalize || s.length() == 1? s : Case.LOWER.apply(s[0]) + s[1..-1] def s = com.intellij.psi.codeStyle.NameUtil.splitNameIntoWords(str) .collect { Case.LOWER.apply(it).capitalize() } .join(\u0026quot;\u0026quot;) .replaceAll(/[^\\p{javaJavaIdentifierPart}[_]]/, \u0026quot;_\u0026quot;) capitalize || s.length() == 1? s : Case.LOWER.apply(s[0]) + s[1..-1] } def isNotEmpty(content) { return content != null \u0026amp;\u0026amp; content.toString().trim().length() \u0026gt; 0 } static String changeStyle(String str, boolean toCamel){ if(!str || str.size() \u0026lt;= 1) return str if(toCamel){ String r = str.toLowerCase().split('_').collect{cc -\u0026gt; Case.LOWER.apply(cc).capitalize()}.join('') return r[0].toLowerCase() + r[1..-1] }else{ str = str[0].toLowerCase() + str[1..-1] return str.collect{cc -\u0026gt; ((char)cc).isUpperCase() ? '_' + cc.toLowerCase() : cc}.join('') } } static String genSerialID() { return \u0026quot;\\tprivate static final long serialVersionUID = \u0026quot;+Math.abs(new Random().nextLong())+\u0026quot;L;\u0026quot; } 把脚本保存为groovy格式然后移动到项目目录中入图所示：\n接下来就可以直接在idea右侧database中对需要生成实体类的表执行脚本了，如图所示，点击右侧的database-选择表右键-选择scripted-Extensions-然后选择添加的脚本\n之后选择生成的目录为entity目录： 之后类就生成好了：\n其中继承的类，引入的包，注解都可以在脚本中修改。\n三、生成service、serviceImpL和dao 与实体生成不一样，这里使用codemaker插件功能生成。先在codemaker中添加模板。 进入settings，搜索codemaker，进入codemaker相关配置项。\n添加所需要的模板，以生成Dao为例：上面只需要改className，这里写入${class0.className}Dao，表示取输入的类的类名后面加个Dao\n模板代码也是使用现成的，自己根据需要猜着改就行，模板代码如下： dao/mapper模板：\n######################################################################################## ## ## Common variables: ## $YEAR - yyyy ## $TIME - yyyy-MM-dd HH:mm:ss ## $USER - 陈之聪 ## ## Available variables: ## $class0 - the context class, alias: $class ## $class1 - the selected class, like $class1, $class2 ## $ClassName - generate by the config of \u0026quot;Class Name\u0026quot;, the generated class name ## ## Class Entry Structure: ## $class0.className - the class Name ## $class0.packageName - the packageName ## $class0.importList - the list of imported classes name ## $class0.fields - the list of the class fields ## - type: the field type ## - name: the field name ## - modifier: the field modifier, like \u0026quot;private\u0026quot;,or \u0026quot;@Setter private\u0026quot; if include annotations ## $class0.allFields - the list of the class fields include all fields of superclass ## - type: the field type ## - name: the field name ## - modifier: the field modifier, like \u0026quot;private\u0026quot;,or \u0026quot;@Setter private\u0026quot; if include annotations ## $class0.methods - the list of class methods ## - name: the method name ## - modifier: the method modifier, like \u0026quot;private static\u0026quot; ## - returnType: the method returnType ## - params: the method params, like \u0026quot;(String name)\u0026quot; ## $class0.allMethods - the list of class methods include all methods of superclass ## - name: the method name ## - modifier: the method modifier, like \u0026quot;private static\u0026quot; ## - returnType: the method returnType ## - params: the method params, like \u0026quot;(String name)\u0026quot;# ######################################################################################## package $class0.PackageName; import com.mapper.GeneralMapper; /** * * @author chenzhicong * @version $Id: ${ClassName}.java, v 0.1 $TIME $USER Exp $$ */ public interface $ClassName extends GeneralMapper\u0026lt;${class0.className}\u0026gt;{ } service、serviceImpl操作方法类似，就不赘述了，按照上面的方法分别建立模板就行。\n模板建立好了之后，我们就只需要在刚刚生成的实体类中按快捷键Alt+Insert（或者右键-Generate）-选择对应模板-然后选择生成目录：\n这里好像只能生成在entity同目录，不能选择其他目录，需要我们移动到对应的包。 生成后的dao如图所示：\n总结 至此，代码就生成完毕了，这个方法让自己初次接触到了groovy脚本，不过还是不会怎么用，不过凑合复制粘贴别人的脚本也能改改。先就这样吧，以后遇到相关该脚本的问题再百度。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-07-04T00:00:00Z","permalink":"https://runningccode.github.io/2019/idea%E5%BF%AB%E9%80%9F%E7%94%9F%E6%88%90entitydaoservice/","title":"idea快速生成entity、dao、service"},{"content":"\rimg.png\r本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-05-10T00:00:00Z","permalink":"https://runningccode.github.io/2019/activiti%E6%B5%81%E7%A8%8B%E5%BC%95%E6%93%8E%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"Activiti流程引擎学习笔记"},{"content":"一、定义 Provide a surrogate or placeholder for another object to control access to it.（为其他对象提供一种代理以控制对这个对象的访问。）\n二、基本组成 （一）类图 img.png\r（二）角色定义 Subject抽象主题角色。抽象主题类可以是抽象类也可以是接口，是一个最普通的业务类型定义，无特殊要求 RealSubject具体主题角色。也叫做被委托角色、被代理角色。是业务逻辑的具体执行者。 Proxy代理主题角色。也叫做委托类、代理类。它负责对真实角色的应用，把所有抽象主题类定义的方法限制委托给真实主题角色实现，并且在真实主题角色处理完毕前后做预处理和善后处理工作。一个代理类可以代理多个被委托者或被代理者。 三、代理方式与实现 普通代理。客户端可以指定代理类对象，其实现为客户端创建被代理对象，然后将被代理对象注入代理对象，用代理对象去执行业务。 强制代理。客户端只能使用业务对象规定的代理对象，其实现为客户端创建被代理对象，通过被代理对象获取代理对象，然后用代理对象去执行业务。 四、说一说动态代理 代理模式的高级应用是动态代理。动态代理的优势在于相较于普通的使用代理模式需要为每个具体类创建一个类，而动态代理是动态的生成代理类而不需要定义代理类。\n（一）动态代理的两种方式 JDK提供的java.lang.reflect.Proxy动态代理 cglib库 两者的区别在于：\n实现原理的区别：jdk是基于业务类实现的接口动态生成代理类定义，然后将代理类方法的实现转发给InvocationHanlder（InvocationHanlder需先注入被代理对象），而cglib是基于继承动态生成其子类，然后重写其子类方法来实现的。 限制上的区别：jdk限制被代理类必须实现接口，而cglib没有这个限制。 创建的对象个数不同：因为原理的不同，jdk需要实例化两个对象，而cglib只实例化一个对象。 （二）基于动态代理的代理模式（以jdk方式为例） 类图：\nimg_1.png\r其中，动态代理DynamicProxy实现代理的职责，业务逻辑Subject实现相关的逻辑功能。\n三、代理模式的优点 在不改变原方法的基础上通过代理提供访问控制、日志记录等等，与业务逻辑相独立，符合单一职责原则。\n四、与装饰器模式的区分 个人感觉装饰器模式和代理模式用法上来说差不多，同样的代码不看其他地方的应用说不清楚到底是装饰器模式和代理模式，两者最重要的区别只是说关注的点不一样罢了。一个是注重的是装饰，一个注重的是代理。需要注意的是装饰类或代理类本身也可以再次装饰和代理。装饰注重的是增强对象原有的功能，而代理模式注重的是控制访问。\n代理模式对于整个系统来说是控制了真实对象的访问，我们往往只需要使用代理类完成相关业务功能就行了，被代理类相对透明。如果是装饰器模式，那么我们需要关注的是被装饰的类需要添加什么装饰这个过程，系统中的代码还是得依赖被装饰对象。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-04-14T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/","title":"设计模式之代理模式"},{"content":"\rimg.png\r简而言之，原型模式就是通过一个创建好的已经初始化完成的一个模板对象，拷贝获取另一个一样的对象，能便利的获取基于原型模板的对象。并且能解决并发修改问题。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-04-14T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/","title":"设计模式之原型模式"},{"content":"一、什么是中介者模式 Define an object that encapsulates how a set of objects interact.Mediator promotes loose coupling by keeping objects from referring to each other explicitly,and it lets you vary their interaction independently.（用一个中介对象封装一系列的对象交互，中介者使各对象不需要显示地相互作用，从而使其耦合松散，而且可以独立地改变它们之间的交互。）\n（一）类图 img.png\r（二）角色定义 Mediator: 抽象中介者 ConcreteMediator: 具体中介者 Colleague: 抽象同事类 ConcreteColleague: 具体同事类 二、优缺点 优点：将原本一对多的依赖关系，通过中介者转换为1对1的依赖关系，使得类间依赖关系更为直观；同时通过中间者封装了与所有同事之间的交互，分离了对象自身业务逻辑与其依赖的同事关系行为，使得职责更清晰，业务逻辑也更易于修改了。 缺点：在具体中介者类中包含了多个同事之间的交互细节，有可能会导致具体中介者类变得很复杂；抽象同事类难以抽象出同事类行为的共性，如果提取不出来则中介类直接引入同事类的具体实现，与依赖倒置原则相冲突。 三、应用场景 一个业务需要多个对象参与，他们之间的依赖关系联系紧密，但却很混乱。\n四、个人理解 一系列的对象依赖关系复杂，层次混乱，那么可以用中介者模式进行重构，抽象对象的关系为中介者与同事类的关系。重构后的架构有一个中介者，多个同时类，原来的每一个类都有一个对应的同事类， 完成自己的事情，如果这一个类需要完成的事情需要其他类来协作的话，统一用中介者的方法来代理，中介者该方法的实现实际上就是调用另一个具体的同事类来完成的。这样可以避免类之间调用关系的混乱。原来的如原来的A-\u0026gt;B-\u0026gt;C-\u0026gt;B-\u0026gt;A优化为了A-\u0026gt;中介者，B-\u0026gt;中介者，C-\u0026gt;中介者，然后中介者-\u0026gt;A,B,C,D\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-04-14T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%B8%AD%E4%BB%8B%E8%80%85%E6%A8%A1%E5%BC%8F/","title":"设计模式之中介者模式"},{"content":"一、定义 Separate the construction of a complex object from its representation so that the same construction process can create different representations.（将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。）\n二、构成 （二）类图 引用《设计模式之禅》中的类图：\nimg.png\r其中：\nProduct产品抽象 ConcreteProduct具体产品类 Builder抽象建造者 规范产品的组建，一般是由子类实现。 ConcreteBuilder具体建造者 实现抽象类定义的所有方法，提供设置产品类成员变量的方法以及返回产品类对象的方法，也就是只提供一系列建造对象的行为。 Director导演类 负责准备基础数据，调用具体建造者进行建造，只关注建造对象的过程。 注：其中产品类的特点是，同一个类成员变量不同则行为也有不同 三、优点 封装各个产品的生成逻辑，上层业务只关心需要获取哪个产品。\n四、使用场景 我对建造者模式使用场景的理解是同一个类的成员变量不同则行为也有不同，通过组装成员变量封装该类对象的创建。 对于一类对象，对一个行为的区别我们使用成员变量区分更方便，这意味着我们将某一个行为抽象到了成员变量上面，成员变量不同，导致的产品特征也不同。这个时候用建造者模式来获取这些具体的对象更方便。这时候我们可以将设置成员变量和获取对象两个行为抽象到建造者对象的方法中，其中建造者产出的是一类对象，这一类对象仅仅是成员变量的不同，那么导演类的职责就是为场景方提供确切的对象，只关注构建的过程，上层场景只关注获取某一个确切的类。当我们需要某一个特殊的产品时仅仅通过导演类去调用建造者的设置变量的方法(setPart()方法)和获取对象的方法（build()方法）就能得到想要的对象\n五、与工厂模式的区别 个人理解是，工厂模式里面的工厂关注的是直接实例化某类对象得到产品，但对于建造者模式里面的建造者而言，建造者并不直接返回产品，建造者只是提供建造方法（关注的是setPart方法-设置成员变量，product方法-产出设置后的产品）去获取产品，真正获取产品是由导演类获取，通过同一个建造者，两种应用场景的区别是我们需要获取的产品对应的JAVA类，是否是依赖成员变量不同来维护一种需求上的变化。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-04-11T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/","title":"设计模式之建造者模式"},{"content":"一、定义 Define the skeleton of an algorithm in an operation,deferring some steps to subclasses.Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm\u0026rsquo;s structure.（定义一个操作中的算法的框架，而将一些步骤延迟到子类中。使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。）\n在JDK的IO流中就提供了抽象类InputStream和OutPutStream中就定义了许多模板方法，而基本方法只有一个read()和write()，实现对流单个字节的读入和写出，其他方法抽象类都提供模板，当然子类处于优化的目的也可以进行重写。\n二、结构 ##（一）类图\nimg.png\r##（二）名词解释\n抽象模板:子类的抽象默认实现类，类图中为AbstractClass。 基本方法：由子类实现的方法，类图中为doAnything()和doSomething()，权限修饰符尽量为protectd。 模板方法：抽象类的默认实现方法，实现对基本方法的调度。类图中为templateMethod()，一般加final关键字禁止被重写。 三、优点 代码复用，扩展良好。\n四、应用场景 多个子类有公有的方法，并且逻辑基本相同时。 重要、复杂的算法，可以把核心算法设计为模板方法，周边的相关细节功能则由各个子类实现 重构时，模板方法模式是一个经常使用的模式，把相同的代码抽取到父类中，然后通过钩子函数（Hook Method）约束其行为。(钩子函数：即模板方法存在分支，这可以定义一个钩子方法供子类实现，末班方法中根据钩子方法返回值来控制分支走向) 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-04-09T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/","title":"设计模式之模板方法模式"},{"content":"一、定义 Software entities like classes,modules and functions should be open for extension but closed for modifications.（一个软件实体如类、模块和函数应该对扩展开放，对修改关闭。）\n开闭原则相比前面介绍的5项原则更模糊，其只界定了我们要达到什么目标，而没有告诉我们怎么去做。\n开闭原则对扩展开放，对修改关闭，并不意味着不做任何修改，低层模块的变更，必然要有高层模块进行耦合，否则就是一个孤立无意义的代码片段。\n二、为什么要实现开闭原则 对测试的影响。若软件实现开闭原则，那么针对一个需求的扩展我们只能修改以前的代码，这对以前写好的单元测试会造成影响。 提高复用性。所有的逻辑都是从原子逻辑组合而来的，实现开闭原则的手段之一是通过缩小逻辑粒度，抽象相同的逻辑，提高了代码复用。 提高可维护性。开闭原则通过扩展的方式实现产品迭代，比修改原有代码更容易。 面向对象开发的开发需求。是对象就会有变化，而开闭原则通过抽象，限制了变化边界，是面向对象开发的开发需求。 三、如何使用开闭原则 （一） 制定抽象约束 通过接口或抽象类约束一组可能变化的行为。\n通过接口或抽象类约束扩展，对扩展进行边界限定。 参数类型、引用对象尽量使用接口或者抽象类，而不是实现类； 抽象层尽量保持稳定，一旦确定尽量不要修改。 （二） 元数据（metadata）控制模块行为 所谓元数据指的是配置信息，可以从文件中或数据库中读取，如springBoot的application.yml或application.propertis等等。\n（三） 封装变化 将相同的变化封装到一个接口或抽象类中。 将不同的变化封装到不同的接口或抽象类中。 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-04-07T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%85%AD%E5%A4%A7%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99%E5%85%AD%E5%BC%80%E9%97%AD%E5%8E%9F%E5%88%99/","title":"设计模式六大设计原则（六）：开闭原则"},{"content":"单例模式应该都很熟悉，就简单上一张图吧：\nimg.png\r本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-04-07T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/","title":"设计模式之单例模式"},{"content":"一、定义 Define an interface for creating an object,but let subclasses decide which class to instantiate.Factory Method lets a class defer instantiation to subclasses.（定义一个用于创建对象的接口，让子类决定实例化哪一个类。工厂方法使一个类的实例化延迟到其子类。）\n类图：\nimg.png\r二、用法 定义工厂抽象方法，产品抽象 新建工厂的具体子类去实现工厂抽象方法（可以有参数，如传入产品抽象实现类的Class），返回产品抽象。(所有的产品可以抽象为多个产品族，此时可以考虑定义多个具体工厂类分别对应每个产品族的生产-这就是抽象工厂模式) 三、优点 封装创建过程、高层模块与创建过程的解耦，使得程序易于扩展。 符合迪米特法则\u0026ndash;业务只和工厂抽象打交道，隔离了复杂的创建过程。 符合依赖倒置原则\u0026ndash;业务通过抽象完成，具体的创建产品的细节被工厂方法隐藏了。 符合里氏替换原则 四、应用场景 很多业务都会用到某些产品，但是产品却不一样，创建产品对象的流程有点复杂。\n五、扩展 （一）简单工厂模式 弱化了工厂方法模式，即业务模块只需要一个工厂，则不对工厂进行抽象，同时工厂方法转为静态方法。缺点是不能进行扩展，不符合开闭原则，如果以后创建出的产品流程需要修改，则简单工厂模式必须修改代码。\n（二）延迟初始化（有点类似缓存） 一个对象被消费完毕后，并不立刻释放，工厂类保持其初始状态（最简单的保存在LinkedHashMap（可以做一个LRU容器）里面），等待再次被使用。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-04-07T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","title":"设计模式之工厂模式"},{"content":"一、定义 High level modules should not depend upon low level modules.Both should depend upon abstractions.Abstractions should not depend upon details.Details should depend upon abstractions.（高层模块不应该依赖低层模块，两者都应该依赖其抽象。 抽象不应该依赖细节。 细节应该依赖抽象）\n用JAVA语言来阐述的话意思是约束了：\n模块间的依赖通过抽象发生，实现类之间不发生直接的依赖关系，其依赖关系是通过接口或抽象类产生的 接口或抽象类不依赖于实现类 实现类依赖接口或抽象类 二、优点与限制 这不就是我们常听到的面向接口编程吗，其中的好处不言自明。但我还是简单总结下把：\n（一）优点 减少类间的耦合性，因为模块间的依赖变为了通过接口的方式，模块间调用依赖具体类变为了依赖接口，而接口更加抽象。 提高系统的稳定性，因为接口是具体类职责的抽象出的相对稳定的部分。 降低并行开发引起的风险，先定义接口职责，各模块可以通过接口来并行开发。 提高代码的可读性和可维护性，减少耦合，自然可读性可维护性就提高了。 （二）限制 对于一个业务需求来说，并不是都能抽象出一个接口，如果强行抽象出接口，这个接口的复用性也不会很高。因为这个业务在这里的联系本来就是依赖于具体细节的。所以有时候我们看到WEB开发中，有些单体项目并不会为Service层定义接口，因为在这种项目中，定义接口的复用性也不是很高。 三、建议 每个类尽量都有接口或抽象类，或者抽象类和接口两者都具备（接口定义职责，抽象类提供默认实现） 变量的表面类型尽量是接口或者是抽象类 尽量不要从具体类派生类 尽量不要覆写基类已经实现的方法 结合里氏替换原则使用，意思就是说类实现时不要破坏抽象的职责定义 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-04-06T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%85%AD%E5%A4%A7%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99%E4%B8%89%E4%BE%9D%E8%B5%96%E5%80%92%E7%BD%AE%E5%8E%9F%E5%88%99/","title":"设计模式六大设计原则（三）：依赖倒置原则"},{"content":"一、定义 一个对象应该对其他对象有最少的了解。\n在介绍具体含义之前先解释下什么是朋友类：\n出现在成员变量、方法的输入输出参数中的类称为成员朋友类，而出现在方法体内部的类不属于朋友类。\n最少知识原则的包含的具体含义：\n只和朋友交流。类与类之间的关系是建立在类间的，而不是方法间，因此一个方法尽量不引入一个类中不存在的对象。换句话说在方法中应尽量避免调用朋友返回的对象（朋友的朋友）的方法。当然，JDK API提供的类除外。 朋友间也是有距离的。尽量不要对外公布太多的public方法和非静态的public变量，尽量内敛，多使用private、package-private、protected等访问权限。 是自己的就是自己的。如果一个方法放在本类中，既不增加类间关系，也对本类不产生负面影响，那就放置在本类中。 二、优缺点 （一）优点 降低模块间的耦合，提升了软件的可维护性和可重用性。\n（二）缺点 在系统里造出大量的小方法，这些方法仅仅是传递间接的调用，与系统的业务逻辑无关。\n三、建议 如果一个类跳转两次以上才能访问到另一个类，就需要想办法进行重构了。 需要反复权衡，既做到让结构清晰，又做到高内聚、低耦合。 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-04-06T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%85%AD%E5%A4%A7%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99%E4%BA%94%E6%9C%80%E5%B0%91%E7%9F%A5%E8%AF%86%E5%8E%9F%E5%88%99/","title":"设计模式六大设计原则（五）：最少知识原则"},{"content":"一、里氏替换原则的概念 里氏替换由Barbara Liskov女士提出，其给出了两种定义：\nIf for each object o1 of type S there is an object o2 of type T such that for all programs P defined in terms of T,the behavior of P is unchanged when o1 is substituted for o2 then S is a subtype of T.（如果对每一个类型为S的对象o1，都有类型为T的对象o2，使得以T定义的所有程序P在所有的对象o1都代换成o2时，程序P的行为没有发生变化，那么类型S是类型T的子类型。）\nFunctions that use pointers or references to base classes must be able to use objects of derived classes without knowing it.（所有引用基类的地方必须能透明地使用其子类的对象。）\n二、里氏替换原则的含义 结合我的理解，我认为里氏替换有两层含义：\n（一）对于业务而言，能够运用多态透明的使用子类对象，增强代码复用 子类必须完全实现父类的方法，且实现的方法不能破坏父类的职责定义。 因为若破坏了职责定义后，对于通过父类引用操作子类对象的程序来讲，会破坏多态的封装，使得程序普适性降低。反之，则可以充分发挥多态的优点，提高代码的复用性。 （二）父类引用的子类对象可以安全的替换为子类引用，增强代码扩展性 重载父类的方法时输入参数不能缩小。 在缩小的情况下，父类引用替换为子类引用的时候，可能会出现子类并没有重写父类同名同参数列表方法，但是却调用到了子类的方法。反之，一个程序模块的功能本来是通过父类引用操作子类对象来实现，但现在又面临扩展功能，且我们的需求没有普适到对所有的子类都扩展这个功能，我们希望通过重建（参数列表为父类型的通过重建子类）或修改（代码内实例引用为父类型的通过修改）来扩展这个方法，若程序符合里氏替换原则，这种扩展就是安全的。 三、里氏替换规范的行为——继承类与实现接口的方式对比 无论采取继承类或实现接口，我们都应该遵循里氏替换原则，保证职责定义不被破坏，父类引用能安全的被子类对象替换。那么这两种方式，在实际开发中，有什么需要注意的地方，应该怎么处理嘞\n（一）继承类 继承类的优点在于能够实现便捷、直观的共享代码，也能实现多态，但继承是把双刃剑，也有需要注意的地方：\n1.父类内部实现之间依赖需警惕 基类代码:\npublic class Base { private static final int MAX_NUM = 1000; private int[] arr = new int[MAX_NUM]; private int count; public void add(int number){ if(count\u0026lt;MAX_NUM){ arr[count++] = number; } } public void addAll(int[] numbers){ for(int num : numbers){ add(num); } } } 子类代码：\npublic class Child extends Base { private long sum; @Override public void add(int number) { super.add(number); sum+=number; } @Override public void addAll(int[] numbers) { super.addAll(numbers); for(int i=0;i\u0026lt;numbers.length;i++){ sum+=numbers[i]; } } public long getSum() { return sum; } } 基类的add方法和addAll方法用于将数字添加到内部数组中，子类在此基础上添加了成员变量sum，用于表示数组元素之和。\npublic static void main(String[] args) { Child c = new Child(); c.addAll(new int[]{1,2,3}); System.out.println(c.getSum()); } 期望结果是1+2+3=6，可是结果却是12。为什么嘞，这是因为子类调用的父类的addAll方法依赖的add方法同时也被子类重写了，这里先addALL再自己统计一遍和相当于统计了两遍和。\n此时若想正确输出需要我们把子类的addAll方法修改为：\n@Override public void addAll(int[] numbers) { super.addAll(numbers); } 可是，这样又会产生新的一个问题，如果父类修改了add方法的实现为：\npublic void addAll(int[] numbers){ for(int num : numbers){ if(count\u0026lt;MAX_NUM){ arr[count++] = num; } } } 那么输出又会变为0了。\n从这个例子我们可以看出： 如果父类内部方法可能存在依赖，重写方法不仅仅改变了被重写的方法，同时另一个方法（假设为A）也导致出现了偏差，此时若按照原有的职责定义去调用父类的A方法，可能会导致出乎意料的结果。并且，若就算子类在编写时意识到了父类方法间的依赖，修改为正确实现，那么父类就无法自由的修改内部实现了。\n这个问题产生的原因在于我们重写方法时往往容易只关注父类被重写方法的职责定义，而容易忽视父类其他方法是否存在依赖此方法。导致我们还是破坏了父类行为的职责定义，违反了里氏替换原则，其具有一定的隐蔽性。这就要求我们在编写子类实现的时候必须注意到其他方法受没受影响。同时依赖于内部方法的父类方法也不能随意修改，若被修改方法依赖的方法在其中一个子类被重写。那么就算父类在本类没有改变职责定义，实现结果并没有区别，但是若该子类调用，也有可能导致子类预期职责偏差的风险。\n2.继承关系难以界定 继承反映的是‘是不是’的关系，假设有两个类，鸟类有会fly（）的方法，此时我们需要添加一个企鹅类，从常识上来看企鹅应该是鸟类的子类。但是由于企鹅的个性，他不能飞，此时就产生了矛盾，原本我们在父类定义了鸟会飞的职责，按照里氏替换原则，我们企鹅这个子类的fly（）方法必须符合职责定义，但是实际上无法符合，所以就无法实现继承，这与常识相违背。\n3.存在单继承限制 继承只能继承一个类，相比接口缺少一定灵活性。\n（二）实现接口 实现接口相比继承就灵活多了，也没有那么多弊端，因为接口仅仅包含职责定义，并没有包含代码实现。其优点在于：\n实现多态 同时子类可以实现多个接口，相比继承更为灵活 但是与继承类的方式相比，也有不足的地方，其不能实现代码的共享，虽然能够在实现类中通过注入公共类，用公共类实现代码共享，但是却没有继承便捷，直观。\n（三）建议 无论是继承类还是实现接口，都需要按父类职责定义实现方法 优先使用接口+注入而非继承 运用继承时，实现父类内部方法时最好不要互相依赖，若需要依赖，可以使用final修饰被依赖的方法，因为父类对于子类来说最好是封装好的，子类不考虑内部实现也能自由的重写父类方法，同时注意行为实现的普适性，只实现真正公共的部分。 运用继承时，子类尽量不要重写父类方法，若需重写也不能破坏父类的职责定义，需了解父类具体实现，了解父类的方法之间的依赖关系。 本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-04-05T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%85%AD%E5%A4%A7%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99%E4%BA%8C%E9%87%8C%E5%BC%8F%E6%9B%BF%E6%8D%A2%E5%8E%9F%E5%88%99/","title":"设计模式六大设计原则（二）：里式替换原则"},{"content":"什么是单一职责原则 单一职责原则的原话是：\nThere should never be more than one reason for a class to change。\n翻译过来就是应该有且仅有一个原因引起类的变更。\n要知道，我们的方法、接口、类都是基于职责设计的，只是粒度不同，单一职责原则，强调的是我们的方法、接口、类不应该包含太多的职责。\n为什么要实现单一职责原则 最重要的一点，是因为需要尽量控制职责变化造成不可控的大范围的变化。降低变更风险，如果一个方法包含的职责过多，那么只要其中一个职责变化，我们就需要修改方法，但是修改方法原则上来说我们需要确保所有调用者的期望返回不会受影响。可是如果方法职责过多，那么基于需求变动，方法修改的频率也会增多，同时另一方面方法影响的调用者也比实现了单一职责的方法范围大，这回对我们维护带来很大的麻烦。同理，接口、类包含的职责过多，弊端也是如此。 增强了可读性。因为方法、接口、类都是对代码的封装，对于使用者来说不需要了解其内部实现。单一职责使得我们看到这个方法、接口、类的引用时我们就明确的知道了使用者的意图，想使用什么职责而不至于深入去看内部实现。 实践中的痛点 实践中最最重要的痛点是我不知道怎么划分这个职责啊，方法的职责粒度该是什么，接口的职责粒度该是什么，类的职责粒度该是什么。 职责粒度划分的越细，编写时类和代码量都会增多。 关于如何划分职责粒度的建议 基于需求变动的预判考虑 基于实际需求的变化可能性程度自己判断，职责划分的粒度取决于需求的粒度，如果一个职责预判会根据需求经常发生改变，那么这个职责应该被独立出来。\n按方法、接口、类粒度从小到大进行控制 其次对于方法、接口、类实现单一职责的原则，对职责划分粒度排序应该是方法\u0026gt;接口\u0026gt;类。方法粒度最小可以理解，因为方法属于接口或类，接口排在类前面是因为我们在实际编程当中是面向接口编程，方法的入参出参都是接口，只有在方法内部实现的时候是采用实例实现接口。所以接口粒度要求比类要严格一些（两个拥有各自职责的接口可以由一个类来实现）。\n基于可读性考虑 考虑是否职责的进一步划分会使得代码可读性提高，封装之后，只看外部引用，不看内部细节，是否能一目了然。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-04-05T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%85%AD%E5%A4%A7%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99%E4%B8%80%E5%8D%95%E4%B8%80%E8%81%8C%E8%B4%A3%E5%8E%9F%E5%88%99/","title":"设计模式六大设计原则（一）：单一职责原则"},{"content":"什么是单一职责原则 单一职责原则的原话是：\nThere should never be more than one reason for a class to change。\n翻译过来就是应该有且仅有一个原因引起类的变更。\n要知道，我们的方法、接口、类都是基于职责设计的，只是粒度不同，单一职责原则，强调的是我们的方法、接口、类不应该包含太多的职责。\n为什么要实现单一职责原则 最重要的一点，是因为需要尽量控制职责变化造成不可控的大范围的变化。降低变更风险，如果一个方法包含的职责过多，那么只要其中一个职责变化，我们就需要修改方法，但是修改方法原则上来说我们需要确保所有调用者的期望返回不会受影响。可是如果方法职责过多，那么基于需求变动，方法修改的频率也会增多，同时另一方面方法影响的调用者也比实现了单一职责的方法范围大，这回对我们维护带来很大的麻烦。同理，接口、类包含的职责过多，弊端也是如此。 增强了可读性。因为方法、接口、类都是对代码的封装，对于使用者来说不需要了解其内部实现。单一职责使得我们看到这个方法、接口、类的引用时我们就明确的知道了使用者的意图，想使用什么职责而不至于深入去看内部实现。 实践中的痛点 实践中最最重要的痛点是我不知道怎么划分这个职责啊，方法的职责粒度该是什么，接口的职责粒度该是什么，类的职责粒度该是什么。 职责粒度划分的越细，编写时类和代码量都会增多。 关于如何划分职责粒度的建议 基于需求变动的预判考虑 基于实际需求的变化可能性程度自己判断，职责划分的粒度取决于需求的粒度，如果一个职责预判会根据需求经常发生改变，那么这个职责应该被独立出来。\n按方法、接口、类粒度从小到大进行控制 其次对于方法、接口、类实现单一职责的原则，对职责划分粒度排序应该是方法\u0026gt;接口\u0026gt;类。方法粒度最小可以理解，因为方法属于接口或类，接口排在类前面是因为我们在实际编程当中是面向接口编程，方法的入参出参都是接口，只有在方法内部实现的时候是采用实例实现接口。所以接口粒度要求比类要严格一些（两个拥有各自职责的接口可以由一个类来实现）。\n基于可读性考虑 考虑是否职责的进一步划分会使得代码可读性提高，封装之后，只看外部引用，不看内部细节，是否能一目了然。\n本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-04-05T00:00:00Z","permalink":"https://runningccode.github.io/2019/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%85%AD%E5%A4%A7%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99%E4%B8%80%E5%8D%95%E4%B8%80%E8%81%8C%E8%B4%A3%E5%8E%9F%E5%88%99/","title":"设计模式六大设计原则（一）：单一职责原则"},{"content":"\rimg.png\r本文原载于runningccode.github.io，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。\n","date":"2019-04-02T16:17:00+08:00","permalink":"https://runningccode.github.io/2019/markdown%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"MarkDown学习笔记"}]